#!/usr/bin/env python3
"""
Simple Feature Sensitivity Analysis - Focus on GIF Generation

This tool creates the most important output: animated GIFs showing:
1. Actual fire spreading (ground truth)
2. Raw model predictions (using actual feature values) 
3. Modified predictions (changing individual feature values)
"""

import os
# Set environment variable to avoid OpenMP duplicate library warnings
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import h5py
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from pathlib import Path
from tqdm import tqdm
import warnings
from scipy import ndimage
from PIL import Image
from sklearn.metrics import average_precision_score
import json

warnings.filterwarnings('ignore')

# ============================================================================
# SIMPLE CONFIGURATION
# ============================================================================

class SimpleConfig:
    SEQUENCE_LENGTH = 5
    SPATIAL_SIZE = (128, 128)  # Same as test_simulation!
    BEST_FEATURES = [3, 4, 0, 1, 2, 13, 14, 15, 16, 5, 8, 9, 22]  # NDVI, EVI2, etc.
    FEATURE_NAMES = [
        'VIIRS_M11', 'VIIRS_I2', 'VIIRS_I1', 'NDVI', 'EVI2', 'Total_Precip', 'Wind_Speed',
        'Wind_Direction', 'Min_Temp_K', 'Max_Temp_K', 'ERC', 'Spec_Hum', 'PDSI',
        'Slope', 'Aspect', 'Elevation', 'Landcover', 'Forecast_Precip', 'Forecast_Wind_Speed',
        'Forecast_Wind_Dir', 'Forecast_Temp_C', 'Forecast_Spec_Hum', 'Active_Fire'
    ]
    ANGULAR_FEATURES = [7, 14, 19]  # Wind_Direction, Aspect, Forecast_Wind_Dir
    FIRE_THRESHOLD = 0.3
    SIMULATION_DAYS = 26  # Show entire available fire event period
    
    # Enhanced perturbation settings
    PERTURBATION_LEVELS = [-50, -30, -20, -10, 0, 10, 20, 30]  # 8 levels including baseline

# ============================================================================
# COMPATIBILITY CLASSES FOR MODEL LOADING
# ============================================================================

class WildFireConfig:
    """Compatibility config class for checkpoint loading"""
    def __init__(self):
        self.SPATIAL_SIZE = (128, 128)
        self.SEQUENCE_LENGTH = 5
        self.BEST_FEATURES = [3, 4, 0, 1, 2, 13, 14, 15, 16, 5, 8, 9, 22]

class FirePredictionConfig:
    """Alternative config class name"""
    def __init__(self):
        self.SPATIAL_SIZE = (128, 128) 
        self.SEQUENCE_LENGTH = 5
        self.BEST_FEATURES = [3, 4, 0, 1, 2, 13, 14, 15, 16, 5, 8, 9, 22]

# Make these available for unpickling
import sys
sys.modules[__name__].WildFireConfig = WildFireConfig
sys.modules[__name__].FirePredictionConfig = FirePredictionConfig

# ============================================================================
# SIMPLE MODEL LOADER
# ============================================================================

def load_model_with_compatibility(model_path, input_channels, sequence_length=5, device='cpu'):
    """Load model with same method as test_simulation"""
    print(f"Loading model from {model_path}...")
    
    checkpoint = None
    for method in ['safe_with_globals', 'legacy', 'fallback']:
        try:
            if method == 'safe_with_globals':
                # Add required safe globals for checkpoint loading
                torch.serialization.add_safe_globals([
                    WildFireConfig,
                    FirePredictionConfig,
                    'numpy.core.multiarray.scalar',
                    'numpy.core.multiarray._reconstruct',
                    'numpy.ndarray',
                    'collections.OrderedDict'
                ])
                checkpoint = torch.load(model_path, map_location=device, weights_only=True)
            elif method == 'legacy':
                checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            elif method == 'fallback':
                # Last resort - try to load just the state dict
                checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            
            print(f"Model loaded with {method} method")
            break
            
        except Exception as e:
            print(f"Failed with {method}: {e}")
            if method == 'fallback':
                print(f"All loading methods failed. Last error: {e}")
                print("Creating compatible model architecture with random weights...")
                break
            continue
    
    # Create model with same architecture as test_simulation
    model = OfficialFireUNet(input_channels, sequence_length)
    
    if checkpoint is not None:
        try:
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                    print(f"Model loaded from epoch {checkpoint.get('epoch', 'unknown')}")
                    if 'best_ap' in checkpoint:
                        print(f"Best AP: {checkpoint['best_ap']:.4f}")
                else:
                    model.load_state_dict(checkpoint, strict=False)
            else:
                model.load_state_dict(checkpoint, strict=False)
            print("Successfully loaded trained weights (with relaxed matching)!")
        except Exception as e:
            print(f"Failed to load state dict: {e}")
            print("Using random weights for demonstration")
    else:
        print("Using random weights for demonstration")
    
    # Ensure model is on correct device
    model = model.to(device)
    model.eval()
    
    return model

# ============================================================================
# U-NET MODEL ARCHITECTURE (SAME AS TEST_SIMULATION)
# ============================================================================

class OfficialFireUNet(nn.Module):
    """U-Net architecture matching training exactly - SAME AS TEST_SIMULATION"""
    
    def __init__(self, input_channels, sequence_length=5):
        super().__init__()
        
        self.sequence_length = sequence_length
        total_input_channels = input_channels * sequence_length
        
        # Encoder
        self.enc1 = self._double_conv(total_input_channels, 64)
        self.enc2 = self._double_conv(64, 128)
        self.enc3 = self._double_conv(128, 256)
        self.enc4 = self._double_conv(256, 512)
        
        # Bottleneck
        self.bottleneck = self._double_conv(512, 1024)
        
        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dec4 = self._double_conv(1024, 512)
        
        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = self._double_conv(512, 256)
        
        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = self._double_conv(256, 128)
        
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = self._double_conv(128, 64)
        
        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)
        self.pool = nn.MaxPool2d(2)
        
        self._initialize_weights()
    
    def _double_conv(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # Handle sequence input - CRITICAL: Flatten sequence dimension
        if x.dim() == 5:  # (B, T, C, H, W)
            B, T, C, H, W = x.shape
            x = x.view(B, T * C, H, W)  # Flatten sequence into channels
        
        # Encoder path
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        
        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))
        
        # Decoder path with skip connections
        up4 = self.upconv4(bottleneck)
        merge4 = torch.cat([up4, enc4], dim=1)
        dec4 = self.dec4(merge4)
        
        up3 = self.upconv3(dec4)
        merge3 = torch.cat([up3, enc3], dim=1)
        dec3 = self.dec3(merge3)
        
        up2 = self.upconv2(dec3)
        merge2 = torch.cat([up2, enc2], dim=1)
        dec2 = self.dec2(merge2)
        
        up1 = self.upconv1(dec2)
        merge1 = torch.cat([up1, enc1], dim=1)
        dec1 = self.dec1(merge1)
        
        # Final output
        output = self.final_conv(dec1)
        return output

# ============================================================================
# SIMPLE SIMULATOR
# ============================================================================

class SimpleFireSimulator:
    def __init__(self, model, config, device='cpu'):
        self.model = model
        self.config = config
        self.device = device
    
    def predict_single_step(self, input_sequence, debug=False):
        """Predict single fire spread step - SAME AS TEST_SIMULATION"""
        with torch.no_grad():
            if len(input_sequence.shape) == 3:
                input_sequence = input_sequence.unsqueeze(0)
            
            input_tensor = input_sequence.to(self.device)
            
            if debug:
                fire_channel = input_tensor[0, -1, -1]
                print(f"\n=== PREDICTION DEBUG ===")
                print(f"Input shape: {input_tensor.shape}")
                print(f"Input fire pixels: {(fire_channel > 0).sum().item()}")
                print(f"Input range: [{input_tensor.min().item():.3f}, {input_tensor.max().item():.3f}]")
                print(f"Input mean: {input_tensor.mean().item():.3f}")
            
            try:
                with torch.amp.autocast('cuda'):
                    output = self.model(input_tensor)
                    prediction = torch.sigmoid(output)  # CRITICAL: Apply sigmoid like test_simulation
            except:
                output = self.model(input_tensor)
                prediction = torch.sigmoid(output)      # CRITICAL: Apply sigmoid like test_simulation
            
            if debug:
                print(f"Raw output range: [{output.min().item():.3f}, {output.max().item():.3f}]")
                print(f"Sigmoid range: [{prediction.min().item():.3f}, {prediction.max().item():.3f}]")
                print(f"Predictions > 0.5: {(prediction > 0.5).sum().item()}")
                print(f"Predictions > 0.3: {(prediction > 0.3).sum().item()}")
                print(f"Mean prediction: {prediction.mean().item():.4f}")
                print("========================")
            
            return prediction.cpu().squeeze()
    
    def simulate_fire_evolution(self, initial_sequence, weather_data=None, num_days=6):
        """Simulate fire evolution over multiple days"""
        predictions = []
        current_sequence = initial_sequence.clone()
        
        for day in range(num_days):
            # Predict next day
            pred_fire = self.predict_single_step(current_sequence.unsqueeze(0), debug=False)
            pred_fire = self._apply_fire_physics(pred_fire, day, debug=False)
            predictions.append(pred_fire.numpy())
            
            # Update sequence for next prediction
            if day < num_days - 1:
                if weather_data is not None and day < len(weather_data) - self.config.SEQUENCE_LENGTH:
                    next_sequence = weather_data[day + 1:day + 1 + self.config.SEQUENCE_LENGTH].clone()
                    active_fire_idx = len(self.config.BEST_FEATURES) - 1
                    next_sequence[-1, active_fire_idx] = pred_fire
                    current_sequence = next_sequence
                else:
                    new_frame = current_sequence[-1].clone()
                    active_fire_idx = len(self.config.BEST_FEATURES) - 1
                    new_frame[active_fire_idx] = pred_fire
                    current_sequence = torch.cat([
                        current_sequence[1:],
                        new_frame.unsqueeze(0)
                    ], dim=0)
        
        return predictions
    
    def _apply_fire_physics(self, fire_prediction, day, debug=False):
        """Apply corrected fire physics - SAME AS TEST_SIMULATION"""
        if debug:
            print(f"\n=== FIRE PHYSICS (Day {day}) ===")
            print(f"Raw range: [{fire_prediction.min().item():.3f}, {fire_prediction.max().item():.3f}]")
            print(f"Pixels > threshold: {(fire_prediction > self.config.FIRE_THRESHOLD).sum().item()}")
        
        # Apply threshold (same as test_simulation)
        fire_binary = (fire_prediction > self.config.FIRE_THRESHOLD).float()
        
        if debug:
            print(f"After threshold: {fire_binary.sum().item()} pixels")
        
        # Apply decay (same decay rate as test_simulation)
        decay_factor = 1.0 - 0.05 * (day + 1)  # Use 0.05 like test_simulation
        decay_factor = max(0.1, decay_factor)
        
        fire_decayed = fire_binary * decay_factor
        
        if debug:
            print(f"Decay factor: {decay_factor:.3f}")
            print(f"After decay: {(fire_decayed > 0).sum().item()} pixels")
        
        # Spatial smoothing (CRITICAL: same as test_simulation)
        fire_smoothed = torch.tensor(
            ndimage.gaussian_filter(fire_decayed.numpy(), sigma=0.5)
        )
        
        if debug:
            print(f"Final range: [{fire_smoothed.min().item():.3f}, {fire_smoothed.max().item():.3f}]")
            print("=========================")
        
        return fire_smoothed
    
    def apply_feature_perturbation(self, input_sequence, feature_name, perturbation_percent):
        """Apply perturbation to a specific feature"""
        modified_sequence = input_sequence.clone()
        
        if feature_name not in self.config.FEATURE_NAMES:
            return modified_sequence
        
        original_feature_idx = self.config.FEATURE_NAMES.index(feature_name)
        if original_feature_idx not in self.config.BEST_FEATURES:
            return modified_sequence
        
        best_feature_idx = self.config.BEST_FEATURES.index(original_feature_idx)
        
        # Apply perturbation
        current_values = modified_sequence[:, best_feature_idx]
        perturbation_factor = 1 + perturbation_percent / 100.0
        modified_sequence[:, best_feature_idx] = current_values * perturbation_factor
        
        return modified_sequence

# ============================================================================
# PREPROCESSING FUNCTIONS (SAME AS TEST_SIMULATION)
# ============================================================================

def load_feature_stats():
    """Load feature statistics for preprocessing"""
    stats_files = ['feature_stats.npz', 'feature_stats_fold_1.npz', 'feature_stats.pkl']
    
    for stats_file in stats_files:
        if os.path.exists(stats_file):
            try:
                if stats_file.endswith('.pkl'):
                    import pickle
                    with open(stats_file, 'rb') as f:
                        return pickle.load(f)
                else:
                    stats_data = dict(np.load(stats_file))
                    # Convert to expected format
                    return {
                        'mean': stats_data.get('feature_mean', stats_data.get('mean', None)),
                        'std': stats_data.get('feature_std', stats_data.get('std', None)),
                        'best_features': [3, 4, 0, 1, 2, 13, 14, 15, 16, 5, 8, 9, 22],
                        'angular_features': [7, 14, 19]
                    }
            except Exception as e:
                print(f"Could not load {stats_file}: {e}")
                continue
    
    print("No feature statistics found, using default values")
    return None

def process_features_like_test_simulation(data, config, feature_stats=None):
    """Apply same preprocessing as test_simulation"""
    import torch.nn.functional as F
    
    T, C, H, W = data.shape
    processed = data.clone()
    
    print(f"Processing features: {data.shape} -> target size {config.SPATIAL_SIZE}")
    
    # Step 1: Angular features transformation
    if feature_stats and 'angular_features' in feature_stats:
        angular_features = feature_stats['angular_features']
    else:
        angular_features = config.ANGULAR_FEATURES
        
    for angle_idx in angular_features:
        if angle_idx < C:
            processed[:, angle_idx] = torch.sin(torch.deg2rad(processed[:, angle_idx]))
            print(f"  Applied sin transform to feature {angle_idx}")
    
    # Step 2: Handle missing values
    for c in range(C):
        mask = ~torch.isfinite(processed[:, c])
        if mask.any():
            processed[:, c][mask] = 0.0
    
    # Step 3: Resize to match test_simulation size (128x128)
    if (H, W) != config.SPATIAL_SIZE:
        processed = F.interpolate(
            processed.view(-1, 1, H, W),
            size=config.SPATIAL_SIZE,
            mode='bilinear',
            align_corners=False
        ).view(T, C, *config.SPATIAL_SIZE)
        print(f"  Resized from {(H, W)} to {config.SPATIAL_SIZE}")
    
    # Step 4: Select best features
    processed = processed[:, config.BEST_FEATURES]
    print(f"  Selected {len(config.BEST_FEATURES)} best features")
    
    # Step 5: Apply normalization if available
    if feature_stats and 'mean' in feature_stats and 'std' in feature_stats:
        training_mean = torch.FloatTensor(feature_stats['mean'])
        training_std = torch.FloatTensor(feature_stats['std'])
        
        # Select statistics for best features
        if len(training_mean) > len(config.BEST_FEATURES):
            training_mean = training_mean[config.BEST_FEATURES]
            training_std = training_std[config.BEST_FEATURES]
        
        # Apply normalization (exclude angular and categorical features)
        for f_idx, orig_idx in enumerate(config.BEST_FEATURES):
            if orig_idx not in config.ANGULAR_FEATURES and orig_idx != 16:  # Not angular, not landcover
                processed[:, f_idx] = (processed[:, f_idx] - training_mean[f_idx]) / (training_std[f_idx] + 1e-6)
        
        print(f"  Applied training normalization")
    else:
        print("  No normalization applied (no statistics)")
    
    return processed

# ============================================================================
# DATA LOADER
# ============================================================================

def load_fire_event_data(fire_event_path, config, start_day=0):
    """Load fire event data with same preprocessing as test_simulation"""
    
    # Load feature statistics
    feature_stats = load_feature_stats()
    
    try:
        with h5py.File(fire_event_path, 'r') as f:
            # Try different dataset names
            dataset_names = ['sequence', 'data', 'fire_sequence', 'features']
            data = None
            
            for name in dataset_names:
                if name in f:
                    data = f[name][:]
                    print(f"Loaded data from '{name}' dataset")
                    break
            
            if data is None:
                print("Available datasets:", list(f.keys()))
                return None, None, None, 0
            
            print(f"Raw data shape: {data.shape}")
            
            # Convert to tensor
            data_tensor = torch.tensor(data, dtype=torch.float32)
            T, C, H, W = data_tensor.shape
            
            # Extract sequences
            seq_len = config.SEQUENCE_LENGTH
            max_days = min(len(data_tensor) - seq_len - start_day, config.SIMULATION_DAYS)
            
            if max_days <= 0:
                print(f"Not enough data for simulation")
                return None, None, None, 0
            
            # Apply SAME preprocessing as test_simulation
            print("Applying test_simulation preprocessing...")
            
            # Initial sequence (for model input)
            initial_raw = data_tensor[start_day:start_day + seq_len]
            initial_sequence = process_features_like_test_simulation(initial_raw, config, feature_stats)
            
            # Weather data (for simulation continuation)
            weather_raw = data_tensor[start_day:start_day + max_days + seq_len]
            weather_data = process_features_like_test_simulation(weather_raw, config, feature_stats)
            
            # Ground truth (for visualization) - also apply same processing for consistency
            gt_raw = data_tensor[start_day + seq_len:start_day + seq_len + max_days]
            gt_processed = process_features_like_test_simulation(gt_raw, config, feature_stats)
            
            # Extract fire channel from processed ground truth
            ground_truth = []
            ground_truth_raw = []  # ä¿å­˜åŸå§‹è¿ç»­å€¼ç”¨äºæ˜¾ç¤º
            fire_channel_idx = len(config.BEST_FEATURES) - 1  # Active_Fire is last
            for day_idx in range(len(gt_processed)):
                fire_data = gt_processed[day_idx, fire_channel_idx].numpy()
                # ä¿å­˜åŸå§‹è¿ç»­å€¼ç”¨äºGIFæ˜¾ç¤º
                ground_truth_raw.append(fire_data.copy())
                # Apply consistent threshold (0.5) for binary fire detection (ç”¨äºAPè®¡ç®—)
                binary_fire = (fire_data > 0.5).astype(np.float32)
                ground_truth.append(binary_fire)
            
            print(f"Processed sequences: initial={initial_sequence.shape}, weather={weather_data.shape}")
            print(f"Ground truth shape: {len(ground_truth)} days, each {ground_truth[0].shape if ground_truth else 'None'}")
            print(f"Simulation days: {max_days}")
            
            return initial_sequence, weather_data, ground_truth, ground_truth_raw, max_days
            
    except Exception as e:
        print(f"Error loading fire event data: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, 0

# ============================================================================
# GIF GENERATOR
# ============================================================================

def generate_autoregressive_predictions(simulator, initial_seq, weather_data, max_days, 
                                      feature_name, perturbation_levels, fire_event_path, config):
    """
    ç”Ÿæˆé€’å½’é¢„æµ‹ï¼šä½¿ç”¨ä¹‹å‰é¢„æµ‹çš„ç«ç‚¹æ•°æ®è€Œä¸æ˜¯çœŸå®ç«ç‚¹æ•°æ®
    """
    print(f"  Generating AUTOREGRESSIVE predictions (using predicted fire points)...")
    
    autoregressive_predictions = {}
    
    # åˆå§‹åŒ–æ‰€æœ‰æ‰°åŠ¨çº§åˆ«çš„é¢„æµ‹æ•°ç»„
    for perturbation in perturbation_levels:
        autoregressive_predictions[perturbation] = []
    
    # å¼€å§‹é€’å½’é¢„æµ‹è¿‡ç¨‹
    for day in range(max_days):
        print(f"    Autoregressive Day {day+1}/{max_days}")
        
        if day == 0:
            # ç¬¬ä¸€å¤©ï¼šä½¿ç”¨çœŸå®çš„åˆå§‹åºåˆ—
            current_sequences = {}
            for perturbation in perturbation_levels:
                if perturbation == 0:
                    current_sequences[perturbation] = initial_seq.clone()
                else:
                    current_sequences[perturbation] = simulator.apply_feature_perturbation(
                        initial_seq, feature_name, perturbation
                    )
        else:
            # åç»­å¤©æ•°ï¼šæ„å»ºæ–°çš„åºåˆ—ï¼Œç”¨é¢„æµ‹ç«ç‚¹æ›¿ä»£çœŸå®ç«ç‚¹
            new_sequences = {}
            
            for perturbation in perturbation_levels:
                # è·å–ä¸Šä¸€å¤©çš„é¢„æµ‹ç«ç‚¹
                if autoregressive_predictions[perturbation]:
                    last_predicted_fire = torch.tensor(autoregressive_predictions[perturbation][-1])
                else:
                    # å¦‚æœæ²¡æœ‰é¢„æµ‹ï¼Œä½¿ç”¨åˆå§‹åºåˆ—çš„æœ€åä¸€å¸§
                    last_predicted_fire = initial_seq[-1, -1]  # Active_Fireé€šé“
                
                # æ„å»ºæ–°çš„è¾“å…¥åºåˆ—ï¼šæ»‘åŠ¨çª—å£ + é¢„æµ‹ç«ç‚¹
                if day < len(weather_data):
                    # ä½¿ç”¨å¤©æ°”æ•°æ®æ„å»ºåŸºç¡€åºåˆ—
                    new_sequence = weather_data[day-1:day-1+config.SEQUENCE_LENGTH].clone()
                    
                    # å°†æœ€æ–°çš„é¢„æµ‹ç«ç‚¹æ”¾å…¥Active_Fireé€šé“ï¼ˆæœ€åä¸€ä¸ªé€šé“ï¼‰
                    active_fire_idx = len(config.BEST_FEATURES) - 1  # Active_Fireç´¢å¼•
                    new_sequence[-1, active_fire_idx] = last_predicted_fire
                    
                    # åº”ç”¨ç‰¹å¾æ‰°åŠ¨
                    if perturbation != 0:
                        new_sequence = simulator.apply_feature_perturbation(
                            new_sequence, feature_name, perturbation
                        )
                    
                    new_sequences[perturbation] = new_sequence
                else:
                    # å¦‚æœæ²¡æœ‰æ›´å¤šå¤©æ°”æ•°æ®ï¼Œé‡ç”¨å½“å‰åºåˆ—
                    new_sequences[perturbation] = current_sequences[perturbation]
            
            current_sequences = new_sequences
        
        # å¯¹æ‰€æœ‰æ‰°åŠ¨çº§åˆ«è¿›è¡Œé¢„æµ‹
        for perturbation in perturbation_levels:
            if perturbation in current_sequences:
                pred = simulator.predict_single_step(
                    current_sequences[perturbation].unsqueeze(0), debug=False
                )
                autoregressive_predictions[perturbation].append(pred.numpy())
            else:
                # å¦‚æœæ²¡æœ‰åºåˆ—ï¼Œé‡ç”¨æœ€åä¸€ä¸ªé¢„æµ‹
                if autoregressive_predictions[perturbation]:
                    autoregressive_predictions[perturbation].append(
                        autoregressive_predictions[perturbation][-1]
                    )
    
    return autoregressive_predictions

def create_difference_visualization(standard_gif_path, autoregressive_gif_path, feature_name, output_dir):
    """
    åˆ›å»ºæ ‡å‡†é¢„æµ‹ä¸é€’å½’é¢„æµ‹çš„å·®å¼‚å¯è§†åŒ–åˆ†æå›¾
    """
    try:
        print(f"  Creating difference analysis for {feature_name}...")
        
        # æ‰“å¼€GIFæ–‡ä»¶
        img1 = Image.open(standard_gif_path)
        img2 = Image.open(autoregressive_gif_path)
        
        # é€‰æ‹©ç¬¬5å¸§è¿›è¡Œå¯¹æ¯”ï¼ˆé€šå¸¸å·®å¼‚æ¯”è¾ƒæ˜æ˜¾ï¼‰
        frame_idx = min(4, img1.n_frames-1, img2.n_frames-1)
        img1.seek(frame_idx)
        img2.seek(frame_idx)
        
        arr1 = np.array(img1.convert('RGB'))
        arr2 = np.array(img2.convert('RGB'))
        
        # è®¡ç®—å·®å¼‚
        diff = np.abs(arr1.astype(float) - arr2.astype(float))
        diff_gray = np.mean(diff, axis=2)  # è½¬æ¢ä¸ºç°åº¦
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{feature_name} - Standard vs Autoregressive Prediction Difference Analysis (Frame {frame_idx+1})', 
                     fontsize=16, fontweight='bold')
        
        # æ ‡å‡†é¢„æµ‹
        axes[0,0].imshow(arr1)
        axes[0,0].set_title('Standard Prediction (Real Fire Points)', fontsize=12, fontweight='bold')
        axes[0,0].axis('off')
        
        # é€’å½’é¢„æµ‹
        axes[0,1].imshow(arr2)
        axes[0,1].set_title('Autoregressive Prediction (Predicted Fire Points)', fontsize=12, fontweight='bold')
        axes[0,1].axis('off')
        
        # å·®å¼‚çƒ­å›¾
        im = axes[1,0].imshow(diff_gray, cmap='hot', vmin=0, vmax=50)
        axes[1,0].set_title(f'Pixel Difference Heatmap\\n(Max Diff: {diff_gray.max():.1f})', fontsize=12, fontweight='bold')
        axes[1,0].axis('off')
        plt.colorbar(im, ax=axes[1,0], shrink=0.8)
        
        # ç»Ÿè®¡ä¿¡æ¯
        axes[1,1].axis('off')
        stats_text = f'''Difference Statistics (Frame {frame_idx+1}):

Mean Pixel Difference: {np.mean(diff):.2f}
Max Pixel Difference: {np.max(diff):.2f}
Standard Deviation: {np.std(diff):.2f}

Significant Difference Regions:
> 10 diff: {(diff_gray > 10).sum()} pixels
> 20 diff: {(diff_gray > 20).sum()} pixels
> 30 diff: {(diff_gray > 30).sum()} pixels

Total Pixels: {diff_gray.size}
Difference Ratio: {100*(diff_gray > 5).sum()/diff_gray.size:.1f}%'''
        
        axes[1,1].text(0.1, 0.9, stats_text, transform=axes[1,1].transAxes, 
                      fontsize=11, verticalalignment='top', fontfamily='monospace',
                      bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        # ä¿å­˜å¯¹æ¯”å›¾
        output_path = Path(output_dir) / f"{feature_name}_difference_analysis.png"
        plt.tight_layout()
        plt.savefig(str(output_path), dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"    âœ“ Difference analysis saved: {output_path}")
        return True
        
    except Exception as e:
        print(f"    âœ— Failed to create difference analysis: {e}")
        return False

def calculate_cumulative_ap(predictions, targets, current_day):
    """è®¡ç®—åˆ°å½“å‰å¤©ä¸ºæ­¢çš„ç´¯ç§¯AP"""
    if current_day < 0 or current_day >= len(predictions) or current_day >= len(targets):
        return 0.0
    
    # è·å–åˆ°å½“å‰å¤©ä¸ºæ­¢çš„æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡
    cumulative_preds = []
    cumulative_targets = []
    
    for day in range(current_day + 1):
        if day < len(predictions) and day < len(targets):
            pred = predictions[day]
            target = targets[day]
            
            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if isinstance(pred, torch.Tensor):
                pred = pred.numpy()
            if isinstance(target, torch.Tensor):
                target = target.numpy()
            
            cumulative_preds.append(pred.flatten())
            cumulative_targets.append(target.flatten())
    
    if not cumulative_preds or not cumulative_targets:
        return 0.0
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_preds = np.concatenate(cumulative_preds)
    all_targets = np.concatenate(cumulative_targets)
    
    # è®¡ç®—AP
    if all_targets.sum() > 0:
        try:
            return average_precision_score(all_targets, all_preds)
        except:
            return 0.0
    else:
        return 0.0

def create_enhanced_feature_sensitivity_gif(feature_name, output_dir, ground_truth, ground_truth_raw,
                                          baseline_predictions, all_perturbation_predictions, perturbation_levels):
    """Create enhanced sensitivity analysis GIF with multiple perturbation levels and real-time AP display"""
    
    output_path = Path(output_dir) / f"{feature_name}_enhanced_evolution.gif"
    output_path.parent.mkdir(exist_ok=True)
    
    # Create a 3x3 grid: Ground truth + 8 perturbation levels
    fig, axes = plt.subplots(3, 3, figsize=(18, 18))
    fig.suptitle(f'{feature_name} Enhanced Feature Sensitivity Analysis\n26-Day Fire Event Evolution', 
                 fontsize=20, fontweight='bold')
    
    # Flatten axes for easier indexing
    axes_flat = axes.flatten()
    
    # Set up subplot titles
    axes_flat[0].set_title('Ground Truth Fire', fontsize=14, fontweight='bold')
    for i, perturbation in enumerate(perturbation_levels):
        if perturbation == 0:
            title = 'Baseline (0%)'
        else:
            title = f'{feature_name} {perturbation:+d}%'
        axes_flat[i + 1].set_title(title, fontsize=12, fontweight='bold')
    
    def animate(day):
        for ax in axes_flat:
            ax.clear()
        
        try:
            # Ground truth - ä½¿ç”¨åŸå§‹è¿ç»­å€¼æ˜¾ç¤ºæ·±æµ…
            if day < len(ground_truth_raw):
                gt_raw_data = ground_truth_raw[day]  # åŸå§‹è¿ç»­å€¼ç”¨äºæ˜¾ç¤º
                gt_binary_data = ground_truth[day]   # äºŒå€¼åŒ–æ•°æ®ç”¨äºç»Ÿè®¡
                
                if gt_raw_data.ndim > 2:
                    gt_raw_data = gt_raw_data.squeeze()
                if gt_binary_data.ndim > 2:
                    gt_binary_data = gt_binary_data.squeeze()
                
                # ä½¿ç”¨åŸå§‹è¿ç»­å€¼æ˜¾ç¤ºï¼Œé‡‡ç”¨ä¸é¢„æµ‹ç›¸åŒçš„é¢œè‰²æ–¹æ¡ˆ
                axes_flat[0].imshow(gt_raw_data, cmap='Reds', vmin=0, vmax=1)
                axes_flat[0].set_title(f'Ground Truth Fire - Day {day+1}', fontsize=14, fontweight='bold')
                
                # ç»Ÿè®¡ä¿¡æ¯åŸºäºäºŒå€¼åŒ–æ•°æ®ï¼ˆä¿æŒAPè®¡ç®—ä¸€è‡´æ€§ï¼‰
                fire_pixels = (gt_binary_data > 0.5).sum()
                total_pixels = gt_binary_data.size
                fire_ratio = fire_pixels / total_pixels * 100
                
                # æ˜¾ç¤ºåŸå§‹å€¼çš„ç»Ÿè®¡ä¿¡æ¯
                raw_max = gt_raw_data.max()
                raw_mean = gt_raw_data.mean()
                
                stats_text = f'Fire: {fire_pixels}\n({fire_ratio:.1f}%)\nMax: {raw_max:.2f}\nMean: {raw_mean:.3f}'
                axes_flat[0].text(0.95, 0.05, stats_text, 
                                transform=axes_flat[0].transAxes, 
                                bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.8),
                                fontsize=9, ha='right', va='bottom', fontweight='bold')
            else:
                axes_flat[0].set_title('Ground Truth Fire - No Data', fontsize=14)
            axes_flat[0].axis('off')
            
            # Perturbation predictions
            for i, perturbation in enumerate(perturbation_levels):
                ax_idx = i + 1
                predictions = all_perturbation_predictions.get(perturbation, [])
                
                if day < len(predictions):
                    pred_data = predictions[day]
                    if pred_data.ndim > 2:
                        pred_data = pred_data.squeeze()
                    
                    # Use adaptive colormap for better visibility of small differences
                    # Calculate adaptive vmax based on all predictions for this day
                    day_predictions = [all_perturbation_predictions.get(p, [None]*50)[min(day, len(all_perturbation_predictions.get(p, []))-1)] 
                                     for p in perturbation_levels if all_perturbation_predictions.get(p)]
                    day_predictions = [p for p in day_predictions if p is not None]
                    
                    if day_predictions:
                        adaptive_vmax = max(np.max(p) for p in day_predictions) * 1.1  # 10% buffer
                        adaptive_vmax = max(adaptive_vmax, 0.02)  # Minimum vmax for visibility
                    else:
                        adaptive_vmax = 1.0
                    
                    axes_flat[ax_idx].imshow(pred_data, cmap='Oranges', vmin=0, vmax=adaptive_vmax)
                    
                    if perturbation == 0:
                        title = f'Baseline - Day {day+1}'
                    else:
                        title = f'{feature_name} {perturbation:+d}% - Day {day+1}'
                    axes_flat[ax_idx].set_title(title, fontsize=12)
                    
                    # ğŸ†• è®¡ç®—å¹¶æ˜¾ç¤ºç´¯ç§¯AP
                    cumulative_ap = calculate_cumulative_ap(predictions, ground_truth, day)
                    
                    # é¢„æµ‹ç»Ÿè®¡ (ä½¿ç”¨ä¸€è‡´çš„é˜ˆå€¼0.5)
                    pred_pixels = (pred_data > 0.5).sum()
                    pred_max = pred_data.max()
                    
                    # é€‰æ‹©APæ˜¾ç¤ºé¢œè‰²
                    if cumulative_ap > 0.3:
                        ap_color = 'lightgreen'
                    elif cumulative_ap > 0.15:
                        ap_color = 'lightyellow'
                    else:
                        ap_color = 'lightcoral'
                    
                    # æ˜¾ç¤ºå®æ—¶APå’Œé¢„æµ‹ç»Ÿè®¡
                    ap_text = f'AP: {cumulative_ap:.3f}\nPred: {pred_pixels}\nMax: {pred_max:.2f}'
                    axes_flat[ax_idx].text(0.95, 0.05, ap_text,
                                         transform=axes_flat[ax_idx].transAxes,
                                         bbox=dict(boxstyle="round,pad=0.3", facecolor=ap_color, alpha=0.8),
                                         fontsize=9, ha='right', va='bottom', fontweight='bold')
                else:
                    if perturbation == 0:
                        title = 'Baseline - No Data'
                    else:
                        title = f'{feature_name} {perturbation:+d}% - No Data'
                    axes_flat[ax_idx].set_title(title, fontsize=12)
                
                axes_flat[ax_idx].axis('off')
            
        except Exception as e:
            print(f"Animation error day {day}: {e}")
            for ax in axes_flat:
                ax.clear()
                ax.text(0.5, 0.5, f'Error in day {day}', ha='center', va='center',
                       transform=ax.transAxes)
                ax.axis('off')
    
    # Create animation for all days
    max_days = max(len(ground_truth), max(len(preds) for preds in all_perturbation_predictions.values()))
    
    try:
        print(f"Creating animation with {max_days} frames...")
        anim = animation.FuncAnimation(
            fig, animate, frames=max_days, interval=800, repeat=True, blit=False
        )
        
        print(f"Saving enhanced GIF to {output_path}...")
        anim.save(str(output_path), writer='pillow', fps=1.25)
        plt.close(fig)
        
        print(f"âœ“ Enhanced GIF saved: {output_path}")
        return True
        
    except Exception as e:
        print(f"âœ— Error creating enhanced GIF: {e}")
        plt.close(fig)
        return False

def create_feature_sensitivity_gif(feature_name, output_dir, ground_truth, 
                                  baseline_predictions, perturbed_predictions_minus, 
                                  perturbed_predictions_plus):
    """Create the main sensitivity analysis GIF"""
    
    output_path = Path(output_dir) / f"{feature_name}_evolution.gif"
    output_path.parent.mkdir(exist_ok=True)
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle(f'{feature_name} Feature Sensitivity Analysis', fontsize=16, fontweight='bold')
    
    # Subplot titles
    axes[0, 0].set_title('Ground Truth Fire')
    axes[0, 1].set_title('Baseline Prediction')
    axes[1, 0].set_title(f'{feature_name} -20% Effect')
    axes[1, 1].set_title(f'{feature_name} +20% Effect')
    
    def animate(day):
        for ax in axes.flat:
            ax.clear()
        
        try:
            # Ground truth
            if day < len(ground_truth):
                gt_data = ground_truth[day]
                if gt_data.ndim > 2:
                    gt_data = gt_data.squeeze()
                axes[0, 0].imshow(gt_data, cmap='Reds', vmin=0, vmax=1)
                axes[0, 0].set_title(f'Ground Truth Fire - Day {day+1}')
            else:
                axes[0, 0].set_title('Ground Truth Fire - No Data')
            
            # Baseline prediction
            if day < len(baseline_predictions):
                pred_data = baseline_predictions[day]
                if pred_data.ndim > 2:
                    pred_data = pred_data.squeeze()
                axes[0, 1].imshow(pred_data, cmap='Oranges', vmin=0, vmax=1)
                axes[0, 1].set_title(f'Baseline Prediction - Day {day+1}')
            else:
                axes[0, 1].set_title('Baseline Prediction - No Data')
            
            # Perturbed predictions
            if day < len(perturbed_predictions_minus):
                pred_minus = perturbed_predictions_minus[day]
                if pred_minus.ndim > 2:
                    pred_minus = pred_minus.squeeze()
                axes[1, 0].imshow(pred_minus, cmap='Oranges', vmin=0, vmax=1)
                axes[1, 0].set_title(f'{feature_name} -20% - Day {day+1}')
            else:
                axes[1, 0].set_title(f'{feature_name} -20% - No Data')
            
            if day < len(perturbed_predictions_plus):
                pred_plus = perturbed_predictions_plus[day]
                if pred_plus.ndim > 2:
                    pred_plus = pred_plus.squeeze()
                axes[1, 1].imshow(pred_plus, cmap='Oranges', vmin=0, vmax=1)
                axes[1, 1].set_title(f'{feature_name} +20% - Day {day+1}')
            else:
                axes[1, 1].set_title(f'{feature_name} +20% - No Data')
                
        except Exception as e:
            print(f"Error in animation frame {day}: {e}")
        
        # Remove axis ticks
        for ax in axes.flat:
            ax.set_xticks([])
            ax.set_yticks([])
    
    # Create animation
    frames = min(len(ground_truth), len(baseline_predictions), 8)  # Limit frames
    anim = animation.FuncAnimation(fig, animate, frames=frames, interval=1000, repeat=True)
    
    # Save animation
    try:
        anim.save(str(output_path), writer='pillow', fps=1)
        print(f"âœ“ GIF saved: {output_path}")
        return True
    except Exception as e:
        print(f"âœ— Could not save GIF: {e}")
        return False
    finally:
        plt.close(fig)

# ============================================================================
# AP ANALYSIS FUNCTIONS
# ============================================================================

def analyze_fire_no_fire_distribution(targets):
    """åˆ†ææœ‰ç«å¤©å’Œæ— ç«å¤©çš„åˆ†å¸ƒ"""
    fire_days = []
    no_fire_days = []
    
    for day_idx, target in enumerate(targets):
        if isinstance(target, torch.Tensor):
            fire_pixels = (target > 0.5).sum().item()
            total_pixels = target.numel()
        else:
            # numpy array
            fire_pixels = (target > 0.5).sum()
            total_pixels = target.size
        
        fire_ratio = fire_pixels / total_pixels
        
        if fire_pixels > 0:
            fire_days.append({
                'day': day_idx,
                'fire_pixels': fire_pixels,
                'fire_ratio': fire_ratio
            })
        else:
            no_fire_days.append({
                'day': day_idx,
                'fire_pixels': fire_pixels,
                'fire_ratio': fire_ratio
            })
    
    return fire_days, no_fire_days

def calculate_comprehensive_ap_analysis(predictions, targets, scenario_name):
    """
    è®¡ç®—å…¨é¢çš„APåˆ†æï¼ŒåŒ…æ‹¬å¤šç§è®¡ç®—æ–¹å¼
    """
    results = {}
    
    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
    pred_arrays = []
    target_arrays = []
    
    for pred, target in zip(predictions, targets):
        if isinstance(pred, torch.Tensor):
            pred = pred.numpy()
        if isinstance(target, torch.Tensor):
            target = target.numpy()
        
        pred_arrays.append(pred.flatten())
        target_arrays.append(target.flatten())
    
    # åˆ†ææ¯å¤©çš„æƒ…å†µ
    fire_day_predictions = []
    fire_day_targets = []
    no_fire_day_predictions = []
    no_fire_day_targets = []
    daily_aps = []
    
    for day_idx, (pred_flat, target_flat) in enumerate(zip(pred_arrays, target_arrays)):
        fire_pixels = (target_flat > 0.5).sum()
        
        if fire_pixels > 0:  # æœ‰ç«å¤©
            fire_day_predictions.append(pred_flat)
            fire_day_targets.append(target_flat)
            # è®¡ç®—å•å¤©AP
            daily_ap = average_precision_score(target_flat, pred_flat)
            daily_aps.append(daily_ap)
        else:  # æ— ç«å¤©
            no_fire_day_predictions.append(pred_flat)
            no_fire_day_targets.append(target_flat)
            daily_aps.append(0.0)  # æ— ç«å¤©APä¸º0
    
    # æ–¹æ³•1: æ‰€æœ‰å¤©åˆå¹¶è®¡ç®—ï¼ˆæ¨èæ–¹å¼ï¼‰
    all_preds = np.concatenate(pred_arrays)
    all_targets = np.concatenate(target_arrays)
    
    if all_targets.sum() > 0:
        results['combined_ap'] = average_precision_score(all_targets, all_preds)
    else:
        results['combined_ap'] = 0.0
    
    # æ–¹æ³•2: åªè®¡ç®—æœ‰ç«å¤©çš„AP
    if fire_day_predictions:
        fire_preds = np.concatenate(fire_day_predictions)
        fire_targets = np.concatenate(fire_day_targets)
        if fire_targets.sum() > 0:
            results['fire_days_only_ap'] = average_precision_score(fire_targets, fire_preds)
        else:
            results['fire_days_only_ap'] = 0.0
    else:
        results['fire_days_only_ap'] = 0.0
    
    # æ–¹æ³•3: æ¯å¤©å•ç‹¬è®¡ç®—APç„¶åå¹³å‡ï¼ˆåŒ…å«0å€¼ï¼‰
    results['daily_average_ap'] = np.mean(daily_aps)
    
    # ç»Ÿè®¡ä¿¡æ¯
    results['fire_days'] = len(fire_day_predictions)
    results['no_fire_days'] = len(no_fire_day_predictions)
    results['total_days'] = len(predictions)
    results['fire_day_ratio'] = len(fire_day_predictions) / len(predictions) if predictions else 0
    results['daily_aps'] = daily_aps
    
    # è®¡ç®—æ¯”ä¾‹
    if results['fire_days_only_ap'] > 0 and results['combined_ap'] > 0:
        results['fire_to_combined_ratio'] = results['fire_days_only_ap'] / results['combined_ap']
    else:
        results['fire_to_combined_ratio'] = 1.0
    
    return results

def create_feature_ap_summary(feature_name, output_dir, ground_truth, baseline_predictions, perturbation_predictions, perturbation_levels):
    """ä¸ºç‰¹å¾åˆ›å»ºè¯¦ç»†çš„APæ€»ç»“æŠ¥å‘Š"""
    
    print(f"\nğŸ“Š Computing AP analysis for {feature_name}...")
    
    # åˆ†æground truthåˆ†å¸ƒ
    fire_days, no_fire_days = analyze_fire_no_fire_distribution(ground_truth)
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    ap_results = {}
    
    # åˆ†æbaseline
    baseline_analysis = calculate_comprehensive_ap_analysis(
        baseline_predictions, ground_truth, "Baseline"
    )
    ap_results['Baseline (0%)'] = baseline_analysis
    
    # åˆ†ææ¯ä¸ªæ‰°åŠ¨çº§åˆ«
    for perturbation in perturbation_levels:
        if perturbation == 0:
            continue  # å·²ç»å¤„ç†äº†baseline
        
        scenario_name = f"{perturbation:+.0%}"
        if perturbation in perturbation_predictions:
            perturbation_analysis = calculate_comprehensive_ap_analysis(
                perturbation_predictions[perturbation], ground_truth, scenario_name
            )
            ap_results[scenario_name] = perturbation_analysis
    
    # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    
    report_path = output_path / f"{feature_name}_AP_Analysis.json"
    summary_path = output_path / f"{feature_name}_AP_Summary.txt"
    
    # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
    json_data = {
        'feature_name': feature_name,
        'ground_truth_analysis': {
            'fire_days': len(fire_days),
            'no_fire_days': len(no_fire_days),
            'total_days': len(ground_truth),
            'fire_day_ratio': len(fire_days) / len(ground_truth) if ground_truth else 0,
            'fire_day_details': [
                {
                    'day': int(day_info['day']),
                    'fire_pixels': int(day_info['fire_pixels']),
                    'fire_ratio': float(day_info['fire_ratio'])
                } for day_info in fire_days[:5]
            ]
        },
        'ap_results': {}
    }
    
    # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    for scenario, results in ap_results.items():
        json_data['ap_results'][scenario] = {
            'combined_ap': float(results['combined_ap']),
            'fire_days_only_ap': float(results['fire_days_only_ap']),
            'daily_average_ap': float(results['daily_average_ap']),
            'fire_days': int(results['fire_days']),
            'no_fire_days': int(results['no_fire_days']),
            'total_days': int(results['total_days']),
            'fire_day_ratio': float(results['fire_day_ratio']),
            'fire_to_combined_ratio': float(results['fire_to_combined_ratio'])
        }
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºäººç±»å¯è¯»çš„æ€»ç»“æŠ¥å‘Š
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(f"ğŸ”¥ {feature_name} FEATURE SENSITIVITY - AP ANALYSIS REPORT\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("ğŸ“Š GROUND TRUTH DATA DISTRIBUTION\n")
        f.write("-" * 40 + "\n")
        f.write(f"â€¢ Total days analyzed: {len(ground_truth)}\n")
        f.write(f"â€¢ Days with fire: {len(fire_days)} ({len(fire_days)/len(ground_truth)*100:.1f}%)\n")
        f.write(f"â€¢ Days without fire: {len(no_fire_days)} ({len(no_fire_days)/len(ground_truth)*100:.1f}%)\n\n")
        
        if fire_days:
            f.write("ğŸ”¥ FIRE DAYS DETAILS:\n")
            for day_info in fire_days[:10]:  # æ˜¾ç¤ºå‰10å¤©
                f.write(f"  Day {day_info['day']+1}: {day_info['fire_pixels']} pixels ({day_info['fire_ratio']*100:.3f}%)\n")
            if len(fire_days) > 10:
                f.write(f"  ... and {len(fire_days)-10} more fire days\n")
            f.write("\n")
        
        f.write("ğŸ“ˆ AVERAGE PRECISION (AP) ANALYSIS\n")
        f.write("-" * 40 + "\n")
        f.write("Legend:\n")
        f.write("â€¢ Combined AP: All days merged (RECOMMENDED method)\n")
        f.write("â€¢ Fire-only AP: Only days with actual fire\n")
        f.write("â€¢ Daily Average: Average of individual daily APs (includes 0s)\n\n")
        
        # æŒ‰APåˆ†æ•°æ’åº
        sorted_results = sorted(ap_results.items(), 
                              key=lambda x: x[1]['combined_ap'], 
                              reverse=True)
        
        f.write(f"{'Scenario':<15} {'Combined AP':<12} {'Fire-only AP':<13} {'Daily Avg':<11} {'Ratio':<8}\n")
        f.write("-" * 70 + "\n")
        
        for scenario, results in sorted_results:
            f.write(f"{scenario:<15} ")
            f.write(f"{results['combined_ap']:<12.4f} ")
            f.write(f"{results['fire_days_only_ap']:<13.4f} ")
            f.write(f"{results['daily_average_ap']:<11.4f} ")
            f.write(f"{results['fire_to_combined_ratio']:<8.2f}\n")
        
        f.write("\n")
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®çš„æ‰°åŠ¨
        best_scenario = max(ap_results.items(), key=lambda x: x[1]['combined_ap'])
        worst_scenario = min(ap_results.items(), key=lambda x: x[1]['combined_ap'])
        
        f.write("ğŸ¯ KEY INSIGHTS\n")
        f.write("-" * 40 + "\n")
        f.write(f"â€¢ Best performing scenario: {best_scenario[0]} (AP: {best_scenario[1]['combined_ap']:.4f})\n")
        f.write(f"â€¢ Worst performing scenario: {worst_scenario[0]} (AP: {worst_scenario[1]['combined_ap']:.4f})\n")
        
        baseline_ap = ap_results.get('Baseline (0%)', {}).get('combined_ap', 0)
        if baseline_ap > 0:
            best_improvement = (best_scenario[1]['combined_ap'] - baseline_ap) / baseline_ap * 100
            worst_degradation = (worst_scenario[1]['combined_ap'] - baseline_ap) / baseline_ap * 100
            f.write(f"â€¢ Best improvement over baseline: {best_improvement:+.1f}%\n")
            f.write(f"â€¢ Worst degradation from baseline: {worst_degradation:+.1f}%\n")
        
        f.write("\nğŸ’¡ METHODOLOGY NOTES\n")
        f.write("-" * 40 + "\n")
        f.write("â€¢ Combined AP is the most reliable metric (merges all predictions/targets)\n")
        f.write("â€¢ Fire-only AP shows performance on fire-active days only\n")
        f.write("â€¢ Daily Average includes 0 AP from no-fire days (may underestimate performance)\n")
        f.write("â€¢ Ratio shows how much no-fire days affect the combined score\n")
        
        f.write(f"\nğŸ“ Detailed data saved to: {report_path.name}\n")
    
    print(f"âœ… {feature_name} AP analysis completed!")
    print(f"   ğŸ“Š Summary: {summary_path}")
    print(f"   ğŸ“‹ Detailed: {report_path}")
    
    # æ‰“å°å…³é”®ç»“æœåˆ°æ§åˆ¶å°
    print(f"\nğŸ¯ {feature_name} QUICK RESULTS:")
    baseline_ap = ap_results.get('Baseline (0%)', {}).get('combined_ap', 0)
    print(f"   Baseline AP: {baseline_ap:.4f}")
    
    for scenario, results in sorted_results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæœ€ä½³ç»“æœ
        if scenario != 'Baseline (0%)':
            improvement = (results['combined_ap'] - baseline_ap) / baseline_ap * 100 if baseline_ap > 0 else 0
            print(f"   {scenario}: {results['combined_ap']:.4f} ({improvement:+.1f}%)")
    
    return ap_results

# ============================================================================
# MAIN ANALYSIS FUNCTION
# ============================================================================

def run_simple_sensitivity_analysis(model_path, fire_event_path, output_dir='simple_sensitivity_results'):
    """Run simplified sensitivity analysis focused on GIF generation"""
    
    config = SimpleConfig()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    
    # Load model
    print("Loading model...")
    model = load_model_with_compatibility(model_path, len(config.BEST_FEATURES), config.SEQUENCE_LENGTH, device)
    if model is None:
        print("Failed to load model")
        return
    
    # Initialize simulator
    simulator = SimpleFireSimulator(model, config, device)
    
    # Load data
    print("Loading fire event data...")
    initial_seq, weather_data, ground_truth, ground_truth_raw, max_days = load_fire_event_data(
        fire_event_path, config, start_day=0
    )
    
    if initial_seq is None:
        print("Failed to load fire event data")
        return
    
    # Run baseline prediction
    print("Running baseline prediction...")
    # Use EVOLUTION APPROACH: different sequences for each day like create_evolution_animation
    baseline_predictions = []
    for day in range(max_days):
        # Load fresh sequence for this day (like evolution animation)
        day_seq, _, _, _, _ = load_fire_event_data(
            fire_event_path, config, start_day=day
        )
        if day_seq is not None:
            raw_pred = simulator.predict_single_step(day_seq.unsqueeze(0), debug=False)
            baseline_predictions.append(raw_pred.numpy())  # RAW sigmoid predictions
        else:
            # If no more data, reuse last prediction
            if baseline_predictions:
                baseline_predictions.append(baseline_predictions[-1])
            else:
                # Fallback to original sequence
                raw_pred = simulator.predict_single_step(initial_seq.unsqueeze(0), debug=False)
                baseline_predictions.append(raw_pred.numpy())
    
    # Features to analyze - COMPLETE ANALYSIS
    important_features = [
        # Vegetation indices (æ¤è¢«æŒ‡æ•°)
        'NDVI', 'EVI2',
        # Weather conditions (æ°”è±¡æ¡ä»¶) 
        'Max_Temp_K', 'Min_Temp_K', 'Total_Precip',
        # Satellite data (å«æ˜Ÿæ•°æ®)
        'VIIRS_M11', 'VIIRS_I2', 'VIIRS_I1',
        # Topographic features (åœ°å½¢ç‰¹å¾)
        'Elevation', 'Slope', 'Aspect'
    ]
    
    print(f"\nGenerating sensitivity GIFs for: {important_features}")
    
    for feature_name in important_features:
        print(f"\n{'='*50}")
        print(f"ANALYZING {feature_name}")
        print(f"{'='*50}")
        
        try:
            # FIXED: Reorganize loops to avoid excessive data loading
            print(f"Generating {feature_name} perturbations: {config.PERTURBATION_LEVELS}")
            all_perturbation_predictions = {}
            
            # Initialize prediction arrays for all perturbation levels
            for perturbation in config.PERTURBATION_LEVELS:
                all_perturbation_predictions[perturbation] = []
            
            # Load data once per day and apply all perturbations
            print(f"Processing {max_days} days with {len(config.PERTURBATION_LEVELS)} perturbation levels...")
            for day in range(max_days):
                # Load fresh sequence for this day (like evolution) - ONLY ONCE PER DAY
                day_seq, _, _, _, _ = load_fire_event_data(
                    fire_event_path, config, start_day=day
                )
                
                if day_seq is not None:
                    # Apply all perturbations to this day's sequence
                    for perturbation in config.PERTURBATION_LEVELS:
                        if perturbation == 0:
                            # Baseline - no perturbation
                            perturbed_seq = day_seq
                        else:
                            # Apply perturbation to this day's sequence
                            perturbed_seq = simulator.apply_feature_perturbation(
                                day_seq, feature_name, perturbation
                            )
                        
                        # Get raw predictions (no physics)
                        pred = simulator.predict_single_step(perturbed_seq.unsqueeze(0), debug=False)
                        all_perturbation_predictions[perturbation].append(pred.numpy())
                else:
                    # Fallback: reuse last prediction or use baseline sequence
                    for perturbation in config.PERTURBATION_LEVELS:
                        if all_perturbation_predictions[perturbation]:
                            # Reuse last prediction
                            all_perturbation_predictions[perturbation].append(
                                all_perturbation_predictions[perturbation][-1]
                            )
                        else:
                            # Use baseline sequence as fallback
                            if perturbation == 0:
                                perturbed_seq = initial_seq
                            else:
                                perturbed_seq = simulator.apply_feature_perturbation(
                                    initial_seq, feature_name, perturbation
                                )
                            pred = simulator.predict_single_step(perturbed_seq.unsqueeze(0), debug=False)
                            all_perturbation_predictions[perturbation].append(pred.numpy())
            
            # Create enhanced GIF with multiple perturbation levels
            print(f"Creating {feature_name} enhanced sensitivity GIF...")
            success = create_enhanced_feature_sensitivity_gif(
                feature_name, output_dir, ground_truth, ground_truth_raw,
                baseline_predictions, all_perturbation_predictions, config.PERTURBATION_LEVELS
            )
            
            # Create AUTOREGRESSIVE GIF (using predicted fire points instead of real ones)
            print(f"Creating {feature_name} AUTOREGRESSIVE sensitivity GIF...")
            autoregressive_predictions = generate_autoregressive_predictions(
                simulator, initial_seq, weather_data, max_days, feature_name, 
                config.PERTURBATION_LEVELS, fire_event_path, config
            )
            
            success_auto = create_enhanced_feature_sensitivity_gif(
                f"{feature_name}_AUTOREGRESSIVE", output_dir, ground_truth, ground_truth_raw,
                autoregressive_predictions.get(0, baseline_predictions), 
                autoregressive_predictions, config.PERTURBATION_LEVELS
            )
            
            # Create difference analysis visualization
            if success and success_auto:
                standard_gif_path = Path(output_dir) / f"{feature_name}_enhanced_evolution.gif"
                autoregressive_gif_path = Path(output_dir) / f"{feature_name}_AUTOREGRESSIVE_enhanced_evolution.gif"
                
                difference_success = create_difference_visualization(
                    str(standard_gif_path), str(autoregressive_gif_path), 
                    feature_name, output_dir
                )
                
                if difference_success:
                    print(f"âœ“ {feature_name} complete analysis (GIFs + Difference) finished!")
                else:
                    print(f"âœ“ {feature_name} GIFs created, but difference analysis failed")
            else:
                print(f"âœ— {feature_name} GIF generation failed")
            
            # ğŸ†• ADD AP ANALYSIS
            print(f"\nğŸ“Š Computing AP analysis for {feature_name}...")
            try:
                ap_results = create_feature_ap_summary(
                    feature_name, output_dir, ground_truth, 
                    baseline_predictions, all_perturbation_predictions, 
                    config.PERTURBATION_LEVELS
                )
                print(f"âœ… {feature_name} AP analysis completed successfully!")
            except Exception as ap_error:
                print(f"âš ï¸ {feature_name} AP analysis failed: {ap_error}")
                
        except Exception as e:
            print(f"âœ— Error analyzing {feature_name}: {e}")
            continue
    
    print(f"\n{'='*60}")
    print("ENHANCED SENSITIVITY ANALYSIS COMPLETED")
    print(f"{'='*60}")
    print(f"Results saved in: {output_dir}")
    print("\nğŸ¬ Generated COMPLETE ANALYSIS for each feature:")
    print("\n1. STANDARD GIFs (using real fire points):")
    print("   - 3x3 grid with ground truth + 8 perturbation levels")
    print("   - Uses real historical fire data for predictions")
    print("   - Shows immediate feature sensitivity effects")
    
    print("\n2. AUTOREGRESSIVE GIFs (using predicted fire points):")
    print("   - Same 3x3 grid layout")
    print("   - Uses PREVIOUS PREDICTIONS as fire input")
    print("   - Shows cumulative prediction errors and amplified sensitivity")
    print("   - Reveals how model predictions evolve recursively")
    
    print("\n3. DIFFERENCE ANALYSIS IMAGES (PNG):")
    print("   - Side-by-side comparison of standard vs autoregressive")
    print("   - Pixel-level difference heatmap")
    print("   - Statistical analysis of differences")
    print("   - Automatically generated for visual verification")
    
    print(f"\nğŸ“Š Analysis parameters:")
    print(f"   - Perturbation levels: {config.PERTURBATION_LEVELS}")
    print(f"   - Time period: {config.SIMULATION_DAYS} days")
    print(f"   - Features analyzed: {important_features}")
    
    print("\nğŸ¯ Generated files for each feature:")
    print("   - {feature}_enhanced_evolution.gif (Standard prediction)")
    print("   - {feature}_AUTOREGRESSIVE_enhanced_evolution.gif (Recursive prediction)")
    print("   - {feature}_difference_analysis.png (Difference visualization)")
    print("   - {feature}_AP_Summary.txt (Human-readable AP analysis)")
    print("   - {feature}_AP_Analysis.json (Detailed AP data)")
    
    print("\nğŸ“ˆ Key insights from complete analysis:")
    print("   - Standard GIFs: Feature effects with perfect fire history")
    print("   - Autoregressive GIFs: Feature effects with prediction uncertainty")
    print("   - Difference PNGs: Quantified visual differences and statistics")
    print("   - AP Analysis: Quantitative performance metrics for all perturbations")
    print("   - Combined analysis reveals model's recursive prediction stability!")
    
    print("\nğŸ“Š AP ANALYSIS HIGHLIGHTS:")
    print("   - Uses CORRECT AP calculation (combined method - not simple averaging)")
    print("   - Separates fire days vs no-fire days analysis")
    print("   - Shows which perturbations improve/degrade model performance")
    print("   - Includes statistical significance and improvement percentages")
    
    print(f"\nâœ… Complete analysis finished! Check all generated files for comprehensive insights!")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    import argparse
    import sys
    
    # If no arguments provided, use default paths
    if len(sys.argv) == 1:
        # Default paths for direct execution
        model_path = "best_fire_model_official.pth"
        fire_event_path = "data/processed/2020/fire_24461899.hdf5"
        
        if os.path.exists(model_path) and os.path.exists(fire_event_path):
            print("Running simple sensitivity analysis with default paths...")
            print(f"Model: {model_path}")
            print(f"Fire event: {fire_event_path}")
            run_simple_sensitivity_analysis(model_path, fire_event_path)
        else:
            print("Default files not found. Available files:")
            if os.path.exists("backup/fixed_wildfire_outputs/"):
                print("Models found:")
                for f in os.listdir("backup/fixed_wildfire_outputs/"):
                    if f.endswith(('.pth', '.pt', '.ckpt')):
                        print(f"  - backup/fixed_wildfire_outputs/{f}")
            
            if os.path.exists("data/processed/"):
                print("Fire events found:")
                for year_dir in os.listdir("data/processed/"):
                    year_path = os.path.join("data/processed/", year_dir)
                    if os.path.isdir(year_path):
                        for f in os.listdir(year_path):
                            if f.endswith(('.h5', '.hdf5')):
                                print(f"  - data/processed/{year_dir}/{f}")
            
            print("\nPlease run with:")
            print("python simple_feature_sensitivity.py --model <model_path> --fire_event <data_path>")
    else:
        # Use command line arguments
        parser = argparse.ArgumentParser(description='Simple Feature Sensitivity Analysis - GIF Focus')
        parser.add_argument('--model', required=True, help='Path to trained model')
        parser.add_argument('--fire_event', required=True, help='Path to fire event HDF5 file')
        parser.add_argument('--output_dir', default='simple_sensitivity_results', help='Output directory')
        
        args = parser.parse_args()
        run_simple_sensitivity_analysis(args.model, args.fire_event, args.output_dir)

if __name__ == "__main__":
    main()
