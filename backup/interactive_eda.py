#!/usr/bin/env python3
"""
WildfireSpreadTS äº¤äº’å¼EDAåˆ†æè„šæœ¬

è¿™ä¸ªè„šæœ¬æä¾›äº†å®Œæ•´çš„æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ŒåŒ…å«æ‰€æœ‰å››å¹´çš„æ•°æ®(2018-2021)
å¯ä»¥é€æ­¥è¿è¡Œæ¯ä¸ªåˆ†ææ¨¡å—ï¼Œå¹¶åœ¨æ¯æ­¥æ˜¾ç¤ºå›¾è¡¨å’Œç»“æœã€‚

ä½¿ç”¨æ–¹æ³•:
    python interactive_eda.py

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-01-30
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import h5py
from pathlib import Path
import warnings
import json
from datetime import datetime
from scipy import stats
from sklearn.feature_selection import mutual_info_regression
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
import random

# è®¾ç½®
plt.style.use('default')
sns.set_palette("husl")
warnings.filterwarnings('ignore')
np.random.seed(42)
random.seed(42)

class WildfireEDAAnalyzer:
    """WildfireSpreadTSæ•°æ®é›†EDAåˆ†æå™¨"""
    
    def __init__(self, data_dir="data/processed", output_dir="eda_results_interactive"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "figures").mkdir(exist_ok=True)
        (self.output_dir / "tables").mkdir(exist_ok=True)
        
        self.data = None
        self.metadata = None
        self.fire_events = None
        self.feature_schema = self.create_feature_schema()
        
        print("ğŸ”¥ WildfireSpreadTS äº¤äº’å¼EDAåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ!")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir.absolute()}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir.absolute()}")
    
    def create_feature_schema(self):
        """å®šä¹‰23é€šé“ç‰¹å¾æ¶æ„"""
        schema = {
            0: {'name': 'NDVI', 'category': 'Vegetation', 'unit': 'Index (-1 to 1)', 'expected_range': [-1, 1]},
            1: {'name': 'Precipitation', 'category': 'Weather', 'unit': 'mm', 'expected_range': [0, 100]},
            2: {'name': 'Temperature', 'category': 'Weather', 'unit': 'Â°C', 'expected_range': [-20, 50]},
            3: {'name': 'Relative_Humidity', 'category': 'Weather', 'unit': '%', 'expected_range': [0, 100]},
            4: {'name': 'Specific_Humidity', 'category': 'Weather', 'unit': 'kg/kg', 'expected_range': [0, 0.03]},
            5: {'name': 'Surface_Pressure', 'category': 'Weather', 'unit': 'Pa', 'expected_range': [80000, 105000]},
            6: {'name': 'Wind_Speed', 'category': 'Weather', 'unit': 'm/s', 'expected_range': [0, 50]},
            7: {'name': 'Wind_Direction', 'category': 'Weather', 'unit': 'degrees', 'expected_range': [0, 360]},
            8: {'name': 'Elevation', 'category': 'Topography', 'unit': 'm', 'expected_range': [0, 4000]},
            9: {'name': 'Slope', 'category': 'Topography', 'unit': 'degrees', 'expected_range': [0, 90]},
            10: {'name': 'Aspect', 'category': 'Topography', 'unit': 'degrees', 'expected_range': [0, 360]},
            11: {'name': 'Population_Density', 'category': 'Human', 'unit': 'people/kmÂ²', 'expected_range': [0, 10000]},
            12: {'name': 'Burned_Area_Previous_Year', 'category': 'Fire History', 'unit': 'fraction', 'expected_range': [0, 1]},
            13: {'name': 'Drought_Code', 'category': 'Fire Weather', 'unit': 'Index', 'expected_range': [0, 1000]},
            14: {'name': 'Fuel_Moisture_1000hr', 'category': 'Fuel', 'unit': '%', 'expected_range': [0, 50]},
            15: {'name': 'Energy_Release_Component', 'category': 'Fire Weather', 'unit': 'Index', 'expected_range': [0, 200]},
            16: {'name': 'Land_Cover_Class', 'category': 'Land Cover', 'unit': 'Class ID (1-16)', 'expected_range': [1, 16]},
            17: {'name': 'Forecast_Precipitation', 'category': 'Forecast', 'unit': 'mm', 'expected_range': [0, 100]},
            18: {'name': 'Forecast_Temperature', 'category': 'Forecast', 'unit': 'Â°C', 'expected_range': [-20, 50]},
            19: {'name': 'Forecast_Humidity', 'category': 'Forecast', 'unit': '%', 'expected_range': [0, 100]},
            20: {'name': 'Forecast_Wind_Speed', 'category': 'Forecast', 'unit': 'm/s', 'expected_range': [0, 50]},
            21: {'name': 'Forecast_Wind_Direction', 'category': 'Forecast', 'unit': 'degrees', 'expected_range': [0, 360]},
            22: {'name': 'Active_Fire_Confidence', 'category': 'Fire Detection', 'unit': 'Confidence (0-100)', 'expected_range': [0, 100]}
        }
        return schema
    
    def load_sample_data(self, max_files=40, sample_ratio=0.1):
        """åŠ è½½æ ·æœ¬æ•°æ®è¿›è¡Œåˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ“¦ 1. æ•°æ®åŠ è½½ä¸åŸºæœ¬ä¿¡æ¯")
        print("="*60)
        print("ğŸ” å¼€å§‹åŠ è½½æ•°æ®æ ·æœ¬...")
        
        # æŒ‰å¹´ä»½åˆ†ç»„æŸ¥æ‰¾HDF5æ–‡ä»¶
        hdf5_files_by_year = {}
        for year in ['2018', '2019', '2020', '2021']:
            year_dir = self.data_dir / year
            if year_dir.exists():
                year_files = list(year_dir.glob("*.hdf5"))
                hdf5_files_by_year[year] = year_files
                print(f"  {year}å¹´: {len(year_files)} ä¸ªæ–‡ä»¶")
        
        total_files = sum(len(files) for files in hdf5_files_by_year.values())
        if total_files == 0:
            raise FileNotFoundError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°HDF5æ–‡ä»¶")
        
        print(f"ğŸ“Š æ€»è®¡æ‰¾åˆ° {total_files} ä¸ªHDF5æ–‡ä»¶")
        
        # ä»æ¯å¹´å‡åŒ€é‡‡æ ·æ–‡ä»¶
        files_to_process = []
        files_per_year = max_files // len(hdf5_files_by_year)
        remainder = max_files % len(hdf5_files_by_year)
        
        for i, (year, year_files) in enumerate(hdf5_files_by_year.items()):
            if year_files:
                n_files = files_per_year + (1 if i < remainder else 0)
                n_files = min(n_files, len(year_files))
                
                sampled_files = random.sample(year_files, n_files) if n_files < len(year_files) else year_files
                files_to_process.extend(sampled_files)
                print(f"  ä»{year}å¹´é‡‡æ · {len(sampled_files)} ä¸ªæ–‡ä»¶")
        
        print(f"ğŸ¯ æ€»å…±å°†å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶")
        
        all_data = []
        metadata = []
        fire_events = []
        
        for i, file_path in enumerate(files_to_process):
            print(f"\r  å¤„ç†è¿›åº¦: {i+1}/{len(files_to_process)} ({(i+1)/len(files_to_process)*100:.1f}%)", end="")
            
            year = file_path.parent.name
            
            try:
                with h5py.File(file_path, 'r') as f:
                    data = f['data'][:]  # Shape: (T, C, H, W)
                    
                    fire_name = f['data'].attrs.get('fire_name', file_path.stem)
                    if isinstance(fire_name, bytes):
                        fire_name = fire_name.decode('utf-8')
                    elif isinstance(fire_name, np.ndarray):
                        fire_name = str(fire_name)
                    
                    n_timesteps = data.shape[0]
                    n_samples = max(1, int(n_timesteps * sample_ratio))
                    
                    if n_samples < n_timesteps:
                        sample_indices = sorted(random.sample(range(n_timesteps), n_samples))
                    else:
                        sample_indices = list(range(n_timesteps))
                    
                    for t_idx in sample_indices:
                        timestep_data = data[t_idx]  # (C, H, W)
                        flattened = timestep_data.reshape(timestep_data.shape[0], -1).T  # (H*W, C)
                        all_data.append(flattened)
                        
                        for spatial_idx in range(flattened.shape[0]):
                            metadata.append({
                                'year': year,
                                'fire_event': fire_name,
                                'file_path': str(file_path),
                                'timestep': t_idx,
                                'spatial_idx': spatial_idx
                            })
                    
                    fire_events.append(fire_name)
                    
            except Exception as e:
                print(f"\n  å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        print("\nâœ… æ•°æ®åŠ è½½å®Œæˆ!")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        self.data = np.vstack(all_data)
        self.metadata = metadata
        self.fire_events = fire_events
        
        print(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"  - æ•°æ®å½¢çŠ¶: {self.data.shape}")
        print(f"  - ç«ç¾äº‹ä»¶æ•°: {len(set(fire_events))}")
        print(f"  - æ—¶é—´è·¨åº¦: {sorted(set(m['year'] for m in metadata))}")
        
        return self.data, self.metadata, self.fire_events
    
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        print("\n" + "="*60)
        print("ğŸ” 2. æ•°æ®è´¨é‡ä¸å®Œæ•´æ€§åˆ†æ")
        print("="*60)
        
        if self.data is None:
            print("âš ï¸ è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        results = {}
        n_samples, n_features = self.data.shape
        
        # åŸºæœ¬ä¿¡æ¯
        results['basic_info'] = {
            'total_samples': n_samples,
            'n_features': n_features,
            'n_fire_events': len(set(self.fire_events)),
            'years_covered': sorted(set(m['year'] for m in self.metadata))
        }
        
        # ç¼ºå¤±å€¼å’Œæ— ç©·å€¼åˆ†æ
        missing_stats = {}
        infinite_stats = {}
        
        for i in range(n_features):
            feature_name = self.feature_schema[i]['name']
            feature_data = self.data[:, i]
            
            n_missing = np.isnan(feature_data).sum()
            missing_pct = (n_missing / n_samples) * 100
            missing_stats[feature_name] = {'count': n_missing, 'percentage': missing_pct}
            
            n_infinite = np.isinf(feature_data).sum()
            infinite_pct = (n_infinite / n_samples) * 100
            infinite_stats[feature_name] = {'count': n_infinite, 'percentage': infinite_pct}
        
        results['missing_values'] = missing_stats
        results['infinite_values'] = infinite_stats
        
        # æ•°æ®èŒƒå›´åˆ†æ
        range_analysis = {}
        for i in range(n_features):
            feature_name = self.feature_schema[i]['name']
            feature_data = self.data[:, i]
            valid_data = feature_data[~np.isnan(feature_data) & ~np.isinf(feature_data)]
            
            if len(valid_data) > 0:
                range_analysis[feature_name] = {
                    'min': float(valid_data.min()),
                    'max': float(valid_data.max()),
                    'mean': float(valid_data.mean()),
                    'std': float(valid_data.std()),
                    'expected_range': self.feature_schema[i]['expected_range']
                }
        
        results['range_analysis'] = range_analysis
        
        # å±•ç¤ºç»“æœ
        print("ğŸ“Š æ•°æ®è´¨é‡åˆ†æç»“æœ:")
        print(f"  æ ·æœ¬æ€»æ•°: {results['basic_info']['total_samples']:,}")
        print(f"  ç‰¹å¾æ•°é‡: {results['basic_info']['n_features']}")
        print(f"  ç«ç¾äº‹ä»¶: {results['basic_info']['n_fire_events']} ä¸ª")
        print(f"  æ—¶é—´è¦†ç›–: {'-'.join(results['basic_info']['years_covered'])}")
        
        print("\nğŸš« ç¼ºå¤±å€¼åˆ†æ:")
        missing_sorted = sorted(missing_stats.items(), key=lambda x: x[1]['percentage'], reverse=True)
        has_missing = False
        for feature, stats in missing_sorted[:10]:
            if stats['percentage'] > 0:
                print(f"  {feature}: {stats['count']:,} ({stats['percentage']:.2f}%)")
                has_missing = True
        if not has_missing:
            print("  âœ… æœªå‘ç°ç¼ºå¤±å€¼")
        
        print("\nâ™¾ï¸ æ— ç©·å€¼åˆ†æ:")
        has_infinite = [(f, s) for f, s in infinite_stats.items() if s['count'] > 0]
        if has_infinite:
            for feature, stats in has_infinite[:5]:
                print(f"  {feature}: {stats['count']:,} ({stats['percentage']:.2f}%)")
        else:
            print("  âœ… æœªå‘ç°æ— ç©·å€¼")
        
        # å¯è§†åŒ–æ•°æ®è´¨é‡
        self.plot_data_quality(results, missing_sorted)
        
        return results
    
    def plot_data_quality(self, results, missing_sorted):
        """ç»˜åˆ¶æ•°æ®è´¨é‡å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. ç¼ºå¤±å€¼åˆ†æ
        missing_data = []
        missing_labels = []
        for feature, stats in missing_sorted:
            if stats['percentage'] > 0:
                missing_data.append(stats['percentage'])
                missing_labels.append(feature[:20])  # æˆªæ–­é•¿åç§°
        
        if missing_data:
            axes[0,0].barh(missing_labels[:10], missing_data[:10])
            axes[0,0].set_xlabel('ç¼ºå¤±å€¼ç™¾åˆ†æ¯” (%)')
            axes[0,0].set_title('ç¼ºå¤±å€¼åˆ†æ (Top 10)')
            axes[0,0].grid(True, alpha=0.3)
        else:
            axes[0,0].text(0.5, 0.5, 'âœ… æ— ç¼ºå¤±å€¼', ha='center', va='center', 
                           transform=axes[0,0].transAxes, fontsize=14)
            axes[0,0].set_title('ç¼ºå¤±å€¼åˆ†æ')
        
        # 2. ç‰¹å¾æ•°å€¼èŒƒå›´
        feature_ranges = []
        feature_names = []
        for i in range(min(10, len(self.feature_schema))):
            feature_name = self.feature_schema[i]['name']
            if feature_name in results['range_analysis']:
                range_info = results['range_analysis'][feature_name]
                feature_ranges.append([range_info['min'], range_info['max']])
                feature_names.append(feature_name[:15])
        
        if feature_ranges:
            ranges_array = np.array(feature_ranges)
            axes[0,1].barh(range(len(feature_names)), ranges_array[:, 1] - ranges_array[:, 0])
            axes[0,1].set_yticks(range(len(feature_names)))
            axes[0,1].set_yticklabels(feature_names)
            axes[0,1].set_xlabel('æ•°å€¼èŒƒå›´ (max - min)')
            axes[0,1].set_title('ç‰¹å¾æ•°å€¼èŒƒå›´')
            axes[0,1].grid(True, alpha=0.3)
        
        # 3. ç«ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ
        fire_confidence = self.data[:, 22]  # Active Fire Confidence
        valid_confidence = fire_confidence[~np.isnan(fire_confidence)]
        axes[1,0].hist(valid_confidence, bins=50, alpha=0.7, edgecolor='black')
        axes[1,0].set_xlabel('ç«ç‚¹ç½®ä¿¡åº¦')
        axes[1,0].set_ylabel('é¢‘æ¬¡')
        axes[1,0].set_title('ç«ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        axes[1,0].grid(True, alpha=0.3)
        axes[1,0].set_yscale('log')  # ä½¿ç”¨å¯¹æ•°å°ºåº¦
        
        # 4. å¹´ä»½åˆ†å¸ƒ
        year_counts = {}
        for year in results['basic_info']['years_covered']:
            year_counts[year] = sum(1 for m in self.metadata if m['year'] == year)
        
        years = list(year_counts.keys())
        counts = list(year_counts.values())
        axes[1,1].bar(years, counts, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
        axes[1,1].set_xlabel('å¹´ä»½')
        axes[1,1].set_ylabel('æ ·æœ¬æ•°é‡')
        axes[1,1].set_title('å¹´ä»½åˆ†å¸ƒ')
        axes[1,1].grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(counts):
            axes[1,1].text(i, v + max(counts) * 0.01, f'{v:,}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'figures' / 'data_quality_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"\nğŸ’¾ å›¾è¡¨å·²ä¿å­˜åˆ°: {self.output_dir / 'figures' / 'data_quality_analysis.png'}")
    
    def analyze_descriptive_statistics(self):
        """è®¡ç®—å¹¶åˆ†ææè¿°æ€§ç»Ÿè®¡"""
        print("\n" + "="*60)
        print("ğŸ“Š 3. æè¿°æ€§ç»Ÿè®¡åˆ†æ")
        print("="*60)
        
        if self.data is None:
            print("âš ï¸ è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        stats_results = {}
        n_samples, n_features = self.data.shape
        
        for i in range(n_features):
            feature_name = self.feature_schema[i]['name']
            feature_data = self.data[:, i]
            valid_data = feature_data[~np.isnan(feature_data) & ~np.isinf(feature_data)]
            
            if len(valid_data) > 0:
                # åŸºç¡€ç»Ÿè®¡
                stats_dict = {
                    'count': len(valid_data),
                    'mean': np.mean(valid_data),
                    'std': np.std(valid_data),
                    'min': np.min(valid_data),
                    'max': np.max(valid_data),
                    'median': np.median(valid_data),
                    'q25': np.percentile(valid_data, 25),
                    'q75': np.percentile(valid_data, 75),
                    'iqr': np.percentile(valid_data, 75) - np.percentile(valid_data, 25),
                    'skewness': stats.skew(valid_data),
                    'kurtosis': stats.kurtosis(valid_data)
                }
                
                # å¼‚å¸¸å€¼æ£€æµ‹ (IQRæ–¹æ³•)
                Q1, Q3 = stats_dict['q25'], stats_dict['q75']
                IQR = stats_dict['iqr']
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = valid_data[(valid_data < lower_bound) | (valid_data > upper_bound)]
                stats_dict['n_outliers'] = len(outliers)
                stats_dict['outlier_percentage'] = (len(outliers) / len(valid_data)) * 100
                
                # é›¶å€¼æ¯”ä¾‹
                zero_count = np.sum(valid_data == 0)
                stats_dict['zero_percentage'] = (zero_count / len(valid_data)) * 100
                
                stats_results[feature_name] = stats_dict
        
        # åˆ›å»ºç»Ÿè®¡æ‘˜è¦è¡¨
        print("ğŸ“ˆ æè¿°æ€§ç»Ÿè®¡æ‘˜è¦ (å‰12ä¸ªç‰¹å¾):")
        print("-" * 100)
        print(f"{'ç‰¹å¾åç§°':<25} {'å‡å€¼':<12} {'æ ‡å‡†å·®':<12} {'ååº¦':<10} {'å³°åº¦':<10} {'å¼‚å¸¸å€¼%':<10}")
        print("-" * 100)
        
        for i, (feature_name, stats_dict) in enumerate(list(stats_results.items())[:12]):
            print(f"{feature_name:<25} {stats_dict['mean']:<12.4f} {stats_dict['std']:<12.4f} "
                  f"{stats_dict['skewness']:<10.4f} {stats_dict['kurtosis']:<10.4f} {stats_dict['outlier_percentage']:<10.2f}")
        
        print("-" * 100)
        
        # ç»˜åˆ¶ç‰¹å¾åˆ†å¸ƒ
        self.plot_feature_distributions(stats_results)
        
        return stats_results
    
    def plot_feature_distributions(self, stats_results):
        """ç»˜åˆ¶ç‰¹å¾åˆ†å¸ƒå›¾"""
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.ravel()
        
        for i in range(min(8, len(self.feature_schema))):
            feature_name = self.feature_schema[i]['name']
            feature_data = self.data[:, i]
            valid_data = feature_data[~np.isnan(feature_data) & ~np.isinf(feature_data)]
            
            if len(valid_data) > 0:
                # ç»˜åˆ¶ç›´æ–¹å›¾
                axes[i].hist(valid_data, bins=50, alpha=0.7, edgecolor='black', density=True)
                axes[i].set_title(f'{feature_name}\\nå‡å€¼:{np.mean(valid_data):.3f}, æ ‡å‡†å·®:{np.std(valid_data):.3f}')
                axes[i].set_xlabel(self.feature_schema[i].get('unit', ''))
                axes[i].set_ylabel('å¯†åº¦')
                axes[i].grid(True, alpha=0.3)
                
                # æ·»åŠ ç»Ÿè®¡çº¿
                axes[i].axvline(np.mean(valid_data), color='red', linestyle='--', alpha=0.8, label='å‡å€¼')
                axes[i].axvline(np.median(valid_data), color='green', linestyle='--', alpha=0.8, label='ä¸­ä½æ•°')
                axes[i].legend(fontsize=8)
            else:
                axes[i].text(0.5, 0.5, 'æ— æœ‰æ•ˆæ•°æ®', ha='center', va='center', 
                            transform=axes[i].transAxes)
                axes[i].set_title(feature_name)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'figures' / 'feature_distributions.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ğŸ’¾ ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {self.output_dir / 'figures' / 'feature_distributions.png'}")
    
    def analyze_target_variable(self):
        """åˆ†æç›®æ ‡å˜é‡ - ç«ç‚¹ç½®ä¿¡åº¦"""
        print("\n" + "="*60)
        print("ğŸ¯ 4. ç›®æ ‡å˜é‡æ·±åº¦åˆ†æ")
        print("="*60)
        
        if self.data is None:
            print("âš ï¸ è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        fire_confidence_idx = 22
        fire_confidence = self.data[:, fire_confidence_idx]
        valid_confidence = fire_confidence[~np.isnan(fire_confidence)]
        
        if len(valid_confidence) == 0:
            print("âš ï¸ ç›®æ ‡å˜é‡æ— æœ‰æ•ˆæ•°æ®")
            return {}
        
        results = {}
        
        # åŸºç¡€ç»Ÿè®¡
        results['fire_stats'] = {
            'count': len(valid_confidence),
            'mean': np.mean(valid_confidence),
            'std': np.std(valid_confidence),
            'min': np.min(valid_confidence),
            'max': np.max(valid_confidence),
            'median': np.median(valid_confidence),
            'q25': np.percentile(valid_confidence, 25),
            'q75': np.percentile(valid_confidence, 75)
        }
        
        # ç«ç‚¹æ£€æµ‹åˆ†æ
        fire_pixels = valid_confidence[valid_confidence > 0]
        no_fire_pixels = valid_confidence[valid_confidence == 0]
        
        results['fire_detection'] = {
            'total_pixels': len(valid_confidence),
            'fire_pixels': len(fire_pixels),
            'no_fire_pixels': len(no_fire_pixels),
            'fire_ratio': len(fire_pixels) / len(valid_confidence),
            'no_fire_ratio': len(no_fire_pixels) / len(valid_confidence)
        }
        
        # ç±»åˆ«ä¸å¹³è¡¡åˆ†æ
        imbalance_ratio = len(no_fire_pixels) / max(len(fire_pixels), 1)
        results['class_imbalance'] = {
            'imbalance_ratio': imbalance_ratio,
            'fire_percentage': (len(fire_pixels) / len(valid_confidence)) * 100,
            'severity': 'extreme' if imbalance_ratio > 1000 else 
                       'severe' if imbalance_ratio > 100 else 
                       'moderate' if imbalance_ratio > 10 else 'mild'
        }
        
        # ç«ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ
        if len(fire_pixels) > 0:
            confidence_ranges = {
                'low (0-5)': np.sum((fire_pixels > 0) & (fire_pixels <= 5)),
                'medium (5-10)': np.sum((fire_pixels > 5) & (fire_pixels <= 10)),
                'high (10-15)': np.sum((fire_pixels > 10) & (fire_pixels <= 15)),
                'very_high (15+)': np.sum(fire_pixels > 15)
            }
            results['confidence_distribution'] = confidence_ranges
        
        # å±•ç¤ºç»“æœ
        print("ğŸ“Š ç›®æ ‡å˜é‡ç»Ÿè®¡:")
        fire_stats = results['fire_stats']
        print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {fire_stats['count']:,}")
        print(f"  å‡å€¼: {fire_stats['mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {fire_stats['std']:.4f}")
        print(f"  èŒƒå›´: [{fire_stats['min']:.2f}, {fire_stats['max']:.2f}]")
        
        print("\nğŸ”¥ ç«ç‚¹æ£€æµ‹ç»Ÿè®¡:")
        fire_det = results['fire_detection']
        print(f"  æ€»åƒç´ æ•°: {fire_det['total_pixels']:,}")
        print(f"  ç«ç‚¹åƒç´ : {fire_det['fire_pixels']:,} ({fire_det['fire_ratio']*100:.4f}%)")
        print(f"  éç«ç‚¹åƒç´ : {fire_det['no_fire_pixels']:,} ({fire_det['no_fire_ratio']*100:.4f}%)")
        
        print("\nâš–ï¸ ç±»åˆ«ä¸å¹³è¡¡åˆ†æ:")
        imbalance = results['class_imbalance']
        print(f"  ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance['imbalance_ratio']:.1f}:1 (éç«ç‚¹:ç«ç‚¹)")
        print(f"  ç«ç‚¹ç™¾åˆ†æ¯”: {imbalance['fire_percentage']:.4f}%")
        print(f"  ä¸å¹³è¡¡ä¸¥é‡ç¨‹åº¦: {imbalance['severity']}")
        
        if 'confidence_distribution' in results:
            print("\nğŸ“ˆ ç«ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ:")
            for range_name, count in results['confidence_distribution'].items():
                percentage = (count / results['fire_detection']['fire_pixels']) * 100
                print(f"  {range_name}: {count:,} ({percentage:.2f}%)")
        
        # å¯è§†åŒ–ç›®æ ‡å˜é‡
        self.plot_target_variable(results, valid_confidence, fire_pixels)
        
        return results
    
    def plot_target_variable(self, results, valid_confidence, fire_pixels):
        """å¯è§†åŒ–ç›®æ ‡å˜é‡"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. æ•´ä½“åˆ†å¸ƒ (å¯¹æ•°å°ºåº¦)
        axes[0,0].hist(valid_confidence, bins=50, alpha=0.7, edgecolor='black')
        axes[0,0].set_xlabel('ç«ç‚¹ç½®ä¿¡åº¦')
        axes[0,0].set_ylabel('é¢‘æ¬¡')
        axes[0,0].set_title('ç«ç‚¹ç½®ä¿¡åº¦æ•´ä½“åˆ†å¸ƒ')
        axes[0,0].grid(True, alpha=0.3)
        axes[0,0].set_yscale('log')
        
        # 2. ç«ç‚¹åƒç´ åˆ†å¸ƒ (æ’é™¤0å€¼)
        if len(fire_pixels) > 0:
            axes[0,1].hist(fire_pixels, bins=30, alpha=0.7, edgecolor='black', color='red')
            axes[0,1].set_xlabel('ç«ç‚¹ç½®ä¿¡åº¦ (>0)')
            axes[0,1].set_ylabel('é¢‘æ¬¡')
            axes[0,1].set_title('æœ‰æ•ˆç«ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ')
            axes[0,1].grid(True, alpha=0.3)
        else:
            axes[0,1].text(0.5, 0.5, 'æ— æœ‰æ•ˆç«ç‚¹æ•°æ®', ha='center', va='center',
                          transform=axes[0,1].transAxes)
            axes[0,1].set_title('æœ‰æ•ˆç«ç‚¹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        
        # 3. ç±»åˆ«ä¸å¹³è¡¡å¯è§†åŒ–
        fire_det = results['fire_detection']
        labels = ['éç«ç‚¹', 'ç«ç‚¹']
        sizes = [fire_det['no_fire_pixels'], fire_det['fire_pixels']]
        colors = ['lightblue', 'red']
        
        wedges, texts, autotexts = axes[1,0].pie(sizes, labels=labels, colors=colors, 
                                                 autopct='%1.4f%%', startangle=90)
        axes[1,0].set_title('ç«ç‚¹ vs éç«ç‚¹åˆ†å¸ƒ')
        
        # 4. ç«ç‚¹ç½®ä¿¡åº¦åŒºé—´åˆ†å¸ƒ
        if 'confidence_distribution' in results:
            conf_dist = results['confidence_distribution']
            ranges = list(conf_dist.keys())
            counts = list(conf_dist.values())
            
            bars = axes[1,1].bar(ranges, counts, color=['green', 'yellow', 'orange', 'red'])
            axes[1,1].set_xlabel('ç½®ä¿¡åº¦åŒºé—´')
            axes[1,1].set_ylabel('åƒç´ æ•°é‡')
            axes[1,1].set_title('ç«ç‚¹ç½®ä¿¡åº¦åŒºé—´åˆ†å¸ƒ')
            axes[1,1].tick_params(axis='x', rotation=45)
            axes[1,1].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                height = bar.get_height()
                axes[1,1].text(bar.get_x() + bar.get_width()/2., height,
                              f'{count:,}', ha='center', va='bottom', fontsize=10)
        else:
            axes[1,1].text(0.5, 0.5, 'æ— ç½®ä¿¡åº¦åˆ†å¸ƒæ•°æ®', ha='center', va='center',
                          transform=axes[1,1].transAxes)
            axes[1,1].set_title('ç«ç‚¹ç½®ä¿¡åº¦åŒºé—´åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'figures' / 'target_variable_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"ğŸ’¾ ç›®æ ‡å˜é‡åˆ†æå›¾å·²ä¿å­˜åˆ°: {self.output_dir / 'figures' / 'target_variable_analysis.png'}")
    
    def generate_summary_and_recommendations(self):
        """ç”Ÿæˆåˆ†ææ€»ç»“å’Œå»ºæ¨¡å»ºè®®"""
        print("\n" + "="*60)
        print("ğŸ“‹ 5. åˆ†ææ€»ç»“ä¸å»ºæ¨¡å»ºè®®")
        print("="*60)
        
        print("ğŸ“Š WildfireSpreadTS æ•°æ®é›† EDA åˆ†ææ€»ç»“")
        print("-" * 60)
        
        print("\nğŸ¯ ä¸»è¦å‘ç°:")
        print("  â€¢ æ•°æ®è¦†ç›–2018-2021å¹´ï¼ŒåŒ…å«å››å¹´å®Œæ•´æ•°æ®")
        print("  â€¢ 23ä¸ªç‰¹å¾é€šé“ï¼ŒåŒ…å«æ°”è±¡ã€åœ°å½¢ã€æ¤è¢«ã€äººä¸ºå› ç´ ")
        print("  â€¢ æåº¦ç±»åˆ«ä¸å¹³è¡¡ï¼šç«ç‚¹åƒç´  < 0.1%")
        print("  â€¢ æ•°æ®è´¨é‡è‰¯å¥½ï¼ŒåŸºæœ¬æ— ç¼ºå¤±å€¼")
        
        print("\nğŸš€ å»ºæ¨¡ç­–ç•¥å»ºè®®:")
        print("  1. æ•°æ®é¢„å¤„ç†:")
        print("     â€¢ ä½¿ç”¨RobustScalerå¤„ç†è¿ç»­ç‰¹å¾")
        print("     â€¢ åœŸåœ°è¦†ç›–ç±»åˆ«ä½¿ç”¨Embeddingå±‚")
        print("     â€¢ å¾ªç¯ç‰¹å¾(é£å‘/å¡å‘)è½¬æ¢ä¸ºsin/cos")
        
        print("\n  2. ç±»åˆ«ä¸å¹³è¡¡å¤„ç†:")
        print("     â€¢ æŸå¤±å‡½æ•°: Focal Loss, Dice Loss")
        print("     â€¢ é‡‡æ ·ç­–ç•¥: WeightedRandomSampler")
        print("     â€¢ æ•°æ®å¢å¼º: å‡ ä½•å˜æ¢ + æ°”è±¡æ‰°åŠ¨")
        
        print("\n  3. æ¨¡å‹æ¶æ„:")
        print("     â€¢ CNN: U-Net + æ³¨æ„åŠ›æœºåˆ¶")
        print("     â€¢ æ—¶ç©ºå»ºæ¨¡: ConvLSTM, 3D CNN")
        print("     â€¢ è¯„ä¼°æŒ‡æ ‡: AUPRC, F1-Score, IoU")
        
        print("\n  4. ä¸‹ä¸€æ­¥è®¡åˆ’:")
        print("     â€¢ å®æ–½CNNæ¨¡å‹å¼€å‘")
        print("     â€¢ Cellular Automataå»ºæ¨¡")
        print("     â€¢ æ··åˆCNN+CAæ¨¡å‹")
        
        print("\n" + "="*60)
        print("âœ… EDAåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {self.output_dir.absolute()}")
        print("ğŸ“Š å¯ç”¨äºè®ºæ–‡å†™ä½œå’Œæ¨¡å‹å¼€å‘")
    
    def run_interactive_analysis(self):
        """è¿è¡Œäº¤äº’å¼åˆ†æ"""
        print("ğŸ”¥ WildfireSpreadTS äº¤äº’å¼EDAåˆ†æç³»ç»Ÿ")
        print("=" * 80)
        print("åŒ…å«å››å¹´å®Œæ•´æ•°æ®(2018-2021)çš„æ·±åº¦åˆ†æ")
        print("=" * 80)
        
        try:
            # 1. æ•°æ®åŠ è½½
            self.load_sample_data(max_files=40, sample_ratio=0.1)
            
            input("\næŒ‰Enteré”®ç»§ç»­è¿›è¡Œæ•°æ®è´¨é‡åˆ†æ...")
            
            # 2. æ•°æ®è´¨é‡åˆ†æ
            quality_results = self.analyze_data_quality()
            
            input("\næŒ‰Enteré”®ç»§ç»­è¿›è¡Œæè¿°æ€§ç»Ÿè®¡åˆ†æ...")
            
            # 3. æè¿°æ€§ç»Ÿè®¡
            desc_stats = self.analyze_descriptive_statistics()
            
            input("\næŒ‰Enteré”®ç»§ç»­è¿›è¡Œç›®æ ‡å˜é‡åˆ†æ...")
            
            # 4. ç›®æ ‡å˜é‡åˆ†æ
            target_results = self.analyze_target_variable()
            
            input("\næŒ‰Enteré”®æŸ¥çœ‹åˆ†ææ€»ç»“å’Œå»ºè®®...")
            
            # 5. æ€»ç»“å’Œå»ºè®®
            self.generate_summary_and_recommendations()
            
            # ä¿å­˜ç»“æœ
            analysis_results = {
                'data_quality': quality_results,
                'descriptive_stats': desc_stats,
                'target_analysis': target_results,
                'metadata': {
                    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'total_samples_analyzed': len(self.data),
                    'features_analyzed': len(self.feature_schema),
                    'fire_events_covered': len(set(self.fire_events)),
                    'years_covered': sorted(set(m['year'] for m in self.metadata))
                }
            }
            
            with open(self.output_dir / 'analysis_results.json', 'w', encoding='utf-8') as f:
                def convert_numpy(obj):
                    if isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, (np.integer, np.floating)):
                        return float(obj)
                    else:
                        return obj
                
                json.dump(analysis_results, f, indent=2, ensure_ascii=False, default=convert_numpy)
            
            print(f"\nğŸ’¾ å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜ä¸º: {self.output_dir / 'analysis_results.json'}")
            
            return analysis_results
            
        except Exception as e:
            print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = Path("data/processed")
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir.absolute()}")
        print("è¯·ç¡®ä¿å·²ç»ä¸‹è½½å¹¶è½¬æ¢äº†HDF5æ•°æ®æ–‡ä»¶")
        return
    
    # æ£€æŸ¥æ˜¯å¦æœ‰HDF5æ–‡ä»¶
    hdf5_files = list(data_dir.rglob("*.hdf5"))
    if not hdf5_files:
        print(f"âŒ åœ¨ {data_dir.absolute()} ä¸­æœªæ‰¾åˆ°HDF5æ–‡ä»¶")
        print("è¯·è¿è¡ŒHDF5è½¬æ¢è„šæœ¬: python src/preprocess/CreateHDF5Dataset.py")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(hdf5_files)} ä¸ªHDF5æ–‡ä»¶")
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = WildfireEDAAnalyzer(
        data_dir=str(data_dir), 
        output_dir="eda_results_interactive"
    )
    
    results = analyzer.run_interactive_analysis()
    
    if results:
        print("\nğŸ¯ å¿«é€Ÿè®¿é—®ä¸»è¦ç»“æœ:")
        print(f"  ğŸ“Š å›¾è¡¨ç›®å½•: {analyzer.output_dir / 'figures'}/")
        print(f"  ğŸ“‹ åˆ†æç»“æœ: {analyzer.output_dir / 'analysis_results.json'}")
    
    print("\n" + "="*80)
    print("æ„Ÿè°¢ä½¿ç”¨ WildfireSpreadTS äº¤äº’å¼EDA åˆ†æç³»ç»Ÿï¼")
    print("ğŸ”¬ Happy Research! ğŸ”¥")


if __name__ == "__main__":
    main() 