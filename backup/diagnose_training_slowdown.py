#!/usr/bin/env python3
"""
è®­ç»ƒé€Ÿåº¦è¯Šæ–­å·¥å…·
åˆ†æå’Œè§£å†³è®­ç»ƒè¶Šæ¥è¶Šæ…¢çš„é—®é¢˜
"""

import torch
import psutil
import time
import gc
import numpy as np
from pathlib import Path

def diagnose_training_issues():
    """è¯Šæ–­è®­ç»ƒé€Ÿåº¦é—®é¢˜"""
    
    print("ğŸ” è®­ç»ƒé€Ÿåº¦è¯Šæ–­æŠ¥å‘Š")
    print("=" * 50)
    
    # 1. ç³»ç»Ÿèµ„æºæ£€æŸ¥
    print("\n1. ç³»ç»Ÿèµ„æºçŠ¶æ€:")
    print(f"   CPUä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1):.1f}%")
    print(f"   å†…å­˜ä½¿ç”¨: {psutil.virtual_memory().percent:.1f}%")
    print(f"   å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    
    # 2. GPUçŠ¶æ€æ£€æŸ¥
    if torch.cuda.is_available():
        print("\n2. GPUçŠ¶æ€:")
        for i in range(torch.cuda.device_count()):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"   å†…å­˜å·²ç”¨: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB")
            print(f"   å†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB")
            print(f"   å†…å­˜æ€»é‡: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB")
    else:
        print("\n2. GPUçŠ¶æ€: æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
    
    # 3. PyTorchè®¾ç½®æ£€æŸ¥
    print("\n3. PyTorché…ç½®:")
    print(f"   ç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   CuDNNå¯ç”¨: {torch.backends.cudnn.enabled}")
    print(f"   CuDNNåŸºå‡†: {torch.backends.cudnn.benchmark}")
    
    # 4. å¸¸è§é—®é¢˜æ£€æŸ¥
    print("\n4. å¸¸è§é—®é¢˜æ£€æŸ¥:")
    
    issues_found = []
    
    # æ£€æŸ¥GPUå†…å­˜ç¢ç‰‡åŒ–
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0)
        reserved = torch.cuda.memory_reserved(0)
        if reserved > allocated * 1.5:
            issues_found.append("GPUå†…å­˜ç¢ç‰‡åŒ–ä¸¥é‡")
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    if psutil.virtual_memory().percent > 80:
        issues_found.append("ç³»ç»Ÿå†…å­˜ä½¿ç”¨è¿‡é«˜")
    
    # æ£€æŸ¥CPUä½¿ç”¨
    cpu_usage = psutil.cpu_percent(interval=1)
    if cpu_usage > 90:
        issues_found.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
    
    if issues_found:
        print("   âš ï¸  å‘ç°é—®é¢˜:")
        for issue in issues_found:
            print(f"      - {issue}")
    else:
        print("   âœ… æœªå‘ç°æ˜æ˜¾çš„ç¡¬ä»¶é—®é¢˜")
    
    return issues_found

def fix_common_issues():
    """ä¿®å¤å¸¸è§é—®é¢˜"""
    
    print("\nğŸ”§ åº”ç”¨å¸¸è§ä¿®å¤æ–¹æ¡ˆ:")
    
    # 1. æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        print("   æ¸…ç†GPUç¼“å­˜...")
        torch.cuda.empty_cache()
        print("   âœ… GPUç¼“å­˜å·²æ¸…ç†")
    
    # 2. å¼ºåˆ¶åƒåœ¾å›æ”¶
    print("   æ‰§è¡Œåƒåœ¾å›æ”¶...")
    gc.collect()
    print("   âœ… åƒåœ¾å›æ”¶å®Œæˆ")
    
    # 3. ä¼˜åŒ–PyTorchè®¾ç½®
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        print("   âœ… å¯ç”¨CuDNNåŸºå‡†æ¨¡å¼")
    
    print("\nğŸ“‹ æ¨èçš„è®­ç»ƒä¼˜åŒ–:")
    print("   1. å‡å°batch_size (å½“å‰å»ºè®®: 2-4)")
    print("   2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ›¿ä»£å¤§batch")
    print("   3. å®šæœŸæ¸…ç†GPUç¼“å­˜ (æ¯5-10ä¸ªepoch)")
    print("   4. ä½¿ç”¨pin_memory=True")
    print("   5. è®¾ç½®num_workers=0 (Windows)")
    print("   6. é¿å…åœ¨è®­ç»ƒå¾ªç¯ä¸­ç´¯ç§¯æ•°æ®")

def create_optimized_training_tips():
    """åˆ›å»ºä¼˜åŒ–è®­ç»ƒçš„å…·ä½“å»ºè®®"""
    
    tips = """
ğŸš€ è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–æŒ‡å—:

1. å†…å­˜ç®¡ç†:
   - ä½¿ç”¨ optimizer.zero_grad(set_to_none=True)
   - é¿å…åœ¨å¾ªç¯ä¸­ç´¯ç§¯tensor
   - å®šæœŸè°ƒç”¨ torch.cuda.empty_cache()
   - ä½¿ç”¨ del æ˜¾å¼åˆ é™¤å¤§tensor

2. æ•°æ®åŠ è½½ä¼˜åŒ–:
   - è®¾ç½® pin_memory=True
   - Windowsä¸Šä½¿ç”¨ num_workers=0
   - ä½¿ç”¨ non_blocking=True
   - é¿å…æ•°æ®è½¬æ¢åœ¨è®­ç»ƒå¾ªç¯ä¸­

3. æ¨¡å‹ä¼˜åŒ–:
   - ä½¿ç”¨ inplace=True æ“ä½œ
   - å¯ç”¨ torch.backends.cudnn.benchmark
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (autocast)
   - å‡å°‘ä¸å¿…è¦çš„è®¡ç®—

4. è®­ç»ƒå¾ªç¯ä¼˜åŒ–:
   - é¿å…é¢‘ç¹çš„CPU-GPUæ•°æ®ä¼ è¾“
   - ä½¿ç”¨ .item() è·å–æ ‡é‡å€¼
   - ä¸è¦åœ¨å¾ªç¯ä¸­ä¿å­˜å®Œæ•´é¢„æµ‹ç»“æœ
   - å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ

5. ç›‘æ§å’Œè°ƒè¯•:
   - è®°å½•æ¯ä¸ªepochçš„æ—¶é—´
   - ç›‘æ§GPUå†…å­˜ä½¿ç”¨
   - æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼
   - ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·
"""
    
    return tips

def test_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡"""
    
    print("\nğŸ§ª å†…å­˜æ•ˆç‡æµ‹è¯•:")
    
    if not torch.cuda.is_available():
        print("   è·³è¿‡GPUæµ‹è¯• (æ— CUDAè®¾å¤‡)")
        return
    
    device = torch.device('cuda')
    
    # è®°å½•åˆå§‹å†…å­˜
    initial_memory = torch.cuda.memory_allocated(device)
    print(f"   åˆå§‹GPUå†…å­˜: {initial_memory / (1024**2):.1f} MB")
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    test_tensors = []
    for i in range(10):
        tensor = torch.randn(100, 100, device=device)
        test_tensors.append(tensor)
    
    peak_memory = torch.cuda.memory_allocated(device)
    print(f"   å³°å€¼GPUå†…å­˜: {peak_memory / (1024**2):.1f} MB")
    
    # æ¸…ç†æµ‹è¯•
    del test_tensors
    torch.cuda.empty_cache()
    
    final_memory = torch.cuda.memory_allocated(device)
    print(f"   æ¸…ç†åå†…å­˜: {final_memory / (1024**2):.1f} MB")
    
    if final_memory <= initial_memory * 1.1:
        print("   âœ… å†…å­˜æ¸…ç†æ­£å¸¸")
    else:
        print("   âš ï¸  å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")

def main():
    """ä¸»è¯Šæ–­å‡½æ•°"""
    
    print("ğŸ¥ é‡ç«CNNè®­ç»ƒé€Ÿåº¦è¯Šæ–­å·¥å…·")
    print("åˆ†æè®­ç»ƒè¶Šæ¥è¶Šæ…¢çš„åŸå› å¹¶æä¾›è§£å†³æ–¹æ¡ˆ")
    
    # æ‰§è¡Œè¯Šæ–­
    issues = diagnose_training_issues()
    
    # ä¿®å¤å¸¸è§é—®é¢˜
    fix_common_issues()
    
    # å†…å­˜æ•ˆç‡æµ‹è¯•
    test_memory_efficiency()
    
    # æä¾›ä¼˜åŒ–å»ºè®®
    tips = create_optimized_training_tips()
    print(tips)
    
    # æ€»ç»“å»ºè®®
    print("\nğŸ¯ é’ˆå¯¹æ‚¨çš„é—®é¢˜çš„å…·ä½“å»ºè®®:")
    print("1. ç«‹å³è¿è¡Œ optimized_wildfire_cnn.py (å·²ä¿®å¤å†…å­˜æ³„æ¼)")
    print("2. å°†batch_sizeè®¾ç½®ä¸º2-4")
    print("3. æ¯5ä¸ªepochæ¸…ç†ä¸€æ¬¡GPUç¼“å­˜")
    print("4. ç›‘æ§è®­ç»ƒæ—¶é—´å˜åŒ–")
    print("5. å¦‚æœä»ç„¶å˜æ…¢ï¼Œè€ƒè™‘é‡å¯è®­ç»ƒè¿›ç¨‹")

if __name__ == "__main__":
    main() 