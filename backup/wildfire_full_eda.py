"""
WildfireSpreadTSæ•°æ®é›†å…¨é¢EDAåˆ†æç³»ç»Ÿ - å¢å¼ºç‰ˆ
å¤„ç†å…¨éƒ¨607ä¸ªHDF5æ–‡ä»¶ï¼Œä¸“é—¨è§£å†³NaNå€¼é—®é¢˜
"""

import numpy as np
import pandas as pd
import h5py
import glob
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class WildfireFullEDA:
    """å¤„ç†å…¨éƒ¨607ä¸ªæ–‡ä»¶çš„å¼ºåŒ–EDAç³»ç»Ÿ"""
    
    def __init__(self, data_dir="data/processed", output_dir="full_eda_results"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # 23é€šé“ç‰¹å¾å®šä¹‰
        self.feature_names = [
            'VIIRS_I4', 'VIIRS_I5', 'VIIRS_M13', 'NDVI', 'EVI2', 
            'Temperature', 'Humidity', 'Wind_Direction', 'Wind_Speed', 'Precipitation',
            'Surface_Pressure', 'Solar_Radiation', 'Elevation', 'Slope', 'Aspect',
            'PDSI', 'Land_Cover', 'Forecast_Temperature', 'Forecast_Humidity', 
            'Forecast_Wind_Direction', 'Forecast_Wind_Speed', 'Forecast_Precipitation',
            'Active_Fire_Confidence'
        ]
        
    def analyze_all_files(self, max_files=607):
        """åˆ†ææ‰€æœ‰æˆ–æŒ‡å®šæ•°é‡çš„æ–‡ä»¶"""
        print(f"ğŸ”¥ å¼€å§‹å…¨é¢æ•°æ®è´¨é‡åˆ†æ - æœ€å¤šå¤„ç† {max_files} ä¸ªæ–‡ä»¶")
        
        # æ”¶é›†æ‰€æœ‰HDF5æ–‡ä»¶
        all_files = []
        for year in ['2018', '2019', '2020', '2021']:
            year_dir = self.data_dir / year
            if year_dir.exists():
                year_files = list(year_dir.glob("*.hdf5"))
                all_files.extend(year_files)
        
        print(f"æ‰¾åˆ°æ€»è®¡ {len(all_files)} ä¸ªHDF5æ–‡ä»¶")
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡
        files_to_process = all_files[:max_files] if max_files < len(all_files) else all_files
        print(f"å°†å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶")
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        total_samples = 0
        nan_counts = np.zeros(23)
        value_ranges = {'min': np.full(23, np.inf), 'max': np.full(23, -np.inf)}
        all_means = []
        fire_events = []
        
        print("\nğŸ“Š é€ä¸ªåˆ†ææ–‡ä»¶...")
        
        for i, file_path in enumerate(files_to_process):
            try:
                with h5py.File(file_path, 'r') as f:
                    data = f['data'][:]  # Shape: (T, C, H, W)
                    
                    # è·å–ç«ç¾äº‹ä»¶åç§°
                    fire_name = f['data'].attrs.get('fire_name', file_path.stem)
                    if isinstance(fire_name, bytes):
                        fire_name = fire_name.decode('utf-8')
                    fire_events.append(fire_name)
                    
                    # é‡å¡‘æ•°æ®ä¸º (N, 23)
                    T, C, H, W = data.shape
                    reshaped_data = data.transpose(0, 2, 3, 1).reshape(-1, C)
                    
                    # ç»Ÿè®¡
                    total_samples += len(reshaped_data)
                    
                    # NaNè®¡æ•°
                    nan_counts += np.isnan(reshaped_data).sum(axis=0)
                    
                    # å€¼èŒƒå›´ç»Ÿè®¡
                    for ch in range(23):
                        channel_data = reshaped_data[:, ch]
                        valid_data = channel_data[np.isfinite(channel_data)]
                        if len(valid_data) > 0:
                            value_ranges['min'][ch] = min(value_ranges['min'][ch], valid_data.min())
                            value_ranges['max'][ch] = max(value_ranges['max'][ch], valid_data.max())
                    
                    # æ¯ä¸ªæ–‡ä»¶çš„å‡å€¼
                    file_means = []
                    for ch in range(23):
                        channel_data = reshaped_data[:, ch]
                        valid_data = channel_data[np.isfinite(channel_data)]
                        file_means.append(np.mean(valid_data) if len(valid_data) > 0 else np.nan)
                    all_means.append(file_means)
                    
                if (i + 1) % 50 == 0:
                    print(f"  å·²å¤„ç† {i + 1}/{len(files_to_process)} ä¸ªæ–‡ä»¶...")
                    
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_quality_report(total_samples, nan_counts, value_ranges, all_means, fire_events)
        return total_samples, nan_counts, value_ranges
    
    def generate_quality_report(self, total_samples, nan_counts, value_ranges, all_means, fire_events):
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        print(f"\nğŸ“‹ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
        
        # è®¡ç®—NaNæ¯”ä¾‹
        nan_ratios = nan_counts / total_samples
        
        # åˆ›å»ºæŠ¥å‘ŠDataFrame
        quality_df = pd.DataFrame({
            'Channel_ID': range(23),
            'Feature_Name': self.feature_names,
            'Total_Samples': [total_samples] * 23,
            'NaN_Count': nan_counts.astype(int),
            'NaN_Ratio': nan_ratios,
            'Min_Value': value_ranges['min'],
            'Max_Value': value_ranges['max'],
            'Has_NaN_Problem': nan_ratios > 0.01,  # è¶…è¿‡1%è®¤ä¸ºæœ‰é—®é¢˜
        })
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        quality_df.to_csv(self.output_dir / "data_quality_full_analysis.csv", index=False)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary_report = f"""
# WildfireSpreadTS å…¨é¢æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- **æ€»æ ·æœ¬æ•°**: {total_samples:,}
- **æ€»æ–‡ä»¶æ•°**: {len(fire_events)}
- **ç‹¬ç‰¹ç«ç¾äº‹ä»¶**: {len(set(fire_events))}
- **ç‰¹å¾æ•°**: 23

## NaNå€¼é—®é¢˜åˆ†æ

### ä¸¥é‡NaNé—®é¢˜ (>5%):
"""
        
        severe_nan = quality_df[quality_df['NaN_Ratio'] > 0.05]
        if len(severe_nan) > 0:
            for _, row in severe_nan.iterrows():
                summary_report += f"- **{row['Feature_Name']}**: {row['NaN_Ratio']:.1%} NaNå€¼\n"
        else:
            summary_report += "æ— ä¸¥é‡NaNé—®é¢˜\n"
        
        summary_report += f"""
### ä¸­ç­‰NaNé—®é¢˜ (1-5%):
"""
        
        moderate_nan = quality_df[(quality_df['NaN_Ratio'] > 0.01) & (quality_df['NaN_Ratio'] <= 0.05)]
        if len(moderate_nan) > 0:
            for _, row in moderate_nan.iterrows():
                summary_report += f"- **{row['Feature_Name']}**: {row['NaN_Ratio']:.1%} NaNå€¼\n"
        else:
            summary_report += "æ— ä¸­ç­‰NaNé—®é¢˜\n"
        
        summary_report += f"""
### è½»å¾®NaNé—®é¢˜ (<1%):
"""
        
        light_nan = quality_df[(quality_df['NaN_Ratio'] > 0) & (quality_df['NaN_Ratio'] <= 0.01)]
        if len(light_nan) > 0:
            for _, row in light_nan.iterrows():
                summary_report += f"- **{row['Feature_Name']}**: {row['NaN_Ratio']:.1%} NaNå€¼\n"
        else:
            summary_report += "æ— è½»å¾®NaNé—®é¢˜\n"
        
        # NaNå¤„ç†å»ºè®®
        summary_report += f"""
## NaNå€¼å¤„ç†å»ºè®®

### æ¨èç­–ç•¥:
1. **ä¸¥é‡NaNé€šé“**: è€ƒè™‘ç‰¹å¾å·¥ç¨‹æˆ–åˆ é™¤
2. **ä¸­ç­‰NaNé€šé“**: ä½¿ç”¨æ’å€¼æˆ–åŸºäºé‚»è¿‘åƒç´ çš„æ–¹æ³•
3. **è½»å¾®NaNé€šé“**: ç®€å•å‡å€¼æˆ–ä¸­ä½æ•°æ’å€¼

### é’ˆå¯¹CNNè®­ç»ƒçš„å…·ä½“å»ºè®®:
- ä½¿ç”¨ `sklearn.impute.SimpleImputer` è¿›è¡Œä¸­ä½æ•°æ’å€¼
- å¯¹äºåœŸåœ°è¦†ç›–ç­‰åˆ†ç±»ç‰¹å¾ï¼Œä½¿ç”¨æœ€é¢‘ç¹å€¼å¡«å……
- è€ƒè™‘æ·»åŠ NaNæŒ‡ç¤ºç‰¹å¾æ¥ä¿ç•™ç¼ºå¤±ä¿¡æ¯

## æ•°æ®èŒƒå›´åˆ†æ
"""
        
        # å€¼èŒƒå›´å¼‚å¸¸æ£€æµ‹
        for i, name in enumerate(self.feature_names):
            min_val = value_ranges['min'][i]
            max_val = value_ranges['max'][i]
            if np.isfinite(min_val) and np.isfinite(max_val):
                summary_report += f"- **{name}**: [{min_val:.2f}, {max_val:.2f}]\n"
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        with open(self.output_dir / "quality_summary_report.md", 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        # æ‰“å°å…³é”®å‘ç°
        print(f"\nâœ… åˆ†æå®Œæˆï¼å…³é”®å‘ç°:")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"ğŸ”¥ ç«ç¾äº‹ä»¶æ•°: {len(set(fire_events))}")
        
        problematic_channels = quality_df[quality_df['NaN_Ratio'] > 0.01]
        if len(problematic_channels) > 0:
            print(f"âš ï¸  æœ‰{len(problematic_channels)}ä¸ªé€šé“å­˜åœ¨>1%çš„NaNå€¼:")
            for _, row in problematic_channels.iterrows():
                print(f"   - {row['Feature_Name']}: {row['NaN_Ratio']:.1%}")
        else:
            print(f"âœ… æ‰€æœ‰é€šé“çš„NaNå€¼æ¯”ä¾‹éƒ½<1%")
        
        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šä¿å­˜åœ¨: {self.output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ WildfireSpreadTSå…¨é¢æ•°æ®è´¨é‡åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    print("ä¸“é—¨è§£å†³NaNå€¼é—®é¢˜ï¼Œåˆ†æå…¨éƒ¨607ä¸ªæ–‡ä»¶")
    print("=" * 60)
    
    try:
        analyzer = WildfireFullEDA()
        
        # ç”¨æˆ·é€‰æ‹©
        print("\nğŸ“‹ é€‰æ‹©åˆ†æèŒƒå›´:")
        print("1. å®Œæ•´åˆ†æ (å…¨éƒ¨607ä¸ªæ–‡ä»¶)")
        print("2. å¤§æ ·æœ¬åˆ†æ (400ä¸ªæ–‡ä»¶)")
        print("3. ä¸­æ ·æœ¬åˆ†æ (200ä¸ªæ–‡ä»¶)")
        print("4. å¿«é€Ÿåˆ†æ (100ä¸ªæ–‡ä»¶)")
        
        choice = input("è¯·é€‰æ‹© (1-4, é»˜è®¤1): ").strip()
        
        if choice == "2":
            max_files = 400
        elif choice == "3":
            max_files = 200
        elif choice == "4":
            max_files = 100
        else:
            max_files = 607  # é»˜è®¤å…¨éƒ¨
        
        # è¿è¡Œåˆ†æ
        analyzer.analyze_all_files(max_files=max_files)
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 