#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆé‡ç«CNN - è§£å†³è®­ç»ƒé€Ÿåº¦é€’å‡é—®é¢˜
ä¸“é—¨é’ˆå¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œä¼˜åŒ–ï¼š
1. å†…å­˜æ³„æ¼
2. GPUå†…å­˜ç¢ç‰‡åŒ–
3. æ¢¯åº¦ç´¯ç§¯é—®é¢˜
4. æ•°æ®åŠ è½½æ•ˆç‡
"""

import os
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam
import h5py
from pathlib import Path
import json
import time
import matplotlib.pyplot as plt
from sklearn.metrics import average_precision_score, f1_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# ä¼˜åŒ–CUDAè®¾ç½®
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

class OptimizedWildfireDataset(Dataset):
    """ä¼˜åŒ–ç‰ˆæ•°æ®é›† - é˜²æ­¢å†…å­˜æ³„æ¼"""
    
    def __init__(self, data_dir, mode='train', max_files=100, target_size=(128, 128)):
        self.data_dir = Path(data_dir)
        self.mode = mode
        self.target_size = target_size
        
        print(f"åˆå§‹åŒ–{mode}æ•°æ®é›†...")
        
        # æ”¶é›†æ–‡ä»¶
        all_files = []
        for year in ['2018', '2019', '2020', '2021']:
            year_dir = self.data_dir / year
            if year_dir.exists():
                year_files = list(year_dir.glob("*.hdf5"))
                all_files.extend(year_files)
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡
        files_to_use = all_files[:max_files]
        n_files = len(files_to_use)
        
        if mode == 'train':
            self.files = files_to_use[:int(0.8 * n_files)]
        else:
            self.files = files_to_use[int(0.8 * n_files):]
        
        print(f"{mode}æ¨¡å¼: {len(self.files)} æ–‡ä»¶")
        
        # æ„å»ºæ ·æœ¬ç´¢å¼•
        self.samples = []
        self._build_samples()
        
        # é¢„è®¡ç®—ç»Ÿè®¡é‡ï¼ˆåªè®­ç»ƒé›†ï¼‰
        if mode == 'train':
            self._compute_stats()
    
    def _build_samples(self):
        """é«˜æ•ˆæ„å»ºæ ·æœ¬ç´¢å¼•"""
        for file_idx, file_path in enumerate(self.files):
            try:
                with h5py.File(file_path, 'r') as f:
                    n_timesteps = f['data'].shape[0]
                    # æ¯ä¸ªæ–‡ä»¶æœ€å¤š5ä¸ªæ ·æœ¬
                    step = max(1, n_timesteps // 5)
                    for t in range(0, n_timesteps, step):
                        self.samples.append((file_idx, t))
            except Exception as e:
                continue
        
        print(f"æ„å»º {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def _compute_stats(self):
        """é¢„è®¡ç®—æ•°æ®ç»Ÿè®¡é‡"""
        print("è®¡ç®—æ•°æ®ç»Ÿè®¡é‡...")
        
        sample_data = []
        for i in range(0, min(50, len(self.samples)), 5):
            try:
                file_idx, timestep = self.samples[i]
                with h5py.File(self.files[file_idx], 'r') as f:
                    data = f['data'][timestep][:22]  # åªå–ç‰¹å¾é€šé“
                    sample_data.append(data.flatten())
            except:
                continue
        
        if sample_data:
            all_data = np.concatenate(sample_data)
            valid_data = all_data[np.isfinite(all_data)]
            
            if len(valid_data) > 0:
                self.data_mean = float(np.median(valid_data))
                self.data_std = float(np.std(valid_data))
                self.data_min = float(np.percentile(valid_data, 5))
                self.data_max = float(np.percentile(valid_data, 95))
            else:
                self.data_mean = 0.0
                self.data_std = 1.0
                self.data_min = -5.0
                self.data_max = 5.0
        else:
            self.data_mean = 0.0
            self.data_std = 1.0
            self.data_min = -5.0
            self.data_max = 5.0
        
        print(f"æ•°æ®ç»Ÿè®¡: mean={self.data_mean:.3f}, std={self.data_std:.3f}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        file_idx, timestep = self.samples[idx]
        
        try:
            with h5py.File(self.files[file_idx], 'r') as f:
                data = f['data'][timestep]
                
                # å¿«é€Ÿæå–å’Œå¤„ç†
                features = torch.from_numpy(data[:22]).float()
                target = torch.from_numpy(data[22:23]).float()
                
                # Resize
                features = F.interpolate(
                    features.unsqueeze(0), 
                    size=self.target_size, 
                    mode='bilinear', 
                    align_corners=False
                ).squeeze(0)
                
                target = F.interpolate(
                    target.unsqueeze(0), 
                    size=self.target_size, 
                    mode='bilinear', 
                    align_corners=False
                ).squeeze(0)
                
                # å¿«é€Ÿæ ‡å‡†åŒ–
                features = torch.clamp(features, self.data_min, self.data_max)
                features = (features - self.data_mean) / (self.data_std + 1e-8)
                
                # äºŒå€¼åŒ–ç›®æ ‡
                target = (target > 0).float()
                
                return features, target
                
        except Exception:
            # è¿”å›é›¶å¼ é‡é¿å…å´©æºƒ
            return (torch.zeros(22, *self.target_size), 
                   torch.zeros(1, *self.target_size))


class OptimizedCNN(nn.Module):
    """ä¼˜åŒ–çš„CNNæ¨¡å‹ - å†…å­˜é«˜æ•ˆ"""
    
    def __init__(self, input_channels=22):
        super().__init__()
        
        # ç®€åŒ–æ¶æ„ä»¥æé«˜é€Ÿåº¦
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 2  
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 3
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 2, stride=2),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(64, 32, 2, stride=2),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(32, 16, 2, stride=2),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(16, 1, 3, padding=1),
            nn.Sigmoid()
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.features(x)
        x = self.decoder(x)
        return x


class OptimizedTrainer:
    """ä¼˜åŒ–çš„è®­ç»ƒå™¨ - è§£å†³é€Ÿåº¦é€’å‡é—®é¢˜"""
    
    def __init__(self, model, train_loader, val_loader, device='cpu'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # ä¼˜åŒ–å™¨è®¾ç½®
        self.criterion = nn.BCELoss()
        self.optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
        
        # å†å²è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.val_auprcs = []
        self.best_val_loss = float('inf')
        
        # æ€§èƒ½ç›‘æ§
        self.epoch_times = []
        
        print(f"ä¼˜åŒ–è®­ç»ƒå™¨åˆå§‹åŒ– - è®¾å¤‡: {device}")
        print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    def train_epoch(self):
        """ä¼˜åŒ–çš„è®­ç»ƒepoch - é˜²æ­¢å†…å­˜æ³„æ¼"""
        self.model.train()
        
        total_loss = 0.0
        valid_batches = 0
        
        # æ¯ä¸ªepochå¼€å§‹æ—¶æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
            if torch.isnan(data).any() or torch.isnan(target).any():
                continue
            
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            # é‡è¦ï¼šç¡®ä¿æ¢¯åº¦å®Œå…¨æ¸…é›¶
            self.optimizer.zero_grad(set_to_none=True)
            
            # å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                output = self.model(data)
                loss = self.criterion(output, target)
            
            # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
            if torch.isnan(loss) or torch.isinf(loss):
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # è®°å½•æŸå¤±ï¼ˆä½¿ç”¨.item()é¿å…ä¿ç•™è®¡ç®—å›¾ï¼‰
            total_loss += loss.item()
            valid_batches += 1
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % 50 == 0:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                print(f"  Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.6f}")
            
            # æ˜¾å¼åˆ é™¤å˜é‡é‡Šæ”¾å†…å­˜
            del output, loss
        
        return total_loss / max(valid_batches, 1)
    
    def validate(self):
        """ä¼˜åŒ–çš„éªŒè¯ - é«˜æ•ˆå†…å­˜ç®¡ç†"""
        self.model.eval()
        
        total_loss = 0.0
        valid_batches = 0
        
        # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯ä¸æ–­appendï¼Œæ›´é«˜æ•ˆ
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in self.val_loader:
                if torch.isnan(data).any() or torch.isnan(target).any():
                    continue
                
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                output = self.model(data)
                loss = self.criterion(output, target)
                
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    total_loss += loss.item()
                    valid_batches += 1
                    
                    # é«˜æ•ˆæ”¶é›†é¢„æµ‹ç»“æœ
                    all_preds.append(output.cpu().numpy())
                    all_targets.append(target.cpu().numpy())
                
                # æ¸…ç†GPUå†…å­˜
                del output, loss
        
        # è®¡ç®—AUPRC
        if all_preds and all_targets:
            preds = np.concatenate(all_preds).flatten()
            targets = np.concatenate(all_targets).flatten()
            
            try:
                auprc = average_precision_score(targets, preds)
            except:
                auprc = 0.0
            
            # æ¸…ç†å†…å­˜
            del preds, targets, all_preds, all_targets
        else:
            auprc = 0.0
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        avg_loss = total_loss / max(valid_batches, 1)
        return avg_loss, auprc
    
    def train(self, num_epochs=20):
        """ä¼˜åŒ–çš„è®­ç»ƒæµç¨‹"""
        print(f"å¼€å§‹ä¼˜åŒ–è®­ç»ƒ {num_epochs} epochs")
        
        save_dir = Path('optimized_wildfire_outputs')
        save_dir.mkdir(exist_ok=True)
        
        for epoch in range(num_epochs):
            epoch_start = time.time()
            
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            print("-" * 40)
            
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            
            # éªŒè¯
            val_loss, val_auprc = self.validate()
            
            # è®°å½•æ—¶é—´
            epoch_time = time.time() - epoch_start
            self.epoch_times.append(epoch_time)
            
            # è®°å½•æŒ‡æ ‡
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.val_auprcs.append(val_auprc)
            
            # æ‰“å°ç»“æœ
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"éªŒè¯AUPRC: {val_auprc:.4f}")
            print(f"Epochæ—¶é—´: {epoch_time:.1f}s")
            
            # æ€§èƒ½ç›‘æ§
            if len(self.epoch_times) >= 3:
                recent_avg = np.mean(self.epoch_times[-3:])
                initial_avg = np.mean(self.epoch_times[:3])
                if recent_avg > initial_avg * 1.5:
                    print(f"âš ï¸  è®­ç»ƒé€Ÿåº¦ä¸‹é™æ£€æµ‹! åˆå§‹: {initial_avg:.1f}s, æœ€è¿‘: {recent_avg:.1f}s")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'val_auprc': val_auprc,
                    'epoch_times': self.epoch_times
                }, save_dir / 'best_model.pth')
                print("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if (epoch + 1) % 5 == 0:
                print("ğŸ§¹ æ¸…ç†å†…å­˜...")
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        # ç»˜åˆ¶æ€§èƒ½åˆ†æ
        self.plot_performance_analysis(save_dir)
        print(f"\nâœ… ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    
    def plot_performance_analysis(self, save_dir):
        """ç»˜åˆ¶æ€§èƒ½åˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='è®­ç»ƒæŸå¤±')
        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('æŸå¤±æ›²çº¿')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # AUPRCæ›²çº¿
        axes[0, 1].plot(epochs, self.val_auprcs, 'g-', label='éªŒè¯AUPRC')
        axes[0, 1].set_title('AUPRCæ›²çº¿')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('AUPRC')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # Epochæ—¶é—´åˆ†æ
        if self.epoch_times:
            axes[1, 0].plot(epochs, self.epoch_times, 'purple', marker='o', markersize=4)
            axes[1, 0].set_title('æ¯Epochè®­ç»ƒæ—¶é—´')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('æ—¶é—´ (ç§’)')
            axes[1, 0].grid(True)
            
            # æ·»åŠ è¶‹åŠ¿çº¿
            if len(self.epoch_times) > 1:
                z = np.polyfit(epochs, self.epoch_times, 1)
                p = np.poly1d(z)
                axes[1, 0].plot(epochs, p(epochs), 'r--', alpha=0.8, label=f'è¶‹åŠ¿: {z[0]:.2f}s/epoch')
                axes[1, 0].legend()
        
        # æ€§èƒ½ç»Ÿè®¡
        if self.epoch_times:
            avg_time = np.mean(self.epoch_times)
            min_time = np.min(self.epoch_times)
            max_time = np.max(self.epoch_times)
            std_time = np.std(self.epoch_times)
            
            stats_text = f"""æ€§èƒ½ç»Ÿè®¡:
å¹³å‡æ—¶é—´: {avg_time:.1f}s
æœ€çŸ­æ—¶é—´: {min_time:.1f}s  
æœ€é•¿æ—¶é—´: {max_time:.1f}s
æ ‡å‡†å·®: {std_time:.1f}s
å˜åŒ–ç‡: {((max_time-min_time)/min_time*100):.1f}%"""
            
            axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, 
                           fontsize=10, verticalalignment='center')
            axes[1, 1].set_title('æ€§èƒ½ç»Ÿè®¡')
            axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()


def main():
    """ä¼˜åŒ–ä¸»å‡½æ•°"""
    print("ğŸš€ ä¼˜åŒ–ç‰ˆé‡ç«CNN - è§£å†³è®­ç»ƒé€Ÿåº¦é€’å‡é—®é¢˜")
    print("=" * 60)
    print("ä¼˜åŒ–å†…å®¹:")
    print("âœ… é˜²æ­¢å†…å­˜æ³„æ¼")
    print("âœ… GPUå†…å­˜ç®¡ç†")
    print("âœ… æ¢¯åº¦æ¸…ç†ä¼˜åŒ–")
    print("âœ… æ€§èƒ½ç›‘æ§")
    print("=" * 60)
    
    # é…ç½®
    config = {
        'data_dir': 'data/processed',
        'batch_size': 4,
        'num_epochs': 20,
        'max_files': 100,
        'target_size': (128, 128),
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print(f"\né…ç½®: {json.dumps(config, indent=2)}")
    
    # æ£€æŸ¥æ•°æ®
    data_dir = Path(config['data_dir'])
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åˆ›å»ºæ•°æ®é›†
    print("\n1. åˆ›å»ºä¼˜åŒ–æ•°æ®é›†...")
    train_dataset = OptimizedWildfireDataset(
        config['data_dir'], 
        mode='train',
        max_files=config['max_files'],
        target_size=config['target_size']
    )
    
    val_dataset = OptimizedWildfireDataset(
        config['data_dir'], 
        mode='val',
        max_files=config['max_files']//4,
        target_size=config['target_size']
    )
    
    # å¤åˆ¶ç»Ÿè®¡é‡
    if hasattr(train_dataset, 'data_mean'):
        val_dataset.data_mean = train_dataset.data_mean
        val_dataset.data_std = train_dataset.data_std
        val_dataset.data_min = train_dataset.data_min
        val_dataset.data_max = train_dataset.data_max
    
    print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¼˜åŒ–è®¾ç½®ï¼‰
    print("\n2. åˆ›å»ºä¼˜åŒ–æ•°æ®åŠ è½½å™¨...")
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=0,  # Windowsä¸Šè®¾ä¸º0
        pin_memory=torch.cuda.is_available(),
        persistent_workers=False,
        drop_last=True  # é¿å…ä¸å®Œæ•´batch
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=0,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=False
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\n3. åˆ›å»ºä¼˜åŒ–æ¨¡å‹...")
    model = OptimizedCNN(input_channels=22)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\n4. åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨...")
    trainer = OptimizedTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=config['device']
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\n5. å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
    trainer.train(num_epochs=config['num_epochs'])
    
    print("\nğŸ‰ ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    print("ğŸ“ ç»“æœ: optimized_wildfire_outputs/")
    print("ğŸ“Š æ€§èƒ½åˆ†æ: performance_analysis.png")


if __name__ == "__main__":
    main() 