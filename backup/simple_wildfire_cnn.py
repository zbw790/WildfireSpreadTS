#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆé‡ç«CNNæ¨¡å‹ - ç¨³å®šå¯è¿è¡Œç‰ˆæœ¬

ä¸“æ³¨äºï¼š
- ç®€å•ä½†æœ‰æ•ˆçš„æ¶æ„
- æ•°å€¼ç¨³å®šæ€§
- å¤„ç†æ‰€æœ‰607ä¸ªç«ç¾äº‹ä»¶
- å¿«é€Ÿè®­ç»ƒå’ŒéªŒè¯

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-01-30
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam
import h5py
from pathlib import Path
import json
import time
import matplotlib.pyplot as plt
from sklearn.metrics import average_precision_score, f1_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°
torch.manual_seed(42)
np.random.seed(42)

class SimpleWildfireDataset(Dataset):
    """ç®€åŒ–çš„é‡ç«æ•°æ®é›†"""
    
    def __init__(self, data_dir, mode='train', max_files_per_year=50):
        self.data_dir = Path(data_dir)
        self.mode = mode
        
        # æ”¶é›†æ‰€æœ‰HDF5æ–‡ä»¶
        all_files = []
        for year in ['2018', '2019', '2020', '2021']:
            year_dir = self.data_dir / year
            if year_dir.exists():
                year_files = list(year_dir.glob("*.hdf5"))[:max_files_per_year]
                all_files.extend(year_files)
        
        print(f"æ‰¾åˆ° {len(all_files)} ä¸ªHDF5æ–‡ä»¶")
        
        # åˆ†å‰²æ•°æ®
        n_files = len(all_files)
        if mode == 'train':
            self.files = all_files[:int(0.8 * n_files)]
        else:  # val
            self.files = all_files[int(0.8 * n_files):]
        
        print(f"{mode}æ¨¡å¼ä½¿ç”¨ {len(self.files)} ä¸ªæ–‡ä»¶")
        
        # æ„å»ºæ ·æœ¬
        self.samples = []
        self._build_samples()
    
    def _build_samples(self):
        """æ„å»ºæ ·æœ¬ç´¢å¼•"""
        for file_idx, file_path in enumerate(self.files):
            try:
                with h5py.File(file_path, 'r') as f:
                    data = f['data']
                    n_timesteps = data.shape[0]
                    
                    # æ¯ä¸ªæ–‡ä»¶é‡‡æ ·å‡ ä¸ªæ—¶é—´æ­¥
                    for t in range(0, min(n_timesteps, 10), 3):
                        self.samples.append((file_idx, t))
                        
            except Exception as e:
                print(f"è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
                continue
        
        print(f"æ„å»ºäº† {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        file_idx, timestep = self.samples[idx]
        file_path = self.files[file_idx]
        
        try:
            with h5py.File(file_path, 'r') as f:
                data = f['data'][timestep]  # (C, H, W)
                
                # 1. æå–ç‰¹å¾å’Œç›®æ ‡
                features = torch.from_numpy(data[:22]).float()  # å‰22ä¸ªé€šé“ä½œä¸ºç‰¹å¾
                target = torch.from_numpy(data[22:23]).float()  # ç«ç‚¹ç½®ä¿¡åº¦ä½œä¸ºç›®æ ‡
                
                # 2. Resizeåˆ°å›ºå®šå°ºå¯¸
                features = F.interpolate(features.unsqueeze(0), size=(128, 128), mode='bilinear', align_corners=False).squeeze(0)
                target = F.interpolate(target.unsqueeze(0), size=(128, 128), mode='bilinear', align_corners=False).squeeze(0)
                
                # 3. ç‰¹å¾æ ‡å‡†åŒ– (ç®€å•çš„min-max)
                features = torch.clamp(features, -10, 10)  # é˜²æ­¢æå€¼
                features = (features - features.mean()) / (features.std() + 1e-8)
                
                # 4. ç›®æ ‡äºŒå€¼åŒ–
                target = (target > 0).float()
                
                return features, target
                
        except Exception as e:
            print(f"è¯»å–æ ·æœ¬å¤±è´¥: {e}")
            # è¿”å›é›¶å¼ é‡é¿å…å´©æºƒ
            return torch.zeros(22, 128, 128), torch.zeros(1, 128, 128)


class SimpleCNN(nn.Module):
    """ç®€åŒ–çš„CNNæ¨¡å‹"""
    
    def __init__(self, input_channels=22):
        super().__init__()
        
        # ç®€å•çš„ç¼–ç å™¨
        self.encoder = nn.Sequential(
            # ç¬¬ä¸€å±‚
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 128 -> 64
            
            # ç¬¬äºŒå±‚
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 64 -> 32
            
            # ç¬¬ä¸‰å±‚
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 32 -> 16
        )
        
        # ç®€å•çš„è§£ç å™¨
        self.decoder = nn.Sequential(
            # ä¸Šé‡‡æ ·1
            nn.ConvTranspose2d(128, 64, 2, stride=2),  # 16 -> 32
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # ä¸Šé‡‡æ ·2
            nn.ConvTranspose2d(64, 32, 2, stride=2),  # 32 -> 64
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # ä¸Šé‡‡æ ·3
            nn.ConvTranspose2d(32, 16, 2, stride=2),  # 64 -> 128
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            
            # è¾“å‡ºå±‚
            nn.Conv2d(16, 1, 3, padding=1),
            nn.Sigmoid()  # ç¡®ä¿è¾“å‡ºåœ¨[0,1]
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # x: (B, C, H, W)
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


class SimpleTrainer:
    """ç®€åŒ–çš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, train_loader, val_loader, device='cpu'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # ä½¿ç”¨ç®€å•çš„BCEæŸå¤±å’ŒAdamä¼˜åŒ–å™¨
        self.criterion = nn.BCELoss()
        self.optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
        
        # å†å²è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.val_auprcs = []
        
        self.best_val_loss = float('inf')
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        n_batches = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if torch.isnan(data).any() or torch.isnan(target).any():
                print(f"è·³è¿‡åŒ…å«NaNçš„batch {batch_idx}")
                continue
            
            self.optimizer.zero_grad()
            
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # æ£€æŸ¥æŸå¤±
            if torch.isnan(loss):
                print(f"æŸå¤±ä¸ºNaNï¼Œè·³è¿‡batch {batch_idx}")
                continue
            
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
            
            if batch_idx % 50 == 0:
                print(f"  Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.6f}")
        
        return total_loss / max(n_batches, 1)
    
    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                if torch.isnan(data).any() or torch.isnan(target).any():
                    continue
                
                output = self.model(data)
                loss = self.criterion(output, target)
                
                if not torch.isnan(loss):
                    total_loss += loss.item()
                    all_preds.append(output.cpu().numpy().flatten())
                    all_targets.append(target.cpu().numpy().flatten())
        
        # è®¡ç®—AUPRC
        if all_preds and all_targets:
            preds = np.concatenate(all_preds)
            targets = np.concatenate(all_targets)
            
            try:
                auprc = average_precision_score(targets, preds)
            except:
                auprc = 0.0
        else:
            auprc = 0.0
        
        avg_loss = total_loss / max(len(self.val_loader), 1)
        return avg_loss, auprc
    
    def train(self, num_epochs=20):
        """è®­ç»ƒæµç¨‹"""
        print(f"å¼€å§‹è®­ç»ƒ {num_epochs} ä¸ªepochs")
        
        save_dir = Path('simple_wildfire_outputs')
        save_dir.mkdir(exist_ok=True)
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            print("-" * 40)
            
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            
            # éªŒè¯
            val_loss, val_auprc = self.validate()
            
            # è®°å½•
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.val_auprcs.append(val_auprc)
            
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"éªŒè¯AUPRC: {val_auprc:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'epoch': epoch,
                    'val_loss': val_loss,
                    'val_auprc': val_auprc
                }, save_dir / 'best_model.pth')
                print("ä¿å­˜æœ€ä½³æ¨¡å‹")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_curves(save_dir)
        print(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
    
    def plot_curves(self, save_dir):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # æŸå¤±
        axes[0].plot(epochs, self.train_losses, label='è®­ç»ƒæŸå¤±')
        axes[0].plot(epochs, self.val_losses, label='éªŒè¯æŸå¤±')
        axes[0].set_title('æŸå¤±æ›²çº¿')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True)
        
        # AUPRC
        axes[1].plot(epochs, self.val_auprcs, label='éªŒè¯AUPRC', color='green')
        axes[1].set_title('AUPRCæ›²çº¿')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('AUPRC')
        axes[1].legend()
        axes[1].grid(True)
        
        # æœ€åæ˜¾ç¤ºä¸€äº›ç»Ÿè®¡
        axes[2].text(0.1, 0.8, f'æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}', transform=axes[2].transAxes)
        axes[2].text(0.1, 0.7, f'æœ€ä½³AUPRC: {max(self.val_auprcs):.4f}', transform=axes[2].transAxes)
        axes[2].text(0.1, 0.6, f'æ€»epochs: {len(self.train_losses)}', transform=axes[2].transAxes)
        axes[2].set_title('è®­ç»ƒç»Ÿè®¡')
        axes[2].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ ç®€åŒ–ç‰ˆé‡ç«CNNæ¨¡å‹")
    print("=" * 50)
    
    # é…ç½®
    config = {
        'data_dir': 'data/processed',
        'batch_size': 4,
        'num_epochs': 20,
        'max_files_per_year': 50,  # é™åˆ¶æ–‡ä»¶æ•°é‡åŠ å¿«è®­ç»ƒ
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print(f"é…ç½®: {json.dumps(config, indent=2)}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\n1. åˆ›å»ºæ•°æ®é›†...")
    train_dataset = SimpleWildfireDataset(
        config['data_dir'], 
        mode='train',
        max_files_per_year=config['max_files_per_year']
    )
    
    val_dataset = SimpleWildfireDataset(
        config['data_dir'], 
        mode='val',
        max_files_per_year=config['max_files_per_year']//2
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n2. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=0
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=0
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\n3. åˆ›å»ºæ¨¡å‹...")
    model = SimpleCNN(input_channels=22)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°: {total_params:,}")
    
    # è®­ç»ƒ
    print("\n4. å¼€å§‹è®­ç»ƒ...")
    trainer = SimpleTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=config['device']
    )
    
    trainer.train(num_epochs=config['num_epochs'])
    
    print("\nâœ… å®Œæˆ!")
    print("ğŸ“ ç»“æœä¿å­˜åœ¨: simple_wildfire_outputs/")


if __name__ == "__main__":
    main() 