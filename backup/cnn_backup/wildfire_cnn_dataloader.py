"""
WildFire CNN Data Loader
ä¸“ç”¨äºé‡ç«ä¼ æ’­é¢„æµ‹çš„CNNæ•°æ®åŠ è½½å™¨ï¼Œå¤„ç†23é€šé“å¤šæ¨¡æ€æ•°æ®å’Œä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import numpy as np
import h5py
import os
import glob
from typing import List, Tuple, Optional, Dict
from sklearn.preprocessing import StandardScaler, RobustScaler
import warnings
from tqdm import tqdm

class WildfireDataset(Dataset):
    """
    é‡ç«ä¼ æ’­é¢„æµ‹æ•°æ®é›†
    """
    
    def __init__(
        self,
        data_dir: str,
        years: List[int],
        sequence_length: int = 5,
        prediction_horizon: int = 1,
        crop_size: int = 128,
        stride: int = 64,
        is_training: bool = True,
        normalize: bool = True,
        fire_threshold: float = 0.5,
        augment: bool = True
    ):
        """
        Args:
            data_dir: HDF5æ–‡ä»¶æ ¹ç›®å½•
            years: ä½¿ç”¨çš„å¹´ä»½åˆ—è¡¨
            sequence_length: è¾“å…¥æ—¶é—´åºåˆ—é•¿åº¦
            prediction_horizon: é¢„æµ‹æ—¶é—´æ­¥é•¿
            crop_size: è£å‰ªå°ºå¯¸
            stride: æ»‘åŠ¨çª—å£æ­¥é•¿
            is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            normalize: æ˜¯å¦æ ‡å‡†åŒ–
            fire_threshold: ç«ç‚¹äºŒå€¼åŒ–é˜ˆå€¼
            augment: æ˜¯å¦æ•°æ®å¢å¼º
        """
        self.data_dir = data_dir
        self.years = years
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.crop_size = crop_size
        self.stride = stride
        self.is_training = is_training
        self.normalize = normalize
        self.fire_threshold = fire_threshold
        self.augment = augment
        
        # 23é€šé“ç‰¹å¾å®šä¹‰
        self.feature_channels = 23
        self.fire_channel = 22  # ç«ç‚¹ç½®ä¿¡åº¦é€šé“
        
        # ç‰¹å¾ç±»åˆ«å®šä¹‰ï¼ˆç”¨äºåˆ†ç±»åˆ«æ ‡å‡†åŒ–ï¼‰
        self.remote_sensing_channels = [0, 1, 2]  # VIIRS
        self.vegetation_channels = [3, 4]  # NDVI, EVI2
        self.weather_channels = [5, 6, 7, 8, 9, 10, 11, 17, 18, 19, 20, 21]  # æ°”è±¡+é¢„æµ‹
        self.topography_channels = [12, 13, 14]  # åœ°å½¢
        self.drought_channels = [15]  # PDSI
        self.landcover_channels = [16]  # åœŸåœ°è¦†ç›–
        
        # æ”¶é›†æ‰€æœ‰ç«ç¾æ–‡ä»¶
        self.fire_files = self._collect_fire_files()
        
        # åˆ›å»ºæ ·æœ¬ç´¢å¼•
        self.samples = self._create_sample_indices()
        
        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        if self.normalize:
            self.scalers = self._initialize_scalers()
        
        print(f"æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
        print(f"  å¹´ä»½: {years}")
        print(f"  ç«ç¾äº‹ä»¶: {len(self.fire_files)}")
        print(f"  æ ·æœ¬æ•°é‡: {len(self.samples)}")
        print(f"  åºåˆ—é•¿åº¦: {sequence_length}")
        print(f"  è£å‰ªå°ºå¯¸: {crop_size}x{crop_size}")
    
    def _collect_fire_files(self) -> List[str]:
        """æ”¶é›†æ‰€æœ‰HDF5ç«ç¾æ–‡ä»¶"""
        fire_files = []
        for year in self.years:
            year_dir = os.path.join(self.data_dir, str(year))
            if os.path.exists(year_dir):
                files = glob.glob(os.path.join(year_dir, "*.hdf5"))
                fire_files.extend(files)
        return sorted(fire_files)
    
    def _create_sample_indices(self) -> List[Dict]:
        """åˆ›å»ºæ ·æœ¬ç´¢å¼•ï¼ŒåŒ…å«ç«ç¾æ–‡ä»¶å’Œæ—¶ç©ºä½ç½®"""
        samples = []
        
        for fire_file in tqdm(self.fire_files, desc="Creating sample indices"):
            try:
                with h5py.File(fire_file, 'r') as f:
                    data = f['data']
                    T, C, H, W = data.shape
                    
                    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ—¶é—´æ­¥
                    if T < self.sequence_length + self.prediction_horizon:
                        continue
                    
                    # æ—¶é—´çª—å£
                    for t_start in range(T - self.sequence_length - self.prediction_horizon + 1):
                        t_end = t_start + self.sequence_length
                        t_target = t_end + self.prediction_horizon - 1
                        
                        # ç©ºé—´çª—å£
                        for h_start in range(0, H - self.crop_size + 1, self.stride):
                            for w_start in range(0, W - self.crop_size + 1, self.stride):
                                h_end = h_start + self.crop_size
                                w_end = w_start + self.crop_size
                                
                                samples.append({
                                    'fire_file': fire_file,
                                    't_start': t_start,
                                    't_end': t_end,
                                    't_target': t_target,
                                    'h_start': h_start,
                                    'h_end': h_end,
                                    'w_start': w_start,
                                    'w_end': w_end
                                })
            except Exception as e:
                warnings.warn(f"Error processing {fire_file}: {e}")
                continue
        
        return samples
    
    def _initialize_scalers(self) -> Dict[str, object]:
        """åˆå§‹åŒ–ä¸åŒç±»åˆ«ç‰¹å¾çš„æ ‡å‡†åŒ–å™¨"""
        scalers = {}
        
        # é¥æ„Ÿæ•°æ®ç”¨RobustScalerï¼ˆæŠ—å¼‚å¸¸å€¼ï¼‰
        scalers['remote_sensing'] = RobustScaler()
        scalers['vegetation'] = RobustScaler()
        
        # æ°”è±¡æ•°æ®ç”¨StandardScaler
        scalers['weather'] = StandardScaler()
        scalers['topography'] = StandardScaler()
        scalers['drought'] = StandardScaler()
        
        # è®¡ç®—æ ‡å‡†åŒ–å‚æ•°
        self._fit_scalers(scalers)
        
        return scalers
    
    def _fit_scalers(self, scalers: Dict[str, object]):
        """æ‹Ÿåˆæ ‡å‡†åŒ–å™¨å‚æ•°"""
        print("ğŸ”§ è®¡ç®—ç‰¹å¾æ ‡å‡†åŒ–å‚æ•°...")
        
        # æ”¶é›†æ ·æœ¬æ•°æ®ç”¨äºæ‹Ÿåˆ
        sample_data = {
            'remote_sensing': [],
            'vegetation': [],
            'weather': [],
            'topography': [],
            'drought': []
        }
        
        # é‡‡æ ·éƒ¨åˆ†æ•°æ®è®¡ç®—ç»Ÿè®¡é‡
        sample_files = self.fire_files[::max(1, len(self.fire_files)//10)]  # é‡‡æ ·10%
        
        for fire_file in tqdm(sample_files, desc="Fitting scalers"):
            try:
                with h5py.File(fire_file, 'r') as f:
                    data = f['data'][:]  # (T, C, H, W)
                    
                    # éšæœºé‡‡æ ·ç©ºé—´ä½ç½®
                    T, C, H, W = data.shape
                    sample_size = min(1000, H*W//100)
                    
                    # å±•å¹³å¹¶é‡‡æ ·
                    data_flat = data.reshape(T*C, H*W).T  # (pixels, T*C)
                    indices = np.random.choice(data_flat.shape[0], sample_size, replace=False)
                    data_sample = data_flat[indices]  # (sample_size, T*C)
                    
                    # é‡å¡‘å›æ—¶é—´ç»´åº¦
                    data_sample = data_sample.reshape(sample_size, T, C)
                    data_sample = data_sample.reshape(-1, C)  # (sample_size*T, C)
                    
                    # è¿‡æ»¤æœ‰é™å€¼
                    valid_mask = np.isfinite(data_sample).all(axis=1)
                    data_sample = data_sample[valid_mask]
                    
                    if len(data_sample) > 0:
                        sample_data['remote_sensing'].append(data_sample[:, self.remote_sensing_channels])
                        sample_data['vegetation'].append(data_sample[:, self.vegetation_channels])
                        sample_data['weather'].append(data_sample[:, self.weather_channels])
                        sample_data['topography'].append(data_sample[:, self.topography_channels])
                        sample_data['drought'].append(data_sample[:, self.drought_channels])
                        
            except Exception as e:
                continue
        
        # æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        for category, scaler in scalers.items():
            if sample_data[category]:
                combined_data = np.vstack(sample_data[category])
                scaler.fit(combined_data)
                print(f"  {category}: {combined_data.shape}")
    
    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        """æ ‡å‡†åŒ–ç‰¹å¾"""
        if not self.normalize:
            return features
        
        normalized = features.copy()
        
        # åˆ†ç±»åˆ«æ ‡å‡†åŒ–
        if self.remote_sensing_channels:
            normalized[..., self.remote_sensing_channels] = self.scalers['remote_sensing'].transform(
                features[..., self.remote_sensing_channels].reshape(-1, len(self.remote_sensing_channels))
            ).reshape(features[..., self.remote_sensing_channels].shape)
        
        if self.vegetation_channels:
            normalized[..., self.vegetation_channels] = self.scalers['vegetation'].transform(
                features[..., self.vegetation_channels].reshape(-1, len(self.vegetation_channels))
            ).reshape(features[..., self.vegetation_channels].shape)
        
        if self.weather_channels:
            normalized[..., self.weather_channels] = self.scalers['weather'].transform(
                features[..., self.weather_channels].reshape(-1, len(self.weather_channels))
            ).reshape(features[..., self.weather_channels].shape)
        
        if self.topography_channels:
            normalized[..., self.topography_channels] = self.scalers['topography'].transform(
                features[..., self.topography_channels].reshape(-1, len(self.topography_channels))
            ).reshape(features[..., self.topography_channels].shape)
        
        if self.drought_channels:
            normalized[..., self.drought_channels] = self.scalers['drought'].transform(
                features[..., self.drought_channels].reshape(-1, len(self.drought_channels))
            ).reshape(features[..., self.drought_channels].shape)
        
        return normalized
    
    def _augment_data(self, features: np.ndarray, target: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """æ•°æ®å¢å¼ºï¼ˆä¿æŒç‰©ç†ä¸€è‡´æ€§ï¼‰"""
        if not self.augment or not self.is_training:
            return features, target
        
        # éšæœºç¿»è½¬ï¼ˆæ°´å¹³/å‚ç›´ï¼‰
        if np.random.random() > 0.5:
            features = np.flip(features, axis=-1).copy()  # æ°´å¹³ç¿»è½¬
            target = np.flip(target, axis=-1).copy()
        
        if np.random.random() > 0.5:
            features = np.flip(features, axis=-2).copy()  # å‚ç›´ç¿»è½¬
            target = np.flip(target, axis=-2).copy()
        
        # éšæœºæ—‹è½¬ï¼ˆ90åº¦å€æ•°ï¼Œä¿æŒé£å‘ä¸€è‡´æ€§ï¼‰
        if np.random.random() > 0.5:
            k = np.random.randint(1, 4)  # 90, 180, 270åº¦
            features = np.rot90(features, k, axes=(-2, -1)).copy()
            target = np.rot90(target, k, axes=(-2, -1)).copy()
            
            # è°ƒæ•´é£å‘ï¼ˆé€šé“7å’Œ19ï¼‰
            if 7 < features.shape[1]:
                features[:, 7] = (features[:, 7] + k * 90) % 360  # å†å²é£å‘
            if 19 < features.shape[1]:
                features[:, 19] = (features[:, 19] + k * 90) % 360  # é¢„æµ‹é£å‘å˜åŒ–
        
        return features, target
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        sample = self.samples[idx]
        
        try:
            with h5py.File(sample['fire_file'], 'r') as f:
                data = f['data']
                
                # æå–è¾“å…¥ç‰¹å¾åºåˆ—
                features = data[
                    sample['t_start']:sample['t_end'],
                    :self.feature_channels,  # å‰23ä¸ªé€šé“
                    sample['h_start']:sample['h_end'],
                    sample['w_start']:sample['w_end']
                ]
                features = np.array(features, dtype=np.float32)
                
                # æå–ç›®æ ‡ï¼ˆç«ç‚¹ç½®ä¿¡åº¦ï¼‰
                target_raw = data[
                    sample['t_target'],
                    self.fire_channel,  # é€šé“22
                    sample['h_start']:sample['h_end'],
                    sample['w_start']:sample['w_end']
                ]
                target_raw = np.array(target_raw, dtype=np.float32)
                
                # äºŒå€¼åŒ–ç›®æ ‡
                target = (target_raw > self.fire_threshold).astype(np.float32)
                
                # æ ‡å‡†åŒ–ç‰¹å¾
                features = self._normalize_features(features)
                
                # æ•°æ®å¢å¼º
                features, target = self._augment_data(features, target)
                
                # è½¬æ¢ä¸ºå¼ é‡
                features = torch.from_numpy(features)  # (T, C, H, W)
                target = torch.from_numpy(target)      # (H, W)
                
                return features, target
                
        except Exception as e:
            # è¿”å›é»˜è®¤æ ·æœ¬é¿å…è®­ç»ƒä¸­æ–­
            warnings.warn(f"Error loading sample {idx}: {e}")
            features = torch.zeros(self.sequence_length, self.feature_channels, self.crop_size, self.crop_size)
            target = torch.zeros(self.crop_size, self.crop_size)
            return features, target

def create_weighted_sampler(dataset: WildfireDataset, target_fire_ratio: float = 0.1) -> WeightedRandomSampler:
    """
    åˆ›å»ºåŠ æƒé‡‡æ ·å™¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    
    Args:
        dataset: æ•°æ®é›†
        target_fire_ratio: ç›®æ ‡ç«ç‚¹åƒç´ æ¯”ä¾‹
    """
    print("âš–ï¸ åˆ†æç±»åˆ«åˆ†å¸ƒï¼Œåˆ›å»ºåŠ æƒé‡‡æ ·å™¨...")
    
    fire_counts = []
    total_counts = []
    
    # é‡‡æ ·éƒ¨åˆ†æ•°æ®è®¡ç®—ç«ç‚¹æ¯”ä¾‹
    sample_size = min(1000, len(dataset))
    indices = np.random.choice(len(dataset), sample_size, replace=False)
    
    for idx in tqdm(indices, desc="Analyzing class distribution"):
        try:
            _, target = dataset[idx]
            fire_pixels = target.sum().item()
            total_pixels = target.numel()
            
            fire_counts.append(fire_pixels)
            total_counts.append(total_pixels)
            
        except:
            continue
    
    # è®¡ç®—æ ·æœ¬æƒé‡
    fire_counts = np.array(fire_counts)
    total_counts = np.array(total_counts)
    fire_ratios = fire_counts / (total_counts + 1e-8)
    
    # æ ¹æ®ç«ç‚¹æ¯”ä¾‹åˆ†é…æƒé‡
    weights = np.ones(len(dataset))
    
    # ä¸ºæœ‰ç«ç‚¹çš„æ ·æœ¬åˆ†é…æ›´é«˜æƒé‡
    high_fire_mask = fire_ratios > 0.001  # ç«ç‚¹æ¯”ä¾‹ > 0.1%
    medium_fire_mask = (fire_ratios > 0.0001) & (fire_ratios <= 0.001)  # 0.01-0.1%
    
    if high_fire_mask.sum() > 0:
        weights[indices[high_fire_mask]] = 10.0  # é«˜ç«ç‚¹æ ·æœ¬10å€æƒé‡
    if medium_fire_mask.sum() > 0:
        weights[indices[medium_fire_mask]] = 3.0  # ä¸­ç­‰ç«ç‚¹æ ·æœ¬3å€æƒé‡
    
    print(f"  æ€»æ ·æœ¬: {len(dataset)}")
    print(f"  é«˜ç«ç‚¹æ ·æœ¬ (>0.1%): {high_fire_mask.sum()} (æƒé‡Ã—10)")
    print(f"  ä¸­ç«ç‚¹æ ·æœ¬ (0.01-0.1%): {medium_fire_mask.sum()} (æƒé‡Ã—3)")
    print(f"  ä½ç«ç‚¹æ ·æœ¬: {len(dataset) - high_fire_mask.sum() - medium_fire_mask.sum()} (æƒé‡Ã—1)")
    
    return WeightedRandomSampler(weights, len(dataset), replacement=True)

def create_dataloaders(
    data_dir: str,
    train_years: List[int],
    val_years: List[int],
    test_years: List[int],
    batch_size: int = 16,
    num_workers: int = 4,
    **dataset_kwargs
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = WildfireDataset(
        data_dir=data_dir,
        years=train_years,
        is_training=True,
        **dataset_kwargs
    )
    
    val_dataset = WildfireDataset(
        data_dir=data_dir,
        years=val_years,
        is_training=False,
        **dataset_kwargs
    )
    
    test_dataset = WildfireDataset(
        data_dir=data_dir,
        years=test_years,
        is_training=False,
        **dataset_kwargs
    )
    
    # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨
    train_sampler = create_weighted_sampler(train_dataset)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        sampler=train_sampler,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    return train_loader, val_loader, test_loader

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    
    # é…ç½®
    data_dir = "data/processed"
    train_years = [2018, 2019]
    val_years = [2020]
    test_years = [2021]
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = create_dataloaders(
        data_dir=data_dir,
        train_years=train_years,
        val_years=val_years,
        test_years=test_years,
        batch_size=4,
        sequence_length=3,
        crop_size=64
    )
    
    # æµ‹è¯•åŠ è½½ä¸€ä¸ªbatch
    print("\nğŸ“¦ æµ‹è¯•æ•°æ®åŠ è½½...")
    for features, targets in train_loader:
        print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}")  # (B, T, C, H, W)
        print(f"ç›®æ ‡å½¢çŠ¶: {targets.shape}")   # (B, H, W)
        print(f"ç«ç‚¹åƒç´ æ¯”ä¾‹: {targets.mean():.6f}")
        break
    
    print("âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆï¼") 