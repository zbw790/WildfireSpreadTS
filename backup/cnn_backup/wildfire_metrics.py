"""
WildFire Evaluation Metrics
ä¸“ç”¨äºé‡ç«ä¼ æ’­é¢„æµ‹çš„è¯„ä¼°æŒ‡æ ‡æ¨¡å—
åŒ…å«AUPRCã€IoUã€F1-Scoreç­‰é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡çš„ä¸“ä¸šæŒ‡æ ‡
"""

import torch
import torch.nn as nn
import numpy as np
import json
from typing import Dict, List, Tuple, Optional, Any
from sklearn.metrics import (
    precision_recall_curve, average_precision_score, roc_auc_score,
    confusion_matrix, classification_report, f1_score, precision_score, recall_score
)
import warnings

class WildfireMetrics:
    """
    é‡ç«ä¼ æ’­é¢„æµ‹è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨
    """
    
    def __init__(self, device: torch.device = None, threshold: float = 0.5):
        """
        Args:
            device: è®¡ç®—è®¾å¤‡
            threshold: äºŒå€¼åŒ–é˜ˆå€¼
        """
        self.device = device if device else torch.device('cpu')
        self.threshold = threshold
    
    def compute_metrics(self, predictions: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        
        Args:
            predictions: (N, 1, H, W) æˆ– (N, H, W) é¢„æµ‹æ¦‚ç‡
            targets: (N, H, W) ç›®æ ‡æ ‡ç­¾ (0 or 1)
            
        Returns:
            metrics: æŒ‡æ ‡å­—å…¸
        """
        # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
        if predictions.dim() == 4 and predictions.size(1) == 1:
            predictions = predictions.squeeze(1)  # (N, H, W)
        
        # å±•å¹³ä¸ºå‘é‡
        pred_flat = predictions.view(-1).cpu().numpy()
        target_flat = targets.view(-1).cpu().numpy()
        
        # è¿‡æ»¤æœ‰æ•ˆå€¼
        valid_mask = np.isfinite(pred_flat) & np.isfinite(target_flat)
        pred_flat = pred_flat[valid_mask]
        target_flat = target_flat[valid_mask]
        
        if len(pred_flat) == 0:
            warnings.warn("No valid predictions for metric calculation")
            return self._empty_metrics()
        
        # åŸºç¡€æŒ‡æ ‡
        metrics = {}
        
        # 1. AUPRC (Area Under Precision-Recall Curve) - ä¸»è¦æŒ‡æ ‡
        try:
            metrics['auprc'] = average_precision_score(target_flat, pred_flat)
        except ValueError:
            metrics['auprc'] = 0.0
        
        # 2. AUC-ROC
        try:
            if len(np.unique(target_flat)) > 1:
                metrics['auc_roc'] = roc_auc_score(target_flat, pred_flat)
            else:
                metrics['auc_roc'] = 0.5
        except ValueError:
            metrics['auc_roc'] = 0.5
        
        # äºŒå€¼åŒ–é¢„æµ‹
        pred_binary = (pred_flat > self.threshold).astype(int)
        
        # 3. Confusion MatrixæŒ‡æ ‡
        cm = confusion_matrix(target_flat, pred_binary, labels=[0, 1])
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
        
        # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
        metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        metrics['f1_score'] = f1_score(target_flat, pred_binary, zero_division=0)
        
        # ç‰¹å¼‚æ€§å’Œæ•æ„Ÿæ€§
        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        metrics['sensitivity'] = metrics['recall']  # æ•æ„Ÿæ€§ = å¬å›ç‡
        
        # 4. IoU (Intersection over Union) - åˆ†å‰²ä»»åŠ¡é‡è¦æŒ‡æ ‡
        metrics['iou'] = self._compute_iou(pred_binary, target_flat)
        
        # 5. Diceç³»æ•°
        metrics['dice'] = self._compute_dice(pred_binary, target_flat)
        
        # 6. ç±»åˆ«ä¸å¹³è¡¡ç›¸å…³æŒ‡æ ‡
        metrics['balanced_accuracy'] = (metrics['sensitivity'] + metrics['specificity']) / 2
        
        # MCC (Matthews Correlation Coefficient)
        metrics['mcc'] = self._compute_mcc(tp, tn, fp, fn)
        
        # 7. ç«ç‚¹ç‰¹å®šæŒ‡æ ‡
        fire_metrics = self._compute_fire_specific_metrics(
            predictions.cpu().numpy(), 
            targets.cpu().numpy()
        )
        metrics.update(fire_metrics)
        
        return metrics
    
    def _compute_iou(self, pred: np.ndarray, target: np.ndarray) -> float:
        """è®¡ç®—IoU"""
        intersection = (pred * target).sum()
        union = (pred + target).sum() - intersection
        return intersection / union if union > 0 else 0.0
    
    def _compute_dice(self, pred: np.ndarray, target: np.ndarray) -> float:
        """è®¡ç®—Diceç³»æ•°"""
        intersection = (pred * target).sum()
        total = pred.sum() + target.sum()
        return (2 * intersection) / total if total > 0 else 0.0
    
    def _compute_mcc(self, tp: int, tn: int, fp: int, fn: int) -> float:
        """è®¡ç®—Matthewsç›¸å…³ç³»æ•°"""
        numerator = tp * tn - fp * fn
        denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
        return numerator / denominator if denominator > 0 else 0.0
    
    def _compute_fire_specific_metrics(
        self, 
        predictions: np.ndarray, 
        targets: np.ndarray
    ) -> Dict[str, float]:
        """
        è®¡ç®—é‡ç«ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡
        
        Args:
            predictions: (N, H, W) é¢„æµ‹æ¦‚ç‡
            targets: (N, H, W) ç›®æ ‡æ ‡ç­¾
        """
        metrics = {}
        
        # è½¬æ¢ä¸ºäºŒå€¼é¢„æµ‹
        pred_binary = (predictions > self.threshold).astype(int)
        
        # 1. ç«ç‚¹æ£€æµ‹ç‡ (Fire Detection Rate)
        fire_pixels_true = (targets == 1).sum()
        fire_pixels_detected = (pred_binary * targets).sum()
        metrics['fire_detection_rate'] = fire_pixels_detected / fire_pixels_true if fire_pixels_true > 0 else 0.0
        
        # 2. è¯¯æŠ¥ç‡ (False Alarm Rate)
        non_fire_pixels = (targets == 0).sum()
        false_alarms = (pred_binary * (1 - targets)).sum()
        metrics['false_alarm_rate'] = false_alarms / non_fire_pixels if non_fire_pixels > 0 else 0.0
        
        # 3. ç«ç‚¹è¦†ç›–åº¦ (Fire Coverage)
        total_pixels = targets.size
        fire_coverage_true = fire_pixels_true / total_pixels
        fire_coverage_pred = pred_binary.sum() / total_pixels
        metrics['fire_coverage_true'] = fire_coverage_true
        metrics['fire_coverage_pred'] = fire_coverage_pred
        
        # 4. ç©ºé—´è¿é€šæ€§æŒ‡æ ‡
        spatial_metrics = self._compute_spatial_metrics(pred_binary, targets)
        metrics.update(spatial_metrics)
        
        return metrics
    
    def _compute_spatial_metrics(
        self, 
        pred_binary: np.ndarray, 
        targets: np.ndarray
    ) -> Dict[str, float]:
        """è®¡ç®—ç©ºé—´è¿é€šæ€§ç›¸å…³æŒ‡æ ‡"""
        metrics = {}
        
        # ç®€åŒ–çš„ç©ºé—´æŒ‡æ ‡è®¡ç®—
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç©ºé—´åˆ†æ
        
        try:
            from scipy import ndimage
            from scipy.spatial.distance import directed_hausdorff
            
            # 1. è¿é€šåŒºåŸŸæ•°é‡
            pred_labeled, pred_num_features = ndimage.label(pred_binary)
            target_labeled, target_num_features = ndimage.label(targets)
            
            metrics['pred_num_regions'] = pred_num_features
            metrics['target_num_regions'] = target_num_features
            
            # 2. æœ€å¤§è¿é€šåŒºåŸŸå¤§å°
            if pred_num_features > 0:
                pred_sizes = ndimage.sum(pred_binary, pred_labeled, range(1, pred_num_features + 1))
                metrics['pred_max_region_size'] = np.max(pred_sizes) if len(pred_sizes) > 0 else 0
            else:
                metrics['pred_max_region_size'] = 0
            
            if target_num_features > 0:
                target_sizes = ndimage.sum(targets, target_labeled, range(1, target_num_features + 1))
                metrics['target_max_region_size'] = np.max(target_sizes) if len(target_sizes) > 0 else 0
            else:
                metrics['target_max_region_size'] = 0
            
            # 3. Hausdorffè·ç¦» (è¾¹ç•Œç›¸ä¼¼æ€§)
            if pred_binary.sum() > 0 and targets.sum() > 0:
                # è·å–è¾¹ç•Œç‚¹
                pred_edges = self._get_edge_points(pred_binary)
                target_edges = self._get_edge_points(targets)
                
                if len(pred_edges) > 0 and len(target_edges) > 0:
                    hausdorff_dist = max(
                        directed_hausdorff(pred_edges, target_edges)[0],
                        directed_hausdorff(target_edges, pred_edges)[0]
                    )
                    metrics['hausdorff_distance'] = hausdorff_dist
                else:
                    metrics['hausdorff_distance'] = float('inf')
            else:
                metrics['hausdorff_distance'] = float('inf')
                
        except ImportError:
            # å¦‚æœæ²¡æœ‰scipyï¼Œè¿”å›åŸºç¡€æŒ‡æ ‡
            metrics['pred_num_regions'] = 0
            metrics['target_num_regions'] = 0
            metrics['pred_max_region_size'] = 0
            metrics['target_max_region_size'] = 0
            metrics['hausdorff_distance'] = float('inf')
        except Exception as e:
            # å¤„ç†å…¶ä»–å¼‚å¸¸
            warnings.warn(f"Error computing spatial metrics: {e}")
            metrics['pred_num_regions'] = 0
            metrics['target_num_regions'] = 0
            metrics['pred_max_region_size'] = 0
            metrics['target_max_region_size'] = 0
            metrics['hausdorff_distance'] = float('inf')
        
        return metrics
    
    def _get_edge_points(self, binary_mask: np.ndarray) -> np.ndarray:
        """è·å–äºŒå€¼æ©ç çš„è¾¹ç•Œç‚¹"""
        if binary_mask.ndim == 3:
            # å¤„ç†3Dæ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªéé›¶åˆ‡ç‰‡
            for i in range(binary_mask.shape[0]):
                if binary_mask[i].sum() > 0:
                    binary_mask = binary_mask[i]
                    break
            else:
                return np.array([])
        
        try:
            from scipy import ndimage
            # è®¡ç®—è¾¹ç•Œ
            eroded = ndimage.binary_erosion(binary_mask)
            boundary = binary_mask.astype(int) - eroded.astype(int)
            edge_points = np.argwhere(boundary > 0)
            return edge_points
        except ImportError:
            # ç®€å•çš„è¾¹ç•Œæ£€æµ‹
            if binary_mask.ndim == 2:
                h, w = binary_mask.shape
                edge_points = []
                for i in range(1, h-1):
                    for j in range(1, w-1):
                        if binary_mask[i, j] == 1:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºè¾¹ç•Œç‚¹
                            neighbors = binary_mask[i-1:i+2, j-1:j+2]
                            if neighbors.sum() < 9:  # ä¸æ˜¯å®Œå…¨è¢«åŒ…å›´
                                edge_points.append([i, j])
                return np.array(edge_points)
            return np.array([])
    
    def compute_threshold_metrics(
        self, 
        predictions: torch.Tensor, 
        targets: torch.Tensor,
        thresholds: Optional[List[float]] = None
    ) -> Dict[str, List[float]]:
        """
        è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹æ¦‚ç‡
            targets: ç›®æ ‡æ ‡ç­¾
            thresholds: é˜ˆå€¼åˆ—è¡¨
            
        Returns:
            threshold_metrics: å„é˜ˆå€¼ä¸‹çš„æŒ‡æ ‡
        """
        if thresholds is None:
            thresholds = np.arange(0.1, 1.0, 0.1).tolist()
        
        threshold_metrics = {
            'thresholds': thresholds,
            'precision': [],
            'recall': [],
            'f1_score': [],
            'iou': [],
            'dice': []
        }
        
        # å±•å¹³æ•°æ®
        if predictions.dim() == 4 and predictions.size(1) == 1:
            predictions = predictions.squeeze(1)
        
        pred_flat = predictions.view(-1).cpu().numpy()
        target_flat = targets.view(-1).cpu().numpy()
        
        # è¿‡æ»¤æœ‰æ•ˆå€¼
        valid_mask = np.isfinite(pred_flat) & np.isfinite(target_flat)
        pred_flat = pred_flat[valid_mask]
        target_flat = target_flat[valid_mask]
        
        for threshold in thresholds:
            pred_binary = (pred_flat > threshold).astype(int)
            
            # è®¡ç®—æŒ‡æ ‡
            precision = precision_score(target_flat, pred_binary, zero_division=0)
            recall = recall_score(target_flat, pred_binary, zero_division=0)
            f1 = f1_score(target_flat, pred_binary, zero_division=0)
            iou = self._compute_iou(pred_binary, target_flat)
            dice = self._compute_dice(pred_binary, target_flat)
            
            threshold_metrics['precision'].append(precision)
            threshold_metrics['recall'].append(recall)
            threshold_metrics['f1_score'].append(f1)
            threshold_metrics['iou'].append(iou)
            threshold_metrics['dice'].append(dice)
        
        return threshold_metrics
    
    def find_optimal_threshold(
        self, 
        predictions: torch.Tensor, 
        targets: torch.Tensor,
        metric: str = 'f1_score'
    ) -> Tuple[float, float]:
        """
        å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        
        Args:
            predictions: é¢„æµ‹æ¦‚ç‡
            targets: ç›®æ ‡æ ‡ç­¾
            metric: ä¼˜åŒ–çš„æŒ‡æ ‡ ('f1_score', 'iou', 'dice')
            
        Returns:
            optimal_threshold, optimal_score
        """
        thresholds = np.arange(0.01, 1.0, 0.01).tolist()
        threshold_metrics = self.compute_threshold_metrics(predictions, targets, thresholds)
        
        if metric not in threshold_metrics:
            raise ValueError(f"Unknown metric: {metric}")
        
        scores = threshold_metrics[metric]
        best_idx = np.argmax(scores)
        
        return thresholds[best_idx], scores[best_idx]
    
    def save_detailed_results(
        self, 
        predictions: torch.Tensor, 
        targets: torch.Tensor,
        save_path: str
    ):
        """
        ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœ
        
        Args:
            predictions: é¢„æµ‹æ¦‚ç‡
            targets: ç›®æ ‡æ ‡ç­¾
            save_path: ä¿å­˜è·¯å¾„
        """
        # åŸºç¡€æŒ‡æ ‡
        basic_metrics = self.compute_metrics(predictions, targets)
        
        # ä¸åŒé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡
        threshold_metrics = self.compute_threshold_metrics(predictions, targets)
        
        # æœ€ä¼˜é˜ˆå€¼
        optimal_f1_th, optimal_f1_score = self.find_optimal_threshold(predictions, targets, 'f1_score')
        optimal_iou_th, optimal_iou_score = self.find_optimal_threshold(predictions, targets, 'iou')
        
        # PRæ›²çº¿æ•°æ®
        pred_flat = predictions.view(-1).cpu().numpy()
        target_flat = targets.view(-1).cpu().numpy()
        
        # è¿‡æ»¤æœ‰æ•ˆå€¼
        valid_mask = np.isfinite(pred_flat) & np.isfinite(target_flat)
        pred_flat = pred_flat[valid_mask]
        target_flat = target_flat[valid_mask]
        
        if len(np.unique(target_flat)) > 1:
            precision_curve, recall_curve, pr_thresholds = precision_recall_curve(target_flat, pred_flat)
            pr_curve_data = {
                'precision': precision_curve.tolist(),
                'recall': recall_curve.tolist(),
                'thresholds': pr_thresholds.tolist()
            }
        else:
            pr_curve_data = None
        
        # ç»„ç»‡ç»“æœ
        detailed_results = {
            'basic_metrics': basic_metrics,
            'threshold_analysis': threshold_metrics,
            'optimal_thresholds': {
                'f1_score': {'threshold': optimal_f1_th, 'score': optimal_f1_score},
                'iou': {'threshold': optimal_iou_th, 'score': optimal_iou_score}
            },
            'pr_curve': pr_curve_data,
            'data_statistics': {
                'total_pixels': int(targets.numel()),
                'fire_pixels': int(targets.sum().item()),
                'fire_ratio': float(targets.mean().item()),
                'prediction_mean': float(predictions.mean().item()),
                'prediction_std': float(predictions.std().item())
            }
        }
        
        # ä¿å­˜ç»“æœ
        with open(save_path, 'w') as f:
            json.dump(detailed_results, f, indent=2)
        
        print(f"ğŸ’¾ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {save_path}")
    
    def _empty_metrics(self) -> Dict[str, float]:
        """è¿”å›ç©ºæŒ‡æ ‡å­—å…¸"""
        return {
            'auprc': 0.0,
            'auc_roc': 0.5,
            'precision': 0.0,
            'recall': 0.0,
            'f1_score': 0.0,
            'specificity': 0.0,
            'sensitivity': 0.0,
            'iou': 0.0,
            'dice': 0.0,
            'balanced_accuracy': 0.0,
            'mcc': 0.0,
            'fire_detection_rate': 0.0,
            'false_alarm_rate': 0.0,
            'fire_coverage_true': 0.0,
            'fire_coverage_pred': 0.0
        }

def test_metrics():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡"""
    print("ğŸ§ª æµ‹è¯•é‡ç«è¯„ä¼°æŒ‡æ ‡...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, height, width = 4, 64, 64
    
    # æ¨¡æ‹Ÿé¢„æµ‹å’Œç›®æ ‡
    predictions = torch.rand(batch_size, 1, height, width)
    targets = torch.zeros(batch_size, height, width)
    
    # æ·»åŠ ä¸€äº›ç«ç‚¹
    targets[:, 10:20, 10:20] = 1.0
    targets[:, 40:45, 40:45] = 1.0
    
    # åˆ›å»ºè¯„ä¼°å™¨
    metrics_calculator = WildfireMetrics()
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = metrics_calculator.compute_metrics(predictions, targets)
    
    print(f"ğŸ”¥ ç«ç‚¹åƒç´ æ¯”ä¾‹: {targets.mean():.4f}")
    print(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
    for name, value in metrics.items():
        if isinstance(value, float):
            print(f"  {name:20}: {value:.4f}")
    
    # æµ‹è¯•é˜ˆå€¼åˆ†æ
    print(f"\nğŸ¯ æœ€ä¼˜é˜ˆå€¼åˆ†æ:")
    optimal_f1_th, optimal_f1_score = metrics_calculator.find_optimal_threshold(
        predictions, targets, 'f1_score'
    )
    print(f"  æœ€ä¼˜F1é˜ˆå€¼: {optimal_f1_th:.3f} (F1: {optimal_f1_score:.4f})")
    
    optimal_iou_th, optimal_iou_score = metrics_calculator.find_optimal_threshold(
        predictions, targets, 'iou'
    )
    print(f"  æœ€ä¼˜IoUé˜ˆå€¼: {optimal_iou_th:.3f} (IoU: {optimal_iou_score:.4f})")
    
    print("âœ… æŒ‡æ ‡æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_metrics() 