"""
WildfireSpreadTSæ•°æ®é›†å…¨é¢EDAåˆ†æç³»ç»Ÿ - å®Œæ•´ç‰ˆ
åŒ…å«9ä¸ªä¸»è¦åˆ†ææ¨¡å—ï¼Œä¸ºæ¨¡å‹å¼€å‘å’Œè®ºæ–‡å†™ä½œæä¾›æ”¯æ’‘
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import h5py
import glob
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ç§‘å­¦è®¡ç®—å’Œç»Ÿè®¡
from scipy import stats
from scipy.spatial.distance import pdist, squareform
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.cluster import DBSCAN

# æ—¶é—´åºåˆ—åˆ†æ
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.stats.diagnostic import acorr_ljungbox

class WildfireEDAAnalyzer:
    """WildfireSpreadTSæ•°æ®é›†EDAåˆ†æå™¨"""
    
    def __init__(self, data_dir="data/processed", output_dir="eda_results"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "figures").mkdir(exist_ok=True)
        (self.output_dir / "tables").mkdir(exist_ok=True)
        (self.output_dir / "reports").mkdir(exist_ok=True)
        
        # ç‰¹å¾å®šä¹‰
        self.feature_schema = self._define_feature_schema()
        self.channel_groups = self._define_channel_groups()
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.results = {}
        self.summary_stats = {}
        
        print(f"EDAåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _define_feature_schema(self):
        """å®šä¹‰23é€šé“ç‰¹å¾æ¨¡å¼"""
        return {
            0: {'name': 'VIIRS_I4', 'category': 'Remote Sensing', 'unit': 'Brightness Temperature (K)', 'range': [200, 400]},
            1: {'name': 'VIIRS_I5', 'category': 'Remote Sensing', 'unit': 'Brightness Temperature (K)', 'range': [200, 400]},
            2: {'name': 'VIIRS_M13', 'category': 'Remote Sensing', 'unit': 'Brightness Temperature (K)', 'range': [200, 400]},
            3: {'name': 'NDVI', 'category': 'Vegetation', 'unit': 'Index', 'range': [-1, 1]},
            4: {'name': 'EVI2', 'category': 'Vegetation', 'unit': 'Index', 'range': [-1, 1]},
            5: {'name': 'Temperature', 'category': 'Weather', 'unit': 'Celsius', 'range': [-50, 60]},
            6: {'name': 'Humidity', 'category': 'Weather', 'unit': 'Percentage', 'range': [0, 100]},
            7: {'name': 'Wind_Direction', 'category': 'Weather', 'unit': 'Degrees', 'range': [0, 360]},
            8: {'name': 'Wind_Speed', 'category': 'Weather', 'unit': 'm/s', 'range': [0, 50]},
            9: {'name': 'Precipitation', 'category': 'Weather', 'unit': 'mm', 'range': [0, 500]},
            10: {'name': 'Surface_Pressure', 'category': 'Weather', 'unit': 'hPa', 'range': [800, 1100]},
            11: {'name': 'Solar_Radiation', 'category': 'Weather', 'unit': 'W/mÂ²', 'range': [0, 1500]},
            12: {'name': 'Elevation', 'category': 'Topography', 'unit': 'meters', 'range': [-500, 9000]},
            13: {'name': 'Slope', 'category': 'Topography', 'unit': 'degrees', 'range': [0, 90]},
            14: {'name': 'Aspect', 'category': 'Topography', 'unit': 'degrees', 'range': [0, 360]},
            15: {'name': 'PDSI', 'category': 'Drought', 'unit': 'Index', 'range': [-10, 10]},
            16: {'name': 'Land_Cover', 'category': 'Land Cover', 'unit': 'Class (1-16)', 'range': [1, 16]},
            17: {'name': 'Forecast_Temperature', 'category': 'Weather Forecast', 'unit': 'Celsius', 'range': [-50, 60]},
            18: {'name': 'Forecast_Humidity', 'category': 'Weather Forecast', 'unit': 'Percentage', 'range': [0, 100]},
            19: {'name': 'Forecast_Wind_Direction', 'category': 'Weather Forecast', 'unit': 'Degrees', 'range': [0, 360]},
            20: {'name': 'Forecast_Wind_Speed', 'category': 'Weather Forecast', 'unit': 'm/s', 'range': [0, 50]},
            21: {'name': 'Forecast_Precipitation', 'category': 'Weather Forecast', 'unit': 'mm', 'range': [0, 500]},
            22: {'name': 'Active_Fire_Confidence', 'category': 'Target', 'unit': 'Confidence (0-100)', 'range': [0, 100]}
        }
    
    def _define_channel_groups(self):
        """å®šä¹‰é€šé“åˆ†ç»„"""
        return {
            'remote_sensing': [0, 1, 2],
            'vegetation': [3, 4],
            'weather_historical': [5, 6, 7, 8, 9, 10, 11],
            'topography': [12, 13, 14],
            'drought': [15],
            'land_cover': [16],
            'weather_forecast': [17, 18, 19, 20, 21],
            'target': [22]
        }
    
    def load_sample_data(self, max_files=200, sample_ratio=0.05):
        """åŠ è½½æ ·æœ¬æ•°æ®è¿›è¡Œåˆ†æ"""
        print(f"æ­£åœ¨åŠ è½½æ•°æ®æ ·æœ¬...")
        
        # æŒ‰å¹´ä»½åˆ†ç»„æŸ¥æ‰¾HDF5æ–‡ä»¶
        hdf5_files_by_year = {}
        for year in ['2018', '2019', '2020', '2021']:
            year_dir = self.data_dir / year
            if year_dir.exists():
                year_files = list(year_dir.glob("*.hdf5"))
                hdf5_files_by_year[year] = year_files
                print(f"  {year}å¹´: {len(year_files)} ä¸ªæ–‡ä»¶")
        
        total_files = sum(len(files) for files in hdf5_files_by_year.values())
        if total_files == 0:
            raise FileNotFoundError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°HDF5æ–‡ä»¶")
        
        print(f"æ€»è®¡æ‰¾åˆ° {total_files} ä¸ªHDF5æ–‡ä»¶")
        
        # ä»æ¯å¹´å‡åŒ€é‡‡æ ·æ–‡ä»¶
        files_to_process = []
        files_per_year = max_files // len(hdf5_files_by_year)
        remainder = max_files % len(hdf5_files_by_year)
        
        for i, (year, year_files) in enumerate(hdf5_files_by_year.items()):
            if year_files:
                # ä¸ºå‰å‡ å¹´åˆ†é…é¢å¤–çš„æ–‡ä»¶
                n_files = files_per_year + (1 if i < remainder else 0)
                n_files = min(n_files, len(year_files))
                
                # éšæœºé‡‡æ ·ä»¥è·å¾—å¤šæ ·æ€§
                import random
                sampled_files = random.sample(year_files, n_files) if n_files < len(year_files) else year_files
                files_to_process.extend(sampled_files)
                print(f"  ä»{year}å¹´é‡‡æ · {len(sampled_files)} ä¸ªæ–‡ä»¶")
        
        print(f"æ€»å…±å°†å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶")
        
        all_data = []
        fire_events = []
        metadata = []
        
        for file_path in files_to_process:
            try:
                with h5py.File(file_path, 'r') as f:
                    year = file_path.parent.name
                    
                    # ç›´æ¥è¯»å–æ•°æ®ï¼ˆHDF5æ–‡ä»¶ç»“æ„ï¼šæ ¹ç›®å½•åªæœ‰'data'é”®ï¼‰
                    data = f['data'][:]  # Shape: (T, C, H, W)
                    
                    # ä»å±æ€§ä¸­è·å–ç«ç¾äº‹ä»¶ä¿¡æ¯
                    fire_name = f['data'].attrs.get('fire_name', file_path.stem)
                    if isinstance(fire_name, bytes):
                        fire_name = fire_name.decode('utf-8')
                    elif isinstance(fire_name, np.ndarray):
                        fire_name = str(fire_name)
                    
                    # é‡‡æ ·æ•°æ®ä»¥å‡å°‘å†…å­˜å ç”¨
                    T, C, H, W = data.shape
                    sample_size = int(H * W * sample_ratio)
                    
                    # éšæœºé‡‡æ ·åƒç´ 
                    pixel_indices = np.random.choice(H*W, size=min(sample_size, H*W), replace=False)
                    h_indices = pixel_indices // W
                    w_indices = pixel_indices % W
                    
                    # æå–é‡‡æ ·æ•°æ®
                    sampled_data = data[:, :, h_indices, w_indices]  # (T, C, N_samples)
                    sampled_data = sampled_data.transpose(2, 0, 1)  # (N_samples, T, C)
                    sampled_data = sampled_data.reshape(-1, C)  # (N_samples*T, C)
                    
                    all_data.append(sampled_data)
                    
                    # è®°å½•å…ƒæ•°æ®
                    for i in range(len(sampled_data)):
                        metadata.append({
                            'year': year,
                            'fire_event': fire_name,
                            'file_path': str(file_path),
                            'sample_id': i
                        })
                    
                    fire_events.append(fire_name)
                        
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        if not all_data:
            raise ValueError("æœªèƒ½æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        self.data = np.vstack(all_data)
        self.metadata_df = pd.DataFrame(metadata)
        self.fire_events = fire_events
        
        print(f"æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  - æ•°æ®å½¢çŠ¶: {self.data.shape}")
        print(f"  - ç«ç¾äº‹ä»¶æ•°: {len(set(fire_events))}")
        print(f"  - æ—¶é—´è·¨åº¦: {sorted(set(self.metadata_df['year']))}")
        
        return self.data, self.metadata_df
    
    def analyze_feature_relationships(self):
        """4. ç‰¹å¾å…³ç³»ä¸ç›¸å…³æ€§åˆ†æ"""
        if not hasattr(self, 'data'):
            self.load_sample_data()
        
        results = {}
        
        # 4.1 ç›¸å…³æ€§çŸ©é˜µåˆ†æ
        # è®¡ç®—æ‰€æœ‰ç‰¹å¾çš„ç›¸å…³æ€§
        valid_data_mask = np.all(np.isfinite(self.data), axis=1)
        clean_data = self.data[valid_data_mask]
        
        if len(clean_data) < 1000:
            # å¦‚æœæ¸…æ´æ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨æ’å€¼
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='median')
            clean_data = imputer.fit_transform(self.data)
        
        # è®¡ç®—Pearsonç›¸å…³ç³»æ•°
        correlation_matrix = np.corrcoef(clean_data.T)
        
        # è®¡ç®—Spearmanç›¸å…³ç³»æ•°ï¼ˆå¯¹éçº¿æ€§å…³ç³»æ›´æ•æ„Ÿï¼‰
        spearman_corr = []
        for i in range(clean_data.shape[1]):
            row_corr = []
            for j in range(clean_data.shape[1]):
                if i == j:
                    row_corr.append(1.0)
                else:
                    corr, _ = stats.spearmanr(clean_data[:, i], clean_data[:, j])
                    row_corr.append(corr if not np.isnan(corr) else 0.0)
            spearman_corr.append(row_corr)
        spearman_matrix = np.array(spearman_corr)
        
        results['correlation_analysis'] = {
            'pearson_matrix': correlation_matrix,
            'spearman_matrix': spearman_matrix,
            'feature_names': [self.feature_schema[i]['name'] for i in range(len(self.feature_schema))]
        }
        
        # 4.2 ä¸ç›®æ ‡å˜é‡çš„å…³ç³»åˆ†æ
        target_correlations = {}
        target_data = clean_data[:, 22]  # ç«ç‚¹ç½®ä¿¡åº¦
        
        for i in range(22):  # æ’é™¤ç›®æ ‡å˜é‡æœ¬èº«
            feature_data = clean_data[:, i]
            
            # Pearsonç›¸å…³
            pearson_corr, pearson_p = stats.pearsonr(feature_data, target_data)
            
            # Spearmanç›¸å…³
            spearman_corr, spearman_p = stats.spearmanr(feature_data, target_data)
            
            # äº’ä¿¡æ¯ï¼ˆéçº¿æ€§å…³ç³»ï¼‰
            from sklearn.feature_selection import mutual_info_regression
            mi_score = mutual_info_regression(feature_data.reshape(-1, 1), target_data)[0]
            
            target_correlations[i] = {
                'feature_name': self.feature_schema[i]['name'],
                'pearson_corr': float(pearson_corr),
                'pearson_p': float(pearson_p),
                'spearman_corr': float(spearman_corr),
                'spearman_p': float(spearman_p),
                'mutual_info': float(mi_score)
            }
        
        results['target_correlations'] = target_correlations
        
        # 4.3 ç‰¹å¾é‡è¦æ€§åˆ†æ
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.inspection import permutation_importance
        
        # ä½¿ç”¨éšæœºæ£®æ—è¯„ä¼°ç‰¹å¾é‡è¦æ€§
        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        X = clean_data[:, :22]  # ç‰¹å¾
        y = target_data  # ç›®æ ‡
        
        rf.fit(X, y)
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = rf.feature_importances_
        
        # æ’åˆ—é‡è¦æ€§
        perm_importance = permutation_importance(rf, X, y, n_repeats=10, random_state=42, n_jobs=-1)
        
        importance_analysis = {}
        for i in range(22):
            importance_analysis[i] = {
                'feature_name': self.feature_schema[i]['name'],
                'rf_importance': float(feature_importance[i]),
                'perm_importance_mean': float(perm_importance.importances_mean[i]),
                'perm_importance_std': float(perm_importance.importances_std[i])
            }
        
        results['feature_importance'] = importance_analysis
        
        # ä¿å­˜ç»“æœ
        self.results['feature_relationships'] = results
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_feature_relationships(results)
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_feature_relationships_report(results)
        
        print(f"  âœ… ç‰¹å¾å…³ç³»ä¸ç›¸å…³æ€§åˆ†æå®Œæˆ")
        return results
    
    def analyze_target_variable(self):
        """5. ç›®æ ‡å˜é‡æ·±åº¦åˆ†æ"""
        if not hasattr(self, 'data'):
            self.load_sample_data()
        
        results = {}
        
        # è·å–ç›®æ ‡å˜é‡æ•°æ®
        target_data = self.data[:, 22]  # ç«ç‚¹ç½®ä¿¡åº¦
        valid_target = target_data[np.isfinite(target_data)]
        
        # 5.1 ç›®æ ‡å˜é‡åˆ†å¸ƒç‰¹å¾
        distribution_analysis = {
            'total_samples': len(target_data),
            'valid_samples': len(valid_target),
            'missing_ratio': (len(target_data) - len(valid_target)) / len(target_data),
            'mean': float(np.mean(valid_target)),
            'std': float(np.std(valid_target)),
            'min': float(np.min(valid_target)),
            'max': float(np.max(valid_target)),
            'median': float(np.median(valid_target)),
            'q25': float(np.percentile(valid_target, 25)),
            'q75': float(np.percentile(valid_target, 75))
        }
        
        # 5.2 ç±»åˆ«ä¸å¹³è¡¡åˆ†æ
        # å®šä¹‰ç«ç‚¹é˜ˆå€¼
        fire_thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
        imbalance_analysis = {}
        
        for threshold in fire_thresholds:
            fire_pixels = (valid_target >= threshold).sum()
            no_fire_pixels = (valid_target < threshold).sum()
            fire_ratio = fire_pixels / len(valid_target)
            imbalance_ratio = no_fire_pixels / max(fire_pixels, 1)  # é¿å…é™¤é›¶
            
            imbalance_analysis[threshold] = {
                'fire_pixels': int(fire_pixels),
                'no_fire_pixels': int(no_fire_pixels),
                'fire_ratio': float(fire_ratio),
                'imbalance_ratio': float(imbalance_ratio)
            }
        
        # 5.3 æ—¶é—´åºåˆ—ç‰¹å¾åˆ†æ
        # æŒ‰ç«ç¾äº‹ä»¶åˆ†æç›®æ ‡å˜é‡çš„æ—¶é—´æ¼”åŒ–
        temporal_patterns = {}
        
        for fire_event in set(self.metadata_df['fire_event']):
            event_mask = self.metadata_df['fire_event'] == fire_event
            event_indices = self.metadata_df[event_mask].index
            event_target = target_data[event_indices]
            valid_event_target = event_target[np.isfinite(event_target)]
            
            if len(valid_event_target) > 0:
                temporal_patterns[fire_event] = {
                    'duration': len(event_target),
                    'max_confidence': float(np.max(valid_event_target)),
                    'mean_confidence': float(np.mean(valid_event_target)),
                    'evolution_trend': self._calculate_trend(valid_event_target),
                    'peak_timing': float(np.argmax(valid_event_target) / len(valid_event_target))
                }
        
        results['distribution_analysis'] = distribution_analysis
        results['imbalance_analysis'] = imbalance_analysis
        results['temporal_patterns'] = temporal_patterns
        
        # ä¿å­˜ç»“æœ
        self.results['target_analysis'] = results
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_target_analysis(results)
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_target_analysis_report(results)
        
        print(f"  âœ… ç›®æ ‡å˜é‡æ·±åº¦åˆ†æå®Œæˆ")
        return results
    
    def analyze_environmental_variables(self):
        """6. ç¯å¢ƒå˜é‡ä¸“é¢˜åˆ†æ"""
        if not hasattr(self, 'data'):
            self.load_sample_data()
        
        results = {}
        
        # 6.1 æ°”è±¡å˜é‡ç»¼åˆåˆ†æ
        weather_channels = self.channel_groups['weather_historical']
        weather_analysis = {}
        
        for channel_id in weather_channels:
            channel_data = self.data[:, channel_id]
            valid_data = channel_data[np.isfinite(channel_data)]
            
            if len(valid_data) > 0:
                # å­£èŠ‚æ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸºäºæ•°æ®åˆ†å¸ƒï¼‰
                seasonal_stats = self._analyze_seasonality(valid_data)
                
                weather_analysis[channel_id] = {
                    'channel_name': self.feature_schema[channel_id]['name'],
                    'mean': float(np.mean(valid_data)),
                    'std': float(np.std(valid_data)),
                    'cv': float(np.std(valid_data) / np.mean(valid_data)) if np.mean(valid_data) != 0 else 0,
                    'seasonal_stats': seasonal_stats
                }
        
        # 6.2 æ¤è¢«æŒ‡æ•°åˆ†æ
        vegetation_channels = self.channel_groups['vegetation']
        vegetation_analysis = {}
        
        for channel_id in vegetation_channels:
            channel_data = self.data[:, channel_id]
            valid_data = channel_data[np.isfinite(channel_data)]
            
            if len(valid_data) > 0:
                # æ¤è¢«å¥åº·çŠ¶æ€åˆ†ç±»
                if 'NDVI' in self.feature_schema[channel_id]['name']:
                    health_categories = self._categorize_vegetation_health(valid_data, 'NDVI')
                else:
                    health_categories = self._categorize_vegetation_health(valid_data, 'EVI2')
                
                vegetation_analysis[channel_id] = {
                    'channel_name': self.feature_schema[channel_id]['name'],
                    'health_categories': health_categories,
                    'mean': float(np.mean(valid_data)),
                    'std': float(np.std(valid_data))
                }
        
        # 6.3 åœ°å½¢å› å­å½±å“åˆ†æ
        topography_channels = self.channel_groups['topography']
        topography_analysis = {}
        
        for channel_id in topography_channels:
            channel_data = self.data[:, channel_id]
            valid_data = channel_data[np.isfinite(channel_data)]
            
            if len(valid_data) > 0:
                # åœ°å½¢åˆ†ç±»åˆ†æ
                if 'Elevation' in self.feature_schema[channel_id]['name']:
                    topo_categories = self._categorize_elevation(valid_data)
                elif 'Slope' in self.feature_schema[channel_id]['name']:
                    topo_categories = self._categorize_slope(valid_data)
                else:  # Aspect
                    topo_categories = self._categorize_aspect(valid_data)
                
                topography_analysis[channel_id] = {
                    'channel_name': self.feature_schema[channel_id]['name'],
                    'categories': topo_categories,
                    'mean': float(np.mean(valid_data)),
                    'std': float(np.std(valid_data))
                }
        
        results['weather_analysis'] = weather_analysis
        results['vegetation_analysis'] = vegetation_analysis
        results['topography_analysis'] = topography_analysis
        
        # ä¿å­˜ç»“æœ
        self.results['environmental_analysis'] = results
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_environmental_analysis(results)
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_environmental_analysis_report(results)
        
        print(f"  âœ… ç¯å¢ƒå˜é‡ä¸“é¢˜åˆ†æå®Œæˆ")
        return results
    
    def analyze_preprocessing_requirements(self):
        """7. æ•°æ®é¢„å¤„ç†éœ€æ±‚åˆ†æ"""
        if not hasattr(self, 'data'):
            self.load_sample_data()
        
        results = {}
        
        # 7.1 æ•°æ®æ ‡å‡†åŒ–éœ€æ±‚åˆ†æ
        normalization_analysis = {}
        
        for i in range(self.data.shape[1]):
            channel_data = self.data[:, i]
            valid_data = channel_data[np.isfinite(channel_data)]
            
            if len(valid_data) > 0:
                # åˆ†ææ•°æ®åˆ†å¸ƒç‰¹å¾
                skewness = stats.skew(valid_data)
                kurtosis = stats.kurtosis(valid_data)
                
                # æ¨èæ ‡å‡†åŒ–æ–¹æ³•
                if abs(skewness) > 2:
                    if skewness > 0:
                        recommended_transform = "Log transform or Box-Cox"
                    else:
                        recommended_transform = "Square or inverse transform"
                elif abs(skewness) > 1:
                    recommended_transform = "StandardScaler or RobustScaler"
                else:
                    recommended_transform = "StandardScaler or MinMaxScaler"
                
                normalization_analysis[i] = {
                    'channel_name': self.feature_schema[i]['name'],
                    'skewness': float(skewness),
                    'kurtosis': float(kurtosis),
                    'recommended_transform': recommended_transform,
                    'range': [float(np.min(valid_data)), float(np.max(valid_data))],
                    'needs_log_transform': abs(skewness) > 2 and skewness > 0
                }
        
        # 7.2 å¼‚å¸¸å€¼å¤„ç†å»ºè®®
        outlier_handling = {}
        
        for i in range(self.data.shape[1]):
            channel_data = self.data[:, i]
            valid_data = channel_data[np.isfinite(channel_data)]
            
            if len(valid_data) > 0:
                # è®¡ç®—å¼‚å¸¸å€¼æ¯”ä¾‹
                Q1 = np.percentile(valid_data, 25)
                Q3 = np.percentile(valid_data, 75)
                IQR = Q3 - Q1
                outliers = ((valid_data < Q1 - 1.5 * IQR) | 
                           (valid_data > Q3 + 1.5 * IQR)).sum()
                outlier_ratio = outliers / len(valid_data)
                
                # å»ºè®®å¤„ç†æ–¹æ³•
                if outlier_ratio > 0.1:
                    handling_method = "Robust methods (clip or transform)"
                elif outlier_ratio > 0.05:
                    handling_method = "Careful inspection and selective removal"
                else:
                    handling_method = "Standard methods acceptable"
                
                outlier_handling[i] = {
                    'channel_name': self.feature_schema[i]['name'],
                    'outlier_ratio': float(outlier_ratio),
                    'handling_method': handling_method
                }
        
        # 7.3 ç±»åˆ«ä¸å¹³è¡¡å¤„ç†å»ºè®®
        target_data = self.data[:, 22]
        valid_target = target_data[np.isfinite(target_data)]
        
        # åˆ†æä¸åŒé˜ˆå€¼ä¸‹çš„ä¸å¹³è¡¡ç¨‹åº¦
        imbalance_strategies = {}
        for threshold in [0.1, 0.3, 0.5, 0.7]:
            positive_ratio = (valid_target >= threshold).sum() / len(valid_target)
            imbalance_ratio = (1 - positive_ratio) / max(positive_ratio, 0.001)
            
            if imbalance_ratio > 100:
                strategy = "SMOTE + Undersampling + Focal Loss"
            elif imbalance_ratio > 50:
                strategy = "Weighted sampling + Focal Loss"
            elif imbalance_ratio > 10:
                strategy = "Weighted loss functions"
            else:
                strategy = "Standard methods"
            
            imbalance_strategies[threshold] = {
                'positive_ratio': float(positive_ratio),
                'imbalance_ratio': float(imbalance_ratio),
                'recommended_strategy': strategy
            }
        
        results['normalization_analysis'] = normalization_analysis
        results['outlier_handling'] = outlier_handling
        results['imbalance_strategies'] = imbalance_strategies
        
        # ä¿å­˜ç»“æœ
        self.results['preprocessing_requirements'] = results
        
        # ç”ŸæˆæŠ¥å‘Š
        self._save_preprocessing_report(results)
        
        print(f"  âœ… æ•°æ®é¢„å¤„ç†éœ€æ±‚åˆ†æå®Œæˆ")
        return results
    
    def create_advanced_visualizations(self):
        """8. é«˜çº§å¯è§†åŒ–ä¸æ´å¯Ÿå‘ç°"""
        if not hasattr(self, 'data'):
            self.load_sample_data()
        
        results = {}
        
        # 8.1 å¤šç»´ç‰¹å¾ç©ºé—´å¯è§†åŒ–
        print("    ç”ŸæˆPCAå’Œt-SNEå¯è§†åŒ–...")
        self._create_dimensionality_reduction_viz()
        
        # 8.2 ç‰¹å¾äº¤äº’ä½œç”¨å¯è§†åŒ–
        print("    ç”Ÿæˆç‰¹å¾äº¤äº’ä½œç”¨å›¾...")
        self._create_feature_interaction_viz()
        
        # 8.3 æ—¶ç©ºæ¨¡å¼å¯è§†åŒ–
        print("    ç”Ÿæˆæ—¶ç©ºæ¨¡å¼å›¾...")
        self._create_spatiotemporal_viz()
        
        # 8.4 ç¯å¢ƒæ¡ä»¶vsç«ç¾å¼ºåº¦çƒ­åŠ›å›¾
        print("    ç”Ÿæˆç¯å¢ƒæ¡ä»¶çƒ­åŠ›å›¾...")
        self._create_environmental_heatmaps()
        
        results['visualizations_created'] = [
            'pca_tsne_scatter.png',
            'feature_interactions.png',
            'spatiotemporal_patterns.png',
            'environmental_heatmaps.png'
        ]
        
        self.results['advanced_visualizations'] = results
        
        print(f"  âœ… é«˜çº§å¯è§†åŒ–ä¸æ´å¯Ÿå‘ç°å®Œæˆ")
        return results
    
    def generate_academic_report(self):
        """9. ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š"""
        if not hasattr(self, 'results') or not self.results:
            print("è¯·å…ˆè¿è¡Œå®Œæ•´çš„EDAåˆ†æ")
            return
        
        report_content = self._generate_comprehensive_report()
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "reports" / "comprehensive_eda_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"  âœ… å­¦æœ¯æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
        return report_content
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„EDAåˆ†æ"""
        print("ğŸš€ å¼€å§‹å…¨é¢EDAåˆ†æ...")
        
        try:
            # 1. æ•°æ®è´¨é‡ä¸å®Œæ•´æ€§åˆ†æ
            print("\nğŸ“‹ 1. æ•°æ®è´¨é‡ä¸å®Œæ•´æ€§åˆ†æ")
            self.analyze_data_quality()
            
            # 2. æè¿°æ€§ç»Ÿè®¡åˆ†æ
            print("\nğŸ“Š 2. æè¿°æ€§ç»Ÿè®¡åˆ†æ")
            self.analyze_descriptive_statistics()
            
            # 3. æ—¶ç©ºåˆ†å¸ƒç‰¹å¾åˆ†æ
            print("\nğŸŒ 3. æ—¶ç©ºåˆ†å¸ƒç‰¹å¾åˆ†æ")
            self.analyze_spatiotemporal_patterns()
            
            # 4. ç‰¹å¾å…³ç³»ä¸ç›¸å…³æ€§åˆ†æ
            print("\nğŸ”— 4. ç‰¹å¾å…³ç³»ä¸ç›¸å…³æ€§åˆ†æ")
            self.analyze_feature_relationships()
            
            # 5. ç›®æ ‡å˜é‡æ·±åº¦åˆ†æ
            print("\nğŸ¯ 5. ç›®æ ‡å˜é‡æ·±åº¦åˆ†æ")
            self.analyze_target_variable()
            
            # 6. ç¯å¢ƒå˜é‡ä¸“é¢˜åˆ†æ
            print("\nğŸŒ¡ï¸ 6. ç¯å¢ƒå˜é‡ä¸“é¢˜åˆ†æ")
            self.analyze_environmental_variables()
            
            # 7. æ•°æ®é¢„å¤„ç†éœ€æ±‚åˆ†æ
            print("\nğŸ“Š 7. æ•°æ®é¢„å¤„ç†éœ€æ±‚åˆ†æ")
            self.analyze_preprocessing_requirements()
            
            # 8. é«˜çº§å¯è§†åŒ–ä¸æ´å¯Ÿå‘ç°
            print("\nğŸ¨ 8. é«˜çº§å¯è§†åŒ–ä¸æ´å¯Ÿå‘ç°")
            self.create_advanced_visualizations()
            
            # 9. ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š
            print("\nğŸ“ 9. ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š")
            self.generate_academic_report()
            
            print(f"\nâœ… EDAåˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨ {self.output_dir}")
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„EDAåˆ†æ"""
    print("ğŸ”¥ WildfireSpreadTSæ•°æ®é›†å…¨é¢EDAåˆ†æç³»ç»Ÿ")
    print("=" * 80)
    print("ä¸“ä¸ºè§£å†³NaNå€¼é—®é¢˜å’Œè®ºæ–‡å†™ä½œè®¾è®¡çš„ä¸“ä¸šçº§åˆ†æç³»ç»Ÿ")
    print("=" * 80)
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = WildfireEDAAnalyzer(
            data_dir="data/processed",
            output_dir="professional_eda_results"
        )
        
        # ç”¨æˆ·é€‰æ‹©å¤„ç†æ•°æ®é‡
        print("\nğŸ“‹ é€‰æ‹©æ•°æ®å¤„ç†é‡:")
        print("1. å¿«é€Ÿåˆ†æ (50ä¸ªæ–‡ä»¶)")
        print("2. ä¸­ç­‰åˆ†æ (200ä¸ªæ–‡ä»¶)")  
        print("3. å®Œæ•´åˆ†æ (å…¨éƒ¨607ä¸ªæ–‡ä»¶)")
        print("4. è‡ªå®šä¹‰æ•°é‡")
        
        try:
            choice = input("è¯·é€‰æ‹© (1-4, é»˜è®¤2): ").strip()
            if choice == "1":
                max_files, sample_ratio = 50, 0.08
            elif choice == "3":
                max_files, sample_ratio = 607, 0.02  # å…¨éƒ¨æ–‡ä»¶ï¼Œé™ä½é‡‡æ ·æ¯”ä¾‹
            elif choice == "4":
                max_files = int(input("è¯·è¾“å…¥æ–‡ä»¶æ•°é‡: "))
                sample_ratio = float(input("è¯·è¾“å…¥é‡‡æ ·æ¯”ä¾‹ (0.01-0.1): "))
            else:  # é»˜è®¤é€‰æ‹©2
                max_files, sample_ratio = 200, 0.05
        except:
            max_files, sample_ratio = 200, 0.05  # é»˜è®¤å€¼
        
        print(f"\nğŸ”„ å°†å¤„ç† {max_files} ä¸ªæ–‡ä»¶ï¼Œæ¯æ–‡ä»¶é‡‡æ ·æ¯”ä¾‹ {sample_ratio}")
        
        # æ‰‹åŠ¨åŠ è½½æ•°æ®ï¼ˆè¦†ç›–é»˜è®¤å‚æ•°ï¼‰
        analyzer.load_sample_data(max_files=max_files, sample_ratio=sample_ratio)
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        print("\nğŸš€ å¼€å§‹å…¨é¢EDAåˆ†æ...")
        analyzer.run_complete_analysis()
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()