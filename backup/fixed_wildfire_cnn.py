#!/usr/bin/env python3
"""
16æ ¸CPUä¼˜åŒ–ç‰ˆé‡ç«è®­ç»ƒç³»ç»Ÿ
å……åˆ†åˆ©ç”¨å¤šæ ¸CPUèµ„æºè¿›è¡Œå¹¶è¡Œå¤„ç†
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam, AdamW
import h5py
from pathlib import Path
import json
import time
import matplotlib.pyplot as plt
from sklearn.metrics import average_precision_score, f1_score
import warnings
import gc
import psutil
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from functools import partial
import threading
warnings.filterwarnings('ignore')

# è®¾ç½®CPUä¼˜åŒ–
def optimize_cpu_settings():
    """ä¼˜åŒ–CPUè®¾ç½®"""
    cpu_count = psutil.cpu_count(logical=True)
    print(f"ğŸ–¥ï¸  æ£€æµ‹åˆ° {cpu_count} ä¸ªé€»è¾‘CPUæ ¸å¿ƒ")
    
    # è®¾ç½®çº¿ç¨‹æ•°
    torch.set_num_threads(cpu_count)
    torch.set_num_interop_threads(cpu_count)
    
    # OpenMPè®¾ç½®
    os.environ['OMP_NUM_THREADS'] = str(cpu_count)
    os.environ['MKL_NUM_THREADS'] = str(cpu_count)
    os.environ['NUMEXPR_NUM_THREADS'] = str(cpu_count)
    
    # å¯ç”¨CPUä¼˜åŒ–
    if hasattr(torch.backends.mkl, 'is_available') and torch.backends.mkl.is_available():
        torch.backends.mkl.enabled = True
        print("âœ… å¯ç”¨MKLä¼˜åŒ–")
    
    print(f"âœ… PyTorchçº¿ç¨‹æ•°è®¾ç½®ä¸º: {torch.get_num_threads()}")
    return cpu_count

class MultiCoreDataProcessor:
    """å¤šæ ¸æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, num_workers=None):
        if num_workers is None:
            self.num_workers = min(16, psutil.cpu_count(logical=True))
        else:
            self.num_workers = num_workers
        
        print(f"ğŸš€ å¤šæ ¸å¤„ç†å™¨åˆå§‹åŒ–ï¼Œä½¿ç”¨ {self.num_workers} ä¸ªè¿›ç¨‹")
    
    def parallel_load_files(self, file_paths, target_size=(128, 128)):
        """å¹¶è¡ŒåŠ è½½å¤šä¸ªæ–‡ä»¶"""
        print(f"ğŸ“‚ å¹¶è¡ŒåŠ è½½ {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            load_func = partial(self._load_single_file, target_size=target_size)
            results = list(executor.map(load_func, file_paths))
        
        # è¿‡æ»¤å¤±è´¥çš„ç»“æœ
        valid_results = [r for r in results if r is not None]
        print(f"âœ… æˆåŠŸåŠ è½½ {len(valid_results)} / {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        return valid_results
    
    @staticmethod
    def _load_single_file(file_path, target_size):
        """åŠ è½½å•ä¸ªæ–‡ä»¶ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå¤šè¿›ç¨‹ï¼‰"""
        try:
            with h5py.File(file_path, 'r') as f:
                data = f['data']
                n_timesteps = data.shape[0]
                
                # é‡‡æ ·æ—¶é—´æ­¥
                step = max(1, n_timesteps // 5)  # æ¯ä¸ªæ–‡ä»¶5ä¸ªæ ·æœ¬
                samples = []
                
                for t in range(0, min(n_timesteps, 50), step):  # æœ€å¤š50ä¸ªæ—¶é—´æ­¥
                    try:
                        sample_data = data[t]  # (C, H, W)
                        samples.append({
                            'data': sample_data,
                            'file_path': str(file_path),
                            'timestep': t
                        })
                    except Exception:
                        continue
                
                return samples
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def parallel_preprocess_data(self, raw_samples, target_size=(128, 128)):
        """å¹¶è¡Œé¢„å¤„ç†æ•°æ®"""
        print(f"âš™ï¸  å¹¶è¡Œé¢„å¤„ç† {len(raw_samples)} ä¸ªæ ·æœ¬...")
        
        # ä½¿ç”¨çº¿ç¨‹æ± ï¼ˆå› ä¸ºä¸»è¦æ˜¯numpyæ“ä½œï¼‰
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            preprocess_func = partial(self._preprocess_single_sample, target_size=target_size)
            results = list(executor.map(preprocess_func, raw_samples))
        
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_results = [r for r in results if r is not None]
        print(f"âœ… æˆåŠŸé¢„å¤„ç† {len(valid_results)} / {len(raw_samples)} ä¸ªæ ·æœ¬")
        
        return valid_results
    
    @staticmethod
    def _preprocess_single_sample(sample, target_size):
        """é¢„å¤„ç†å•ä¸ªæ ·æœ¬"""
        try:
            data = sample['data']  # (C, H, W)
            
            # è½¬æ¢ä¸ºtorchå¼ é‡
            data_tensor = torch.from_numpy(data).float()
            
            # Resize
            if data_tensor.shape[1:] != target_size:
                data_resized = F.interpolate(
                    data_tensor.unsqueeze(0),
                    size=target_size,
                    mode='bilinear',
                    align_corners=False
                ).squeeze(0)
            else:
                data_resized = data_tensor
            
            # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
            features = data_resized[:22]  # å‰22ä¸ªé€šé“
            target = data_resized[22:23]  # ç«ç‚¹ç½®ä¿¡åº¦
            
            # æ•°æ®æ¸…æ´—
            features = torch.where(torch.isfinite(features), features, torch.tensor(0.0))
            target = torch.where(torch.isfinite(target), target, torch.tensor(0.0))
            target = (target > 0).float()
            
            return {
                'features': features,
                'target': target,
                'metadata': sample
            }
            
        except Exception as e:
            return None

class CPUOptimizedDataset(Dataset):
    """CPUä¼˜åŒ–çš„æ•°æ®é›†"""
    
    def __init__(self, data_dir, mode='train', max_files=200, target_size=(128, 128), 
                 num_workers=16, preload_data=True):
        self.data_dir = Path(data_dir)
        self.mode = mode
        self.target_size = target_size
        self.num_workers = num_workers
        
        print(f"ğŸ”¥ åˆå§‹åŒ–CPUä¼˜åŒ–æ•°æ®é›† ({mode}æ¨¡å¼)")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨ {num_workers} ä¸ªCPUæ ¸å¿ƒ")
        
        # å¤šæ ¸æ•°æ®å¤„ç†å™¨
        self.processor = MultiCoreDataProcessor(num_workers)
        
        # æ”¶é›†æ–‡ä»¶
        self._collect_files(max_files)
        
        # é¢„åŠ è½½æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if preload_data:
            self._preload_all_data()
        else:
            self.samples = []
            self._build_sample_index()
        
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {len(self)} ä¸ªæ ·æœ¬")
    
    def _collect_files(self, max_files):
        """æ”¶é›†æ–‡ä»¶"""
        all_files = []
        for year in ['2018', '2019', '2020', '2021']:
            year_dir = self.data_dir / year
            if year_dir.exists():
                year_files = list(year_dir.glob("*.hdf5"))
                all_files.extend(year_files)
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡å¹¶åˆ†å‰²
        files_to_use = all_files[:max_files]
        n_files = len(files_to_use)
        
        if self.mode == 'train':
            self.files = files_to_use[:int(0.8 * n_files)]
        else:  # val
            self.files = files_to_use[int(0.8 * n_files):]
        
        print(f"ğŸ“ {self.mode}æ¨¡å¼ä½¿ç”¨ {len(self.files)} ä¸ªæ–‡ä»¶")
    
    def _preload_all_data(self):
        """é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜"""
        print("ğŸš€ å¼€å§‹é¢„åŠ è½½æ•°æ®...")
        start_time = time.time()
        
        # å¹¶è¡ŒåŠ è½½åŸå§‹æ•°æ®
        raw_samples = self.processor.parallel_load_files(self.files, self.target_size)
        
        # å±•å¹³æ ·æœ¬åˆ—è¡¨
        all_raw_samples = []
        for file_samples in raw_samples:
            if file_samples:
                all_raw_samples.extend(file_samples)
        
        print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_raw_samples)} ä¸ªåŸå§‹æ ·æœ¬")
        
        # å¹¶è¡Œé¢„å¤„ç†
        self.samples = self.processor.parallel_preprocess_data(all_raw_samples, self.target_size)
        
        # è®¡ç®—é¢„å¤„ç†ç»Ÿè®¡
        self._compute_normalization_stats()
        
        # åº”ç”¨æ ‡å‡†åŒ–
        self._normalize_samples()
        
        load_time = time.time() - start_time
        print(f"â±ï¸  æ•°æ®é¢„åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
        print(f"ğŸ“Š æ•°æ®åŠ è½½é€Ÿåº¦: {len(self.samples) / load_time:.1f} æ ·æœ¬/ç§’")
    
    def _compute_normalization_stats(self):
        """è®¡ç®—æ ‡å‡†åŒ–ç»Ÿè®¡é‡"""
        print("ğŸ“Š è®¡ç®—æ ‡å‡†åŒ–ç»Ÿè®¡é‡...")
        
        # æ”¶é›†æ‰€æœ‰ç‰¹å¾æ•°æ®
        all_features = []
        sample_size = min(1000, len(self.samples))
        
        for i in range(0, sample_size, 10):
            sample = self.samples[i]
            features = sample['features'][:21]  # è¿ç»­ç‰¹å¾ï¼ˆæ’é™¤land coverï¼‰
            all_features.append(features.flatten())
        
        if all_features:
            all_features = torch.cat(all_features)
            
            # è®¡ç®—ç»Ÿè®¡é‡
            self.feature_mean = float(torch.median(all_features))
            self.feature_std = float(torch.std(all_features))
            self.feature_min = float(torch.quantile(all_features, 0.05))
            self.feature_max = float(torch.quantile(all_features, 0.95))
            
            print(f"âœ… ç‰¹å¾ç»Ÿè®¡: mean={self.feature_mean:.3f}, std={self.feature_std:.3f}")
        else:
            self.feature_mean = 0.0
            self.feature_std = 1.0
            self.feature_min = -5.0
            self.feature_max = 5.0
    
    def _normalize_samples(self):
        """æ ‡å‡†åŒ–æ‰€æœ‰æ ·æœ¬"""
        print("ğŸ”§ æ ‡å‡†åŒ–æ ·æœ¬...")
        
        def normalize_single_sample(sample):
            features = sample['features']
            
            # åˆ†ç¦»è¿ç»­ç‰¹å¾å’ŒåœŸåœ°è¦†ç›–
            continuous_features = features[:21]
            landcover = features[21:22]
            
            # æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾
            continuous_features = torch.clamp(continuous_features, self.feature_min, self.feature_max)
            continuous_features = (continuous_features - self.feature_mean) / (self.feature_std + 1e-8)
            
            # æ¸…ç†åœŸåœ°è¦†ç›–
            landcover = torch.clamp(landcover, 1, 16) - 1  # è½¬æ¢ä¸º[0,15]
            
            # é‡æ–°ç»„åˆ
            sample['features'] = torch.cat([continuous_features, landcover], dim=0)
            return sample
        
        # å¹¶è¡Œæ ‡å‡†åŒ–
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            self.samples = list(executor.map(normalize_single_sample, self.samples))
        
        print("âœ… æ ·æœ¬æ ‡å‡†åŒ–å®Œæˆ")
    
    def _build_sample_index(self):
        """æ„å»ºæ ·æœ¬ç´¢å¼•ï¼ˆå¦‚æœä¸é¢„åŠ è½½æ•°æ®ï¼‰"""
        for file_idx, file_path in enumerate(self.files):
            try:
                with h5py.File(file_path, 'r') as f:
                    n_timesteps = f['data'].shape[0]
                    step = max(1, n_timesteps // 10)
                    for t in range(0, n_timesteps, step):
                        self.samples.append((file_idx, t))
            except Exception:
                continue
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        if isinstance(sample, dict):
            # é¢„åŠ è½½æ¨¡å¼
            return sample['features'], sample['target']
        else:
            # åŠ¨æ€åŠ è½½æ¨¡å¼ï¼ˆä¸æ¨èï¼Œè¿™é‡Œä¸ºå®Œæ•´æ€§ä¿ç•™ï¼‰
            file_idx, timestep = sample
            return self._load_sample_dynamically(file_idx, timestep)
    
    def _load_sample_dynamically(self, file_idx, timestep):
        """åŠ¨æ€åŠ è½½æ ·æœ¬ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            file_path = self.files[file_idx]
            with h5py.File(file_path, 'r') as f:
                data = torch.from_numpy(f['data'][timestep]).float()
                features = data[:22]
                target = data[22:23]
                target = (target > 0).float()
                return features, target
        except Exception:
            return torch.zeros(22, *self.target_size), torch.zeros(1, *self.target_size)

class CPUOptimizedCNN(nn.Module):
    """CPUä¼˜åŒ–çš„CNNæ¨¡å‹"""
    
    def __init__(self, continuous_channels=21, landcover_classes=16, embed_dim=4):
        super().__init__()
        
        # å‡å°‘åµŒå…¥ç»´åº¦ä»¥é€‚åº”CPU
        self.landcover_embedding = nn.Embedding(landcover_classes, embed_dim)
        
        total_channels = continuous_channels + embed_dim
        
        # CPUå‹å¥½çš„è½»é‡çº§æ¶æ„
        self.encoder = nn.Sequential(
            # Block 1
            nn.Conv2d(total_channels, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 128 -> 64
            
            # Block 2
            nn.Conv2d(16, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 64 -> 32
            
            # Block 3
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 32 -> 16
        )
        
        # è½»é‡çº§è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 2, stride=2),   # 16 -> 32
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(32, 16, 2, stride=2),   # 32 -> 64
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            
            nn.ConvTranspose2d(16, 8, 2, stride=2),    # 64 -> 128
            nn.BatchNorm2d(8),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(8, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # åˆ†ç¦»è¿ç»­ç‰¹å¾å’ŒåœŸåœ°è¦†ç›–
        continuous_features = x[:, :21]
        landcover = x[:, 21].long()
        
        # åœŸåœ°è¦†ç›–åµŒå…¥
        landcover_embedded = self.landcover_embedding(landcover).permute(0, 3, 1, 2)
        
        # åˆå¹¶ç‰¹å¾
        combined_features = torch.cat([continuous_features, landcover_embedded], dim=1)
        
        # ç¼–ç -è§£ç 
        encoded = self.encoder(combined_features)
        decoded = self.decoder(encoded)
        
        return decoded

class CPUOptimizedTrainer:
    """CPUä¼˜åŒ–è®­ç»ƒå™¨"""
    
    def __init__(self, model, train_loader, val_loader, num_workers=16):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.num_workers = num_workers
        
        # CPUä¼˜åŒ–è®¾ç½®
        optimize_cpu_settings()
        
        # ä¼˜åŒ–å™¨ï¼ˆCPUå‹å¥½å‚æ•°ï¼‰
        self.criterion = nn.BCELoss()
        self.optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=20, eta_min=1e-6
        )
        
        # å†å²è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.val_auprcs = []
        self.best_val_loss = float('inf')
        
        # æ€§èƒ½ç›‘æ§
        self.epoch_times = []
        self.cpu_usage = []
        
        print(f"ğŸš€ CPUä¼˜åŒ–è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ–¥ï¸  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    def train_epoch(self, epoch):
        """CPUä¼˜åŒ–è®­ç»ƒepoch"""
        self.model.train()
        total_loss = 0
        valid_batches = 0
        epoch_start_time = time.time()
        
        # ç›‘æ§CPUä½¿ç”¨ç‡
        cpu_percent_start = psutil.cpu_percent(interval=None)
        
        print_interval = max(1, len(self.train_loader) // 10)
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            batch_start_time = time.time()
            
            # æ•°æ®éªŒè¯
            if torch.isnan(data).any() or torch.isnan(target).any():
                continue
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output = self.model(data)
            loss = self.criterion(output, target)
            
            if torch.isnan(loss) or torch.isinf(loss):
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            valid_batches += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if batch_idx % print_interval == 0:
                batch_time = time.time() - batch_start_time
                samples_per_sec = data.size(0) / batch_time
                print(f"  Batch {batch_idx}/{len(self.train_loader)}, "
                      f"Loss: {loss.item():.6f}, "
                      f"Speed: {samples_per_sec:.1f} samples/s")
        
        # Epochç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        cpu_percent_end = psutil.cpu_percent(interval=None)
        
        self.epoch_times.append(epoch_time)
        self.cpu_usage.append((cpu_percent_start + cpu_percent_end) / 2)
        
        print(f"\nğŸ“Š Epoch {epoch} ç»Ÿè®¡:")
        print(f"  è®­ç»ƒæ—¶é—´: {epoch_time:.2f}s")
        print(f"  CPUä½¿ç”¨ç‡: {self.cpu_usage[-1]:.1f}%")
        print(f"  ååé‡: {len(self.train_loader.dataset) / epoch_time:.1f} samples/s")
        
        return total_loss / max(valid_batches, 1)
    
    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_targets = []
        valid_batches = 0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                if torch.isnan(data).any() or torch.isnan(target).any():
                    continue
                
                output = self.model(data)
                loss = self.criterion(output, target)
                
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    total_loss += loss.item()
                    valid_batches += 1
                    
                    all_preds.append(output.numpy().flatten())
                    all_targets.append(target.numpy().flatten())
        
        # è®¡ç®—AUPRC
        if all_preds and all_targets:
            preds = np.concatenate(all_preds)
            targets = np.concatenate(all_targets)
            try:
                auprc = average_precision_score(targets, preds)
            except:
                auprc = 0.0
        else:
            auprc = 0.0
        
        return total_loss / max(valid_batches, 1), auprc
    
    def train(self, num_epochs=25):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸ”¥ å¼€å§‹CPUä¼˜åŒ–è®­ç»ƒ {num_epochs} epochs")
        print(f"ğŸ–¥ï¸  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count(logical=True)}")
        print(f"ğŸ§  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / 1024**3:.1f} GB")
        
        save_dir = Path('cpu_optimized_outputs')
        save_dir.mkdir(exist_ok=True)
        
        for epoch in range(num_epochs):
            print(f"\nğŸš€ Epoch {epoch+1}/{num_epochs}")
            print("=" * 50)
            
            # è®­ç»ƒå’ŒéªŒè¯
            train_loss = self.train_epoch(epoch+1)
            val_loss, val_auprc = self.validate()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.val_auprcs.append(val_auprc)
            
            print(f"ğŸ“Š Results:")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"  éªŒè¯AUPRC: {val_auprc:.4f}")
            print(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'val_auprc': val_auprc,
                    'cpu_usage': self.cpu_usage,
                    'epoch_times': self.epoch_times
                }, save_dir / 'best_cpu_model.pth')
                print("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
            
            # å†…å­˜æ¸…ç†
            gc.collect()
        
        # æ€§èƒ½ç»Ÿè®¡
        self._print_performance_stats()
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_curves(save_dir)
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        
        return self.best_val_loss
    
    def _print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡"""
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡epochæ—¶é—´: {np.mean(self.epoch_times):.2f}s")
        print(f"  å¹³å‡CPUä½¿ç”¨ç‡: {np.mean(self.cpu_usage):.1f}%")
        print(f"  æ€»è®­ç»ƒæ—¶é—´: {sum(self.epoch_times):.2f}s")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        print(f"  æœ€ä½³AUPRC: {max(self.val_auprcs):.4f}")
    
    def plot_curves(self, save_dir):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿å’Œæ€§èƒ½å›¾"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='è®­ç»ƒæŸå¤±')
        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('æŸå¤±æ›²çº¿')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # AUPRC
        axes[0, 1].plot(epochs, self.val_auprcs, 'g-', label='éªŒè¯AUPRC')
        axes[0, 1].set_title('AUPRCæ›²çº¿')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # CPUä½¿ç”¨ç‡
        axes[0, 2].plot(epochs, self.cpu_usage, 'orange', label='CPUä½¿ç”¨ç‡')
        axes[0, 2].set_title('CPUä½¿ç”¨ç‡')
        axes[0, 2].set_ylabel('ä½¿ç”¨ç‡ (%)')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # Epochæ—¶é—´
        axes[1, 0].plot(epochs, self.epoch_times, 'purple', label='Epochæ—¶é—´')
        axes[1, 0].set_title('æ¯Epochè®­ç»ƒæ—¶é—´')
        axes[1, 0].set_ylabel('æ—¶é—´ (ç§’)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # ååé‡
        throughput = [len(self.train_loader.dataset) / t for t in self.epoch_times]
        axes[1, 1].plot(epochs, throughput, 'brown', label='è®­ç»ƒååé‡')
        axes[1, 1].set_title('è®­ç»ƒååé‡')
        axes[1, 1].set_ylabel('æ ·æœ¬/ç§’')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats_text = f"""
æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}
æœ€ä½³AUPRC: {max(self.val_auprcs):.4f}
å¹³å‡epochæ—¶é—´: {np.mean(self.epoch_times):.2f}s
å¹³å‡CPUä½¿ç”¨ç‡: {np.mean(self.cpu_usage):.1f}%
æ€»è®­ç»ƒæ—¶é—´: {sum(self.epoch_times)/3600:.2f}h
CPUæ ¸å¿ƒæ•°: {psutil.cpu_count(logical=True)}
        """
        axes[1, 2].text(0.1, 0.5, stats_text, transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].set_title('è®­ç»ƒç»Ÿè®¡')
        axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'cpu_training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    """ä¸»å‡½æ•° - 16æ ¸CPUä¼˜åŒ–ç‰ˆ"""
    print("ğŸ”¥ 16æ ¸CPUä¼˜åŒ–é‡ç«è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    # CPUä¼˜åŒ–é…ç½®
    cpu_count = optimize_cpu_settings()
    
    config = {
        'data_dir': 'data/processed',
        'batch_size': 16,                    # CPUå¯ä»¥å¤„ç†æ›´å¤§batch
        'num_epochs': 25,
        'max_files': 300,                    # å¢åŠ æ–‡ä»¶æ•°é‡
        'target_size': (128, 128),
        'num_workers': cpu_count,            # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        'preload_data': True,                # é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜
        'device': 'cpu'
    }
    
    print(f"\né…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = Path(config['data_dir'])
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åˆ›å»ºCPUä¼˜åŒ–æ•°æ®é›†
    print(f"\n1. åˆ›å»ºCPUä¼˜åŒ–æ•°æ®é›†...")
    train_dataset = CPUOptimizedDataset(
        config['data_dir'],
        mode='train',
        max_files=config['max_files'],
        target_size=config['target_size'],
    train_dataset = CPUOptimizedDataset(
        config['data_dir'],
        mode='train',
        max_files=config['max_files'],
        target_size=config['target_size'],
        num_workers=config['num_workers'],
        preload_data=config['preload_data']
    )
    
    val_dataset = CPUOptimizedDataset(
        config['data_dir'],
        mode='val',
        max_files=config['max_files']//4,
        target_size=config['target_size'],
        num_workers=config['num_workers'],
        preload_data=config['preload_data']
    )
    
    # å¤åˆ¶æ ‡å‡†åŒ–å‚æ•°
    if hasattr(train_dataset, 'feature_mean'):
        val_dataset.feature_mean = train_dataset.feature_mean
        val_dataset.feature_std = train_dataset.feature_std
        val_dataset.feature_min = train_dataset.feature_min
        val_dataset.feature_max = train_dataset.feature_max
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆCPUä¼˜åŒ–ï¼‰
    print(f"\n2. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=min(8, config['num_workers']//2),  # ä¸ºæ•°æ®åŠ è½½åˆ†é…ä¸€åŠæ ¸å¿ƒ
        pin_memory=False,                              # CPUè®­ç»ƒä¸éœ€è¦pin_memory
        persistent_workers=True,
        prefetch_factor=4
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'] * 2,
        shuffle=False,
        num_workers=min(4, config['num_workers']//4),
        pin_memory=False,
        persistent_workers=True,
        prefetch_factor=2
    )
    
    print(f"âœ… è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ… éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # åˆ›å»ºCPUä¼˜åŒ–æ¨¡å‹
    print(f"\n3. åˆ›å»ºCPUä¼˜åŒ–æ¨¡å‹...")
    model = CPUOptimizedCNN(
        continuous_channels=21,
        landcover_classes=16,
        embed_dim=4  # å‡å°‘åµŒå…¥ç»´åº¦ä»¥é€‚åº”CPU
    )
    
    print(f"âœ… æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print(f"\n4. åˆ›å»ºCPUä¼˜åŒ–è®­ç»ƒå™¨...")
    trainer = CPUOptimizedTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_workers=config['num_workers']
    )
    
    # æ€§èƒ½æµ‹è¯•
    print(f"\n5. æ€§èƒ½æµ‹è¯•...")
    model.eval()
    with torch.no_grad():
        for data, target in train_loader:
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"  è¾“å…¥å½¢çŠ¶: {data.shape}")
            print(f"  ç›®æ ‡å½¢çŠ¶: {target.shape}")
            print(f"  è¾“å…¥èŒƒå›´: [{data.min().item():.3f}, {data.max().item():.3f}]")
            print(f"  ç›®æ ‡èŒƒå›´: [{target.min().item():.3f}, {target.max().item():.3f}]")
            
            # æµ‹è¯•æ¨ç†é€Ÿåº¦
            start_time = time.time()
            output = model(data)
            inference_time = time.time() - start_time
            
            print(f"  æ¨ç†æ—¶é—´: {inference_time:.3f}s")
            print(f"  æ¨ç†é€Ÿåº¦: {data.size(0) / inference_time:.1f} samples/s")
            print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"  è¾“å‡ºèŒƒå›´: [{output.min().item():.3f}, {output.max().item():.3f}]")
            
            break
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\n6. å¼€å§‹CPUä¼˜åŒ–è®­ç»ƒ...")
    print(f"ğŸ–¥ï¸  ç³»ç»Ÿä¿¡æ¯:")
    print(f"  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count(logical=True)}")
    print(f"  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / 1024**3:.1f} GB")
    print(f"  PyTorchçº¿ç¨‹æ•°: {torch.get_num_threads()}")
    
    # è®­ç»ƒ
    best_loss = trainer.train(num_epochs=config['num_epochs'])
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹: cpu_optimized_outputs/best_cpu_model.pth")
    print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿: cpu_optimized_outputs/cpu_training_curves.png")
    print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")


def benchmark_cpu_performance():
    """CPUæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nğŸ”¬ CPUæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 40)
    
    # æµ‹è¯•ä¸åŒæ ¸å¿ƒæ•°çš„æ€§èƒ½
    cpu_counts = [1, 4, 8, 16]
    results = {}
    
    for num_cores in cpu_counts:
        if num_cores <= psutil.cpu_count(logical=True):
            print(f"\næµ‹è¯• {num_cores} æ ¸å¿ƒ...")
            
            # è®¾ç½®çº¿ç¨‹æ•°
            torch.set_num_threads(num_cores)
            
            # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œæ•°æ®
            model = CPUOptimizedCNN()
            test_data = torch.randn(8, 22, 128, 128)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(10):
                    _ = model(test_data)
            
            # è®¡æ—¶æµ‹è¯•
            start_time = time.time()
            with torch.no_grad():
                for _ in range(50):
                    _ = model(test_data)
            
            total_time = time.time() - start_time
            avg_time = total_time / 50
            throughput = test_data.size(0) / avg_time
            
            results[num_cores] = {
                'avg_time': avg_time,
                'throughput': throughput
            }
            
            print(f"  å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.4f}s")
            print(f"  ååé‡: {throughput:.1f} samples/s")
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print("æ ¸å¿ƒæ•° | æ¨ç†æ—¶é—´(s) | ååé‡(samples/s) | ç›¸å¯¹åŠ é€Ÿ")
    print("-" * 50)
    
    baseline_time = results[1]['avg_time'] if 1 in results else None
    
    for cores, result in results.items():
        speedup = baseline_time / result['avg_time'] if baseline_time else 1.0
        print(f"{cores:6d} | {result['avg_time']:11.4f} | {result['throughput']:15.1f} | {speedup:8.2f}x")


def optimize_for_specific_cpu():
    """é’ˆå¯¹ç‰¹å®šCPUæ¶æ„ä¼˜åŒ–"""
    print("\nğŸ”§ CPUæ¶æ„ä¼˜åŒ–")
    print("=" * 40)
    
    # æ£€æµ‹CPUä¿¡æ¯
    cpu_info = {}
    try:
        with open('/proc/cpuinfo', 'r') as f:
            lines = f.readlines()
            for line in lines:
                if 'model name' in line:
                    cpu_info['name'] = line.split(':')[1].strip()
                    break
    except:
        cpu_info['name'] = "Unknown"
    
    cpu_info['physical_cores'] = psutil.cpu_count(logical=False)
    cpu_info['logical_cores'] = psutil.cpu_count(logical=True)
    cpu_info['frequency'] = psutil.cpu_freq().max if psutil.cpu_freq() else "Unknown"
    
    print(f"CPUä¿¡æ¯:")
    print(f"  å‹å·: {cpu_info['name']}")
    print(f"  ç‰©ç†æ ¸å¿ƒ: {cpu_info['physical_cores']}")
    print(f"  é€»è¾‘æ ¸å¿ƒ: {cpu_info['logical_cores']}")
    print(f"  æœ€å¤§é¢‘ç‡: {cpu_info['frequency']} MHz")
    
    # åŸºäºCPUç‰¹æ€§æä¾›ä¼˜åŒ–å»ºè®®
    suggestions = []
    
    if cpu_info['logical_cores'] >= 16:
        suggestions.append("âœ… é«˜æ ¸å¿ƒæ•°CPUï¼Œé€‚åˆå¤§batch sizeå’Œå¤šè¿›ç¨‹æ•°æ®åŠ è½½")
        suggestions.append("ğŸ’¡ å»ºè®®: batch_size=16-32, num_workers=12-16")
    
    if cpu_info['logical_cores'] > cpu_info['physical_cores']:
        suggestions.append("âœ… æ”¯æŒè¶…çº¿ç¨‹ï¼Œå¯ä»¥è®¾ç½®çº¿ç¨‹æ•°=é€»è¾‘æ ¸å¿ƒæ•°")
        suggestions.append("ğŸ’¡ å»ºè®®: torch.set_num_threads(logical_cores)")
    
    # Intelç‰¹å®šä¼˜åŒ–
    if 'Intel' in cpu_info['name']:
        suggestions.append("âœ… Intel CPUï¼Œå¯å¯ç”¨MKLä¼˜åŒ–")
        suggestions.append("ğŸ’¡ å»ºè®®: è®¾ç½®MKL_NUM_THREADS=ç‰©ç†æ ¸å¿ƒæ•°")
    
    # AMDç‰¹å®šä¼˜åŒ–
    if 'AMD' in cpu_info['name']:
        suggestions.append("âœ… AMD CPUï¼Œå»ºè®®ä½¿ç”¨OpenBLAS")
        suggestions.append("ğŸ’¡ å»ºè®®: è®¾ç½®OMP_NUM_THREADS=ç‰©ç†æ ¸å¿ƒæ•°")
    
    print(f"\nä¼˜åŒ–å»ºè®®:")
    for suggestion in suggestions:
        print(f"  {suggestion}")


def memory_usage_monitor():
    """å†…å­˜ä½¿ç”¨ç›‘æ§"""
    print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ:")
    
    # ç³»ç»Ÿå†…å­˜
    memory = psutil.virtual_memory()
    print(f"  ç³»ç»Ÿå†…å­˜:")
    print(f"    æ€»è®¡: {memory.total / 1024**3:.1f} GB")
    print(f"    å¯ç”¨: {memory.available / 1024**3:.1f} GB")
    print(f"    ä½¿ç”¨ç‡: {memory.percent:.1f}%")
    
    # è¿›ç¨‹å†…å­˜
    process = psutil.Process()
    process_memory = process.memory_info()
    print(f"  è¿›ç¨‹å†…å­˜:")
    print(f"    RSS: {process_memory.rss / 1024**2:.1f} MB")
    print(f"    VMS: {process_memory.vms / 1024**2:.1f} MB")
    
    # PyTorchå†…å­˜ï¼ˆå¦‚æœä½¿ç”¨CUDAï¼‰
    if torch.cuda.is_available():
        print(f"  GPUå†…å­˜:")
        print(f"    å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        print(f"    ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**2:.1f} MB")


if __name__ == "__main__":
    print("ğŸ”¥ å¤šæ ¸CPUé‡ç«è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    # ç³»ç»Ÿä¿¡æ¯
    print(f"ğŸ–¥ï¸  ç³»ç»Ÿä¿¡æ¯:")
    print(f"  Pythonç‰ˆæœ¬: {os.sys.version}")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count(logical=True)}")
    print(f"  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / 1024**3:.1f} GB")
    
    # CPUæ¶æ„ä¼˜åŒ–
    optimize_for_specific_cpu()
    
    # å†…å­˜ç›‘æ§
    memory_usage_monitor()
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•(å¯é€‰)
    benchmark_choice = input("\næ˜¯å¦è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•? (y/N): ").lower()
    if benchmark_choice == 'y':
        benchmark_cpu_performance()
    
    # è¿è¡Œä¸»ç¨‹åº
    print(f"\n" + "="*60)
    main()
    
    # æœ€ç»ˆå†…å­˜ç›‘æ§
    print(f"\næœ€ç»ˆå†…å­˜ä½¿ç”¨:")
    memory_usage_monitor()
    
    print(f"\nğŸ‰ ç¨‹åºå®Œæˆ!")
    print(f"ğŸ’¡ æç¤º: å¦‚æœè®­ç»ƒé€Ÿåº¦ä»ç„¶è¾ƒæ…¢ï¼Œå¯ä»¥:")
    print(f"  1. å‡å°‘å›¾åƒå°ºå¯¸ (å¦‚128x128 -> 96x96)")
    print(f"  2. å¢åŠ batch size")
    print(f"  3. å‡å°‘æ¨¡å‹å¤æ‚åº¦")
    print(f"  4. ä½¿ç”¨æ•°æ®é¢„å¤„ç†ç¼“å­˜")