#!/usr/bin/env python3
"""
GPUä¼˜åŒ–çš„é‡ç«CNNè®­ç»ƒç³»ç»Ÿ
ä¸“é—¨è§£å†³è®­ç»ƒé€Ÿåº¦é€’å‡é—®é¢˜ï¼Œå……åˆ†åˆ©ç”¨RTX 4070 Tiçš„12GBæ˜¾å­˜
"""

import os
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import h5py
from pathlib import Path
import json
import time
import matplotlib.pyplot as plt
from sklearn.metrics import average_precision_score, f1_score
import warnings
warnings.filterwarnings('ignore')

# GPUä¼˜åŒ–è®¾ç½®
torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨ä¼˜åŒ–å·ç§¯ç®—æ³•
torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§æ“ä½œä»¥æé«˜æ€§èƒ½

class GPUOptimizedWildfireDataset(Dataset):
    """GPUä¼˜åŒ–çš„é‡ç«æ•°æ®é›†"""
    
    def __init__(self, data_dir, mode='train', max_files=300, target_size=(256, 256)):
        self.data_dir = Path(data_dir)
        self.mode = mode
        self.target_size = target_size
        
        print(f"ğŸ”¥ åˆå§‹åŒ–{mode}æ•°æ®é›† (GPUä¼˜åŒ–ç‰ˆ)...")
        
        # æ”¶é›†HDF5æ–‡ä»¶
        all_files = []
        for year in ['2018', '2019', '2020', '2021']:
            year_dir = self.data_dir / year
            if year_dir.exists():
                year_files = list(year_dir.glob("*.hdf5"))
                all_files.extend(year_files)
        
        # ä½¿ç”¨æ›´å¤šæ–‡ä»¶å……åˆ†åˆ©ç”¨GPU
        files_to_use = all_files[:max_files]
        n_files = len(files_to_use)
        
        if mode == 'train':
            self.files = files_to_use[:int(0.8 * n_files)]
        else:
            self.files = files_to_use[int(0.8 * n_files):]
        
        print(f"ğŸ“ {mode}æ¨¡å¼: {len(self.files)} æ–‡ä»¶")
        
        # æ„å»ºæ ·æœ¬ç´¢å¼•
        self.samples = []
        self._build_samples()
        
        # é¢„è®¡ç®—ç»Ÿè®¡é‡
        if mode == 'train':
            self._compute_stats()
    
    def _build_samples(self):
        """é«˜æ•ˆæ„å»ºæ ·æœ¬ç´¢å¼•"""
        for file_idx, file_path in enumerate(self.files):
            try:
                with h5py.File(file_path, 'r') as f:
                    n_timesteps = f['data'].shape[0]
                    # æ¯ä¸ªæ–‡ä»¶é‡‡æ ·æ›´å¤šæ ·æœ¬
                    step = max(1, n_timesteps // 8)
                    for t in range(0, n_timesteps, step):
                        self.samples.append((file_idx, t))
            except:
                continue
        
        print(f"ğŸ“Š æ„å»º {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def _compute_stats(self):
        """é¢„è®¡ç®—æ•°æ®ç»Ÿè®¡é‡"""
        print("ğŸ“ˆ è®¡ç®—æ•°æ®ç»Ÿè®¡é‡...")
        
        sample_data = []
        for i in range(0, min(100, len(self.samples)), 5):
            try:
                file_idx, timestep = self.samples[i]
                with h5py.File(self.files[file_idx], 'r') as f:
                    data = f['data'][timestep][:22]
                    sample_data.append(data.flatten())
            except:
                continue
        
        if sample_data:
            all_data = np.concatenate(sample_data)
            valid_data = all_data[np.isfinite(all_data)]
            
            if len(valid_data) > 0:
                self.data_mean = float(np.median(valid_data))
                self.data_std = float(np.std(valid_data))
                self.data_min = float(np.percentile(valid_data, 1))
                self.data_max = float(np.percentile(valid_data, 99))
            else:
                self._set_default_stats()
        else:
            self._set_default_stats()
        
        print(f"ğŸ“Š ç»Ÿè®¡: mean={self.data_mean:.3f}, std={self.data_std:.3f}")
    
    def _set_default_stats(self):
        """è®¾ç½®é»˜è®¤ç»Ÿè®¡é‡"""
        self.data_mean = 0.0
        self.data_std = 1.0
        self.data_min = -5.0
        self.data_max = 5.0
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        file_idx, timestep = self.samples[idx]
        
        try:
            with h5py.File(self.files[file_idx], 'r') as f:
                data = f['data'][timestep]
                
                # å¿«é€Ÿæå–ç‰¹å¾å’Œç›®æ ‡
                features = torch.from_numpy(data[:22]).float()
                target = torch.from_numpy(data[22:23]).float()
                
                # GPUä¼˜åŒ–çš„resize
                features = F.interpolate(
                    features.unsqueeze(0), 
                    size=self.target_size, 
                    mode='bilinear', 
                    align_corners=False
                ).squeeze(0)
                
                target = F.interpolate(
                    target.unsqueeze(0), 
                    size=self.target_size, 
                    mode='bilinear', 
                    align_corners=False
                ).squeeze(0)
                
                # é«˜æ•ˆæ•°æ®æ¸…æ´—
                features = torch.clamp(features, self.data_min, self.data_max)
                features = (features - self.data_mean) / (self.data_std + 1e-8)
                target = (target > 0).float()
                
                return features, target
                
        except:
            return (torch.zeros(22, *self.target_size), 
                   torch.zeros(1, *self.target_size))


class GPUOptimizedCNN(nn.Module):
    """GPUä¼˜åŒ–çš„CNNæ¨¡å‹ - å……åˆ†åˆ©ç”¨RTX 4070 Ti"""
    
    def __init__(self, input_channels=22, base_channels=64):
        super().__init__()
        
        # å¢å¤§ç½‘ç»œå®¹é‡å……åˆ†åˆ©ç”¨GPU
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(input_channels, base_channels, 3, padding=1),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels, base_channels, 3, padding=1),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 2
            nn.Conv2d(base_channels, base_channels * 2, 3, padding=1),
            nn.BatchNorm2d(base_channels * 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels * 2, base_channels * 2, 3, padding=1),
            nn.BatchNorm2d(base_channels * 2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 3
            nn.Conv2d(base_channels * 2, base_channels * 4, 3, padding=1),
            nn.BatchNorm2d(base_channels * 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels * 4, base_channels * 4, 3, padding=1),
            nn.BatchNorm2d(base_channels * 4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 4
            nn.Conv2d(base_channels * 4, base_channels * 8, 3, padding=1),
            nn.BatchNorm2d(base_channels * 8),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )
        
        self.decoder = nn.Sequential(
            # Upsample 1
            nn.ConvTranspose2d(base_channels * 8, base_channels * 4, 2, stride=2),
            nn.BatchNorm2d(base_channels * 4),
            nn.ReLU(inplace=True),
            
            # Upsample 2
            nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 2, stride=2),
            nn.BatchNorm2d(base_channels * 2),
            nn.ReLU(inplace=True),
            
            # Upsample 3
            nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
            
            # Upsample 4
            nn.ConvTranspose2d(base_channels, base_channels // 2, 2, stride=2),
            nn.BatchNorm2d(base_channels // 2),
            nn.ReLU(inplace=True),
            
            # Output
            nn.Conv2d(base_channels // 2, 1, 3, padding=1),
            nn.Sigmoid()
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.features(x)
        x = self.decoder(x)
        return x


class FocalLoss(nn.Module):
    """Focal Losså¤„ç†ç±»åˆ«ä¸å¹³è¡¡ - ä¿®å¤æ··åˆç²¾åº¦å…¼å®¹æ€§"""
    
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, pred, target):
        pred = pred.view(-1)
        target = target.view(-1)
        
        # ä½¿ç”¨logitsç‰ˆæœ¬çš„BCEï¼Œå…¼å®¹æ··åˆç²¾åº¦
        # å‡è®¾è¾“å…¥predæ˜¯ç»è¿‡sigmoidçš„æ¦‚ç‡ï¼Œéœ€è¦è½¬æ¢å›logits
        pred = torch.clamp(pred, 1e-7, 1-1e-7)
        logits = torch.log(pred / (1 - pred))  # åsigmoidå˜æ¢
        
        # è®¡ç®—BCE with logits (æ··åˆç²¾åº¦å®‰å…¨)
        bce = F.binary_cross_entropy_with_logits(logits, target, reduction='none')
        
        # è®¡ç®—pt (ä½¿ç”¨åŸå§‹æ¦‚ç‡)
        pt = torch.where(target == 1, pred, 1 - pred)
        
        # è®¡ç®—alpha_t
        alpha_t = torch.where(target == 1, self.alpha, 1 - self.alpha)
        
        # Focalæƒé‡
        focal_weight = alpha_t * (1 - pt) ** self.gamma
        
        return (focal_weight * bce).mean()


class GPUOptimizedTrainer:
    """GPUä¼˜åŒ–çš„è®­ç»ƒå™¨ - è§£å†³é€Ÿåº¦é€’å‡é—®é¢˜"""
    
    def __init__(self, model, train_loader, val_loader):
        self.device = torch.device('cuda')
        self.model = model.to(self.device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        
        # ä¼˜åŒ–çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = FocalLoss(alpha=0.25, gamma=2.0)
        self.optimizer = AdamW(
            model.parameters(), 
            lr=2e-4,  # æ›´å¤§çš„å­¦ä¹ ç‡åˆ©ç”¨GPU
            weight_decay=1e-4,
            eps=1e-8
        )
        
        # Cosineå­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=50, eta_min=1e-6)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = torch.cuda.amp.GradScaler()
        
        # å†å²è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.val_auprcs = []
        self.epoch_times = []
        self.best_val_loss = float('inf')
        
        print(f"ğŸš€ GPUä¼˜åŒ–è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        print(f"ğŸ§  æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    
    def train_epoch(self):
        """GPUä¼˜åŒ–çš„è®­ç»ƒepoch"""
        self.model.train()
        
        total_loss = 0.0
        valid_batches = 0
        
        # æ¯ä¸ªepochå¼€å§‹æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            # æ•°æ®æœ‰æ•ˆæ€§å¿«é€Ÿæ£€æŸ¥
            if torch.isnan(data).any() or torch.isnan(target).any():
                continue
            
            # éé˜»å¡GPUä¼ è¾“
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)
            
            # é‡è¦ï¼šå®Œå…¨æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad(set_to_none=True)
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast():
                output = self.model(data)
                loss = self.criterion(output, target)
            
            # æŸå¤±æœ‰æ•ˆæ€§æ£€æŸ¥
            if torch.isnan(loss) or torch.isinf(loss):
                continue
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–å™¨æ­¥éª¤
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # è®°å½•æŸå¤±
            total_loss += loss.item()
            valid_batches += 1
            
            # å®šæœŸæ¸…ç†å’Œæ‰“å°
            if batch_idx % 25 == 0:
                torch.cuda.empty_cache()
                print(f"  ğŸ“Š Batch {batch_idx}/{len(self.train_loader)}, "
                      f"Loss: {loss.item():.6f}, "
                      f"GPUå†…å­˜: {torch.cuda.memory_allocated() / (1024**3):.1f} GB")
            
            # æ˜¾å¼åˆ é™¤å˜é‡
            del output, loss
        
        return total_loss / max(valid_batches, 1)
    
    def validate(self):
        """GPUä¼˜åŒ–çš„éªŒè¯"""
        self.model.eval()
        
        total_loss = 0.0
        valid_batches = 0
        
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„é¢„æµ‹æ”¶é›†æ–¹å¼
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in self.val_loader:
                if torch.isnan(data).any() or torch.isnan(target).any():
                    continue
                
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                with torch.cuda.amp.autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    total_loss += loss.item()
                    valid_batches += 1
                    
                    # é«˜æ•ˆæ”¶é›†é¢„æµ‹ç»“æœ
                    all_preds.append(output.detach().cpu().numpy())
                    all_targets.append(target.detach().cpu().numpy())
                
                del output, loss
        
        # è®¡ç®—AUPRC
        if all_preds and all_targets:
            preds = np.concatenate(all_preds).flatten()
            targets = np.concatenate(all_targets).flatten()
            
            try:
                auprc = average_precision_score(targets, preds)
            except:
                auprc = 0.0
            
            del preds, targets, all_preds, all_targets
        else:
            auprc = 0.0
        
        # å¼ºåˆ¶æ¸…ç†å†…å­˜
        gc.collect()
        torch.cuda.empty_cache()
        
        avg_loss = total_loss / max(valid_batches, 1)
        return avg_loss, auprc
    
    def train(self, num_epochs=30):
        """GPUä¼˜åŒ–çš„è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹GPUä¼˜åŒ–è®­ç»ƒ {num_epochs} epochs")
        print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name()}")
        
        save_dir = Path('gpu_wildfire_outputs')
        save_dir.mkdir(exist_ok=True)
        
        for epoch in range(num_epochs):
            epoch_start = time.time()
            
            print(f"\nğŸ”¥ Epoch {epoch+1}/{num_epochs}")
            print("-" * 60)
            
            # è®­ç»ƒå’ŒéªŒè¯
            train_loss = self.train_epoch()
            val_loss, val_auprc = self.validate()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•æ—¶é—´å’ŒæŒ‡æ ‡
            epoch_time = time.time() - epoch_start
            self.epoch_times.append(epoch_time)
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.val_auprcs.append(val_auprc)
            
            # æ‰“å°ç»“æœ
            current_lr = self.optimizer.param_groups[0]['lr']
            print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"ğŸ“‰ éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"ğŸ¯ éªŒè¯AUPRC: {val_auprc:.4f}")
            print(f"â±ï¸  Epochæ—¶é—´: {epoch_time:.1f}s")
            print(f"ğŸ›ï¸  å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"ğŸ’¾ GPUå†…å­˜å³°å€¼: {torch.cuda.max_memory_allocated() / (1024**3):.1f} GB")
            
            # æ€§èƒ½ç›‘æ§
            if len(self.epoch_times) >= 5:
                recent_avg = np.mean(self.epoch_times[-3:])
                initial_avg = np.mean(self.epoch_times[:3])
                if recent_avg > initial_avg * 1.3:
                    print(f"âš ï¸  è®­ç»ƒé€Ÿåº¦ä¸‹é™: åˆå§‹ {initial_avg:.1f}s -> å½“å‰ {recent_avg:.1f}s")
                else:
                    print(f"âœ… è®­ç»ƒé€Ÿåº¦ç¨³å®š: ~{recent_avg:.1f}s/epoch")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'val_auprc': val_auprc,
                    'epoch_times': self.epoch_times
                }, save_dir / 'best_gpu_model.pth')
                print("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
            
            # å®šæœŸæ¸…ç†
            if (epoch + 1) % 5 == 0:
                print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
                gc.collect()
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
        
        # ç»˜åˆ¶æ€§èƒ½åˆ†æ
        self.plot_performance_analysis(save_dir)
        print(f"\nğŸ‰ GPUä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ’° æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
    
    def plot_performance_analysis(self, save_dir):
        """ç»˜åˆ¶è¯¦ç»†çš„æ€§èƒ½åˆ†æ"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        axes[0, 0].set_title('æŸå¤±æ›²çº¿', fontsize=14)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # AUPRCæ›²çº¿
        axes[0, 1].plot(epochs, self.val_auprcs, 'g-', label='éªŒè¯AUPRC', linewidth=2)
        axes[0, 1].set_title('AUPRCæ›²çº¿', fontsize=14)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('AUPRC')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # è®­ç»ƒæ—¶é—´åˆ†æ
        axes[0, 2].plot(epochs, self.epoch_times, 'purple', marker='o', markersize=3)
        axes[0, 2].set_title('æ¯Epochè®­ç»ƒæ—¶é—´', fontsize=14)
        axes[0, 2].set_xlabel('Epoch')
        axes[0, 2].set_ylabel('æ—¶é—´ (ç§’)')
        axes[0, 2].grid(True, alpha=0.3)
        
        # æ·»åŠ æ—¶é—´è¶‹åŠ¿çº¿
        if len(self.epoch_times) > 1:
            z = np.polyfit(epochs, self.epoch_times, 1)
            p = np.poly1d(z)
            axes[0, 2].plot(epochs, p(epochs), 'r--', alpha=0.8, 
                           label=f'è¶‹åŠ¿: {z[0]:.3f}s/epoch')
            axes[0, 2].legend()
        
        # å­¦ä¹ ç‡æ›²çº¿
        lrs = [self.optimizer.param_groups[0]['lr']]
        for _ in range(1, len(epochs)):
            lrs.append(lrs[-1] * 0.99)  # è¿‘ä¼¼cosineè°ƒåº¦
        
        axes[1, 0].plot(epochs, lrs, 'orange', linewidth=2)
        axes[1, 0].set_title('å­¦ä¹ ç‡è°ƒåº¦', fontsize=14)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æŸå¤±æ”¹å–„åˆ†æ
        train_improvement = [(self.train_losses[0] - loss) / self.train_losses[0] * 100 
                           for loss in self.train_losses]
        val_improvement = [(self.val_losses[0] - loss) / self.val_losses[0] * 100 
                         for loss in self.val_losses]
        
        axes[1, 1].plot(epochs, train_improvement, 'b-', label='è®­ç»ƒæŸå¤±æ”¹å–„%')
        axes[1, 1].plot(epochs, val_improvement, 'r-', label='éªŒè¯æŸå¤±æ”¹å–„%')
        axes[1, 1].set_title('æŸå¤±æ”¹å–„ç™¾åˆ†æ¯”', fontsize=14)
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('æ”¹å–„ (%)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # ç»Ÿè®¡ä¿¡æ¯
        if self.epoch_times:
            avg_time = np.mean(self.epoch_times)
            min_time = np.min(self.epoch_times)
            max_time = np.max(self.epoch_times)
            std_time = np.std(self.epoch_times)
            best_auprc = max(self.val_auprcs)
            
            stats_text = f"""ğŸš€ GPUè®­ç»ƒç»Ÿè®¡:
            
å¹³å‡æ—¶é—´: {avg_time:.1f}s/epoch
æœ€çŸ­æ—¶é—´: {min_time:.1f}s
æœ€é•¿æ—¶é—´: {max_time:.1f}s
æ—¶é—´æ ‡å‡†å·®: {std_time:.1f}s
æ—¶é—´å˜åŒ–ç‡: {((max_time-min_time)/min_time*100):.1f}%

æœ€ä½³AUPRC: {best_auprc:.4f}
æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}
æ€»è®­ç»ƒæ—¶é—´: {sum(self.epoch_times)/3600:.2f}å°æ—¶

GPU: {torch.cuda.get_device_name()}
æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory/(1024**3):.1f}GB"""
            
            axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes, 
                           fontsize=10, verticalalignment='top', fontfamily='monospace')
            axes[1, 2].set_title('GPUè®­ç»ƒç»Ÿè®¡', fontsize=14)
            axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_dir / 'gpu_performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()


def main():
    """GPUä¼˜åŒ–ä¸»å‡½æ•°"""
    print("ğŸ”¥ GPUä¼˜åŒ–é‡ç«CNNè®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print("ğŸ¯ ä¸“é—¨è§£å†³è®­ç»ƒé€Ÿåº¦é€’å‡é—®é¢˜")
    print("âš¡ å……åˆ†åˆ©ç”¨RTX 4070 Ti 12GBæ˜¾å­˜")
    print("=" * 60)
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
        return
    
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    
    # GPUä¼˜åŒ–é…ç½®
    config = {
        'data_dir': 'data/processed',
        'batch_size': 8,  # åˆ©ç”¨å¤§æ˜¾å­˜
        'num_epochs': 30,
        'max_files': 300,  # æ›´å¤šæ•°æ®
        'target_size': (256, 256),  # æ›´å¤§å›¾åƒ
        'base_channels': 64,  # æ›´å¤§ç½‘ç»œ
    }
    
    print(f"\nğŸ“‹ GPUä¼˜åŒ–é…ç½®:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # æ£€æŸ¥æ•°æ®
    data_dir = Path(config['data_dir'])
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åˆ›å»ºGPUä¼˜åŒ–æ•°æ®é›†
    print("\nğŸ“ åˆ›å»ºGPUä¼˜åŒ–æ•°æ®é›†...")
    train_dataset = GPUOptimizedWildfireDataset(
        config['data_dir'], 
        mode='train',
        max_files=config['max_files'],
        target_size=config['target_size']
    )
    
    val_dataset = GPUOptimizedWildfireDataset(
        config['data_dir'], 
        mode='val',
        max_files=config['max_files']//4,
        target_size=config['target_size']
    )
    
    # å¤åˆ¶ç»Ÿè®¡é‡
    if hasattr(train_dataset, 'data_mean'):
        for attr in ['data_mean', 'data_std', 'data_min', 'data_max']:
            setattr(val_dataset, attr, getattr(train_dataset, attr))
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset):,} æ ·æœ¬")
    
    # åˆ›å»ºGPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨
    print("\nâš¡ åˆ›å»ºGPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨...")
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½
        pin_memory=True,  # å›ºå®šå†…å­˜
        persistent_workers=True,  # æŒä¹…åŒ–worker
        drop_last=True,
        prefetch_factor=2  # é¢„å–å› å­
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False, 
        num_workers=2,
        pin_memory=True,
        persistent_workers=True
    )
    
    # åˆ›å»ºGPUä¼˜åŒ–æ¨¡å‹
    print("\nğŸ§  åˆ›å»ºGPUä¼˜åŒ–æ¨¡å‹...")
    model = GPUOptimizedCNN(
        input_channels=22,
        base_channels=config['base_channels']
    )
    
    print(f"ğŸ”¢ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºGPUä¼˜åŒ–è®­ç»ƒå™¨
    print("\nğŸš€ åˆ›å»ºGPUä¼˜åŒ–è®­ç»ƒå™¨...")
    trainer = GPUOptimizedTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader
    )
    
    # å¼€å§‹GPUä¼˜åŒ–è®­ç»ƒ
    print("\nğŸ”¥ å¼€å§‹GPUä¼˜åŒ–è®­ç»ƒ...")
    trainer.train(num_epochs=config['num_epochs'])
    
    print("\nğŸ‰ GPUä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    print("ğŸ“ ç»“æœä¿å­˜åœ¨: gpu_wildfire_outputs/")
    print("ğŸ’¾ æœ€ä½³æ¨¡å‹: best_gpu_model.pth")
    print("ğŸ“Š æ€§èƒ½åˆ†æ: gpu_performance_analysis.png")


if __name__ == "__main__":
    main() 