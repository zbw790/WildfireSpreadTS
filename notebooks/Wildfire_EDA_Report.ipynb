{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Spread: Exploratory Data Analysis Report\n",
    "\n",
    "This notebook conducts a rigorous, multi-stage Exploratory Data Analysis (EDA) on the wildfire dataset. The structure follows a formal, academically-sound methodology, separating pure observation from intervention to ensure reproducibility and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Baseline Data Characterization & Integrity Assessment (EDA Level 0)\n",
    "\n",
    "**Objective:** To establish a baseline, objective understanding of the dataset as received, covering its structure, content, and quality, prior to any modifications. This stage is purely observational and constitutes an immutable record of the data's initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Adjust display options for pandas\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Define the feature names based on the final documentation (0-indexed)\n",
    "FEATURE_NAMES = [\n",
    "    'VIIRS_M11', 'VIIRS_I2', 'VIIRS_I1', 'NDVI', 'EVI2', # 0-4\n",
    "    'Total_Precip', 'Wind_Speed', 'Wind_Direction', 'Min_Temp_K', 'Max_Temp_K', # 5-9\n",
    "    'ERC', 'Spec_Hum', 'Slope', 'Aspect', 'Elevation', # 10-14\n",
    "    'Landcover', 'Forecast_Precip', 'Forecast_Wind_Speed', 'Forecast_Wind_Dir', # 15-18\n",
    "    'Forecast_Temp_C', 'Forecast_Spec_Hum', 'Active_Fire' # 19-21\n",
    "]\n",
    "\n",
    "# For this EDA, we will load a single file and flatten it for analysis.\n",
    "# This approach is suitable for understanding distributions and initial quality.\n",
    "file_path = '../data/processed/2020/fire_24462610.hdf5'\n",
    "\n",
    "def load_and_flatten_h5(file_path, feature_names):\n",
    "    \"\"\"Loads data from a single H5 file and flattens it into a 2D pandas DataFrame.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f['data'][:] # Load all data into memory\n",
    "        t, c, h, w = data.shape\n",
    "        # Reshape to (T*H*W, C)\n",
    "        data_reshaped = data.transpose(0, 2, 3, 1).reshape(-1, c)\n",
    "        df = pd.DataFrame(data_reshaped, columns=feature_names)\n",
    "        return df\n",
    "\n",
    "df_raw = load_and_flatten_h5(file_path, FEATURE_NAMES)\n",
    "\n",
    "print(\"--- 1.1 Data Import & Structural Overview ---\")\n",
    "print(f\"Dataset Dimensions: {df_raw.shape[0]} observations (pixels) and {df_raw.shape[1]} variables (channels).\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df_raw.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "display(df_raw.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Variable Identification & Data Type Validation\n",
    "\n",
    "**Rationale:** Incorrect data types are a primary source of errors. This step ensures that each variable's data type is consistent with its real-world meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1.2 Variable Data Types ---\")\n",
    "print(df_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preliminary Data Health Check: Macro-Structural Issues\n",
    "\n",
    "**Rationale:** Removing duplicates is critical to avoid biased results. Fixing structural errors ensures data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1.3 Data Health Check ---\")\n",
    "duplicate_rows = df_raw.duplicated().sum()\n",
    "print(f\"Number of duplicate rows found: {duplicate_rows}\")\n",
    "\n",
    "# For structural errors, we'll focus on the 'Landcover' categorical variable\n",
    "print(\"Unique values in 'Landcover' (Channel 16):\")\n",
    "print(sorted(df_raw['Landcover'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Initial Global Descriptive Summary & Data Dictionary\n",
    "\n",
    "**Rationale:** This provides the first quantitative insight into the dataset's scale, range, and distributions, forming the final step of the 'pure understanding' phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1.4.1 Descriptive Summary for Numerical Variables ---\")\n",
    "display(df_raw.describe().T)\n",
    "\n",
    "print(\"\\n--- 1.4.2 Frequency Distribution for Categorical Variables ---\")\n",
    "print(\"Value counts for 'Landcover':\")\n",
    "display(df_raw['Landcover'].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1: Data Dictionary & Preliminary Assessment\n",
    "\n",
    "This table serves as the foundational reference document for the entire analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dictionary(df):\n",
    "    \"\"\"Generates a data dictionary DataFrame from the raw data.\"\"\"\n",
    "    dict_data = {\n",
    "        'Variable Name': df.columns,\n",
    "        'Original Dtype': [str(t) for t in df.dtypes],\n",
    "        'Missing Values': df.isnull().sum().values,\n",
    "        'Missing Percentage': (df.isnull().sum() / len(df) * 100).values\n",
    "    }\n",
    "    data_dict_df = pd.DataFrame(dict_data)\n",
    "    \n",
    "    # Add corrected analysis type and description based on documentation\n",
    "    analysis_types = [\n",
    "        'Numerical-Continuous', 'Numerical-Continuous', 'Numerical-Continuous', 'Numerical-Continuous', 'Numerical-Continuous',\n",
    "        'Numerical-Continuous', 'Numerical-Continuous', 'Numerical-Cyclical', 'Numerical-Continuous (K)', 'Numerical-Continuous (K)',\n",
    "        'Numerical-Continuous', 'Numerical-Continuous', 'Numerical-Continuous', 'Numerical-Cyclical', 'Numerical-Continuous',\n",
    "        'Categorical-Nominal', 'Numerical-Continuous', 'Numerical-Continuous', 'Numerical-Special', \n",
    "        'Numerical-Continuous (C)', 'Numerical-Continuous', 'Categorical-Binary'\n",
    "    ]\n",
    "    descriptions = [\n",
    "        'VIIRS Surface Reflectance Band M11', 'VIIRS Surface Reflectance Band I2', 'VIIRS Surface Reflectance Band I1',\n",
    "        'Normalized Difference Vegetation Index', 'Enhanced Vegetation Index 2',\n",
    "        'Total Daily Precipitation', 'Daily Wind Speed', 'Daily Wind Direction (degrees)',\n",
    "        'Minimum Daily Temperature (Kelvin)', 'Maximum Daily Temperature (Kelvin)',\n",
    "        'Energy Release Component', 'Specific Humidity', 'Topographical Slope (degrees)',\n",
    "        'Topographical Aspect (degrees)', 'Elevation (meters)',\n",
    "        'MODIS Land Cover Class', '24h Forecast Precipitation', '24h Forecast Wind Speed',\n",
    "        '24h Forecast Wind Direction (Vector Component)', '24h Forecast Temperature (Celsius)',\n",
    "        '24h Forecast Specific Humidity', 'Active Fire Mask'\n",
    "    ]\n",
    "    data_dict_df['Analysis Type'] = analysis_types\n",
    "    data_dict_df['Description'] = descriptions\n",
    "    \n",
    "    return data_dict_df[['Variable Name', 'Original Dtype', 'Analysis Type', 'Description', 'Missing Values', 'Missing Percentage']]\n",
    "\n",
    "data_dictionary = create_data_dictionary(df_raw)\n",
    "print(\"--- Table 1: Data Dictionary & Preliminary Assessment ---\")\n",
    "display(data_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: In-Depth Data Cleaning & Preprocessing (EDA Level 1)\n",
    "\n",
    "**Objective:** To systematically address the data quality issues identified in Section 1. This is the 'intervention' phase, transforming the raw data into an analysis-ready format. Every decision and its rationale must be meticulously documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Systematic Management of Missing Values\n",
    "\n",
    "**Rationale:** Understanding the pattern of missingness is more critical than just knowing the count. This diagnosis informs the selection of an appropriate imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The current dataset from one file shows no missing values.\n",
    "# The following code is a template for how one would proceed if NaNs were present.\n",
    "print(\"--- 2.1.1 Missing Value Pattern Diagnosis ---\")\n",
    "\n",
    "if df_raw.isnull().sum().sum() > 0:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df_raw.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Value Heatmap')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the sample file. Skipping heatmap.\")\n",
    "\n",
    "print(\"\\n--- 2.1.2 Imputation Strategy ---\")\n",
    "print(\"Rationale: For this dataset, the primary concern would be NaN values in the Active Fire channel, which should be imputed with 0. For other features, median imputation is a robust choice for skewed distributions, while mean is suitable for symmetric ones. A more advanced approach like KNN imputation could be used for higher accuracy.\")\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_cleaned = df_raw.copy()\n",
    "\n",
    "# Example Imputation (if needed):\n",
    "# median_val = df_cleaned['Some_Variable'].median()\n",
    "# df_cleaned['Some_Variable'].fillna(median_val, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outlier Detection & Management Strategy\n",
    "\n",
    "**Rationale:** Using multiple methods (visual and statistical) provides a more robust identification of potential outliers, prompting careful investigation rather than automatic removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 2.2.1 Univariate Outlier Identification ---\")\n",
    "# We will visualize a few key continuous variables for outliers\n",
    "outlier_check_vars = ['Max_Temp_K', 'Wind_Speed', 'NDVI', 'Elevation']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, var in enumerate(outlier_check_vars):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.boxplot(y=df_cleaned[var])\n",
    "    plt.title(f'Box Plot of {var}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- 2.2.3 Handling Philosophy ---\")\n",
    "print(\"Rationale: Outliers are not always errors; they can be genuine extreme values. For this dataset, outliers in temperature or wind speed are likely real weather events and should be kept. Outliers in sensor data (e.g., VIIRS, NDVI) might be due to atmospheric interference and could be capped or transformed. A log transform is a common strategy for right-skewed data with extreme positive outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 2: Data Processing Strategy Log\n",
    "\n",
    "This table serves as the official logbook of the data cleaning process, documenting every modification made to the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_log_data = {\n",
    "    'Variable Name': ['Forecast_Temp_C', 'Wind_Direction', 'Aspect', 'Landcover', 'Active_Fire'],\n",
    "    'Identified Issue': ['Unit Mismatch (Celsius)', 'Cyclical Nature', 'Cyclical Nature', 'Categorical Nature', 'Target Variable'],\n",
    "    'Chosen Strategy': ['Convert to Kelvin', 'Sine Transformation', 'Sine Transformation', 'One-Hot Encoding', 'Separate as Target'],\n",
    "    'Rationale': [\n",
    "        'To align with historical temperature units (Kelvin) for model consistency.',\n",
    "        'To represent the cyclical continuity of degrees (360 is close to 0).',\n",
    "        'To represent the cyclical continuity of degrees.',\n",
    "        'To convert nominal categories into a numerical format suitable for modeling without implying order.',\n",
    "        'This is the variable to be predicted, not an input feature for the model itself.'\n",
    "    ]\n",
    "}\n",
    "processing_log_df = pd.DataFrame(processing_log_data)\n",
    "\n",
    "print(\"--- Table 2: Data Processing Strategy Log ---\")\n",
    "display(processing_log_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}